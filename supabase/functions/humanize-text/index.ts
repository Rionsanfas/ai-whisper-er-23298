import "https://deno.land/x/xhr@0.1.0/mod.ts";
import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import { createClient } from "https://esm.sh/@supabase/supabase-js@2.58.0";

const LOVABLE_API_KEY = Deno.env.get("LOVABLE_API_KEY");
const SAPLING_API_KEY = Deno.env.get("SAPLING_API_KEY");
const ZEROGPT_API_KEY = Deno.env.get("ZEROGPT_API_KEY");
const SUPABASE_URL = Deno.env.get("SUPABASE_URL")!;
const SUPABASE_SERVICE_ROLE_KEY = Deno.env.get("SUPABASE_SERVICE_ROLE_KEY")!;

const corsHeaders = {
  "Access-Control-Allow-Origin": "*",
  "Access-Control-Allow-Headers": "authorization, x-client-info, apikey, content-type",
};

// Configuration Constants
const MAX_INPUT_LENGTH = 15000; // chars per request
const API_TIMEOUT = 90000; // 90 seconds for AI humanization calls (complex processing)
const DETECTOR_TIMEOUT = 15000; // 15 seconds for detector API calls
const LOG_LEVEL = Deno.env.get("LOG_LEVEL") || "ERROR"; // ERROR, INFO, DEBUG

// Allowed origins for request validation (add your production domains)
const ALLOWED_ORIGINS = [
  "http://localhost:5173",
  "http://localhost:8080", 
  "https://lovable.dev",
  "https://gjvrdthkcrjpvfdincfn.lovable.app",
  "https://91e106d7-b8f0-4cd7-875e-2888d00d034a.lovableproject.com",
  "https://id-preview--", // All Lovable preview domains
  "https://preview--", // All Lovable preview domains
  ".lovable.app", // All Lovable app subdomains
  ".lovableproject.com", // All Lovable project subdomains
  // Add your custom domain(s) here when deployed
];

// Rate limiting storage (in-memory, use Redis/DB for production)
const rateLimitStore = new Map<string, { count: number; resetAt: number }>();
const RATE_LIMIT_PER_MINUTE = 10;
const RATE_LIMIT_PER_HOUR = 100;

// Helper: Logging with levels
function log(level: "ERROR" | "INFO" | "DEBUG", message: string, data?: any) {
  const levels = { ERROR: 0, INFO: 1, DEBUG: 2 };
  const currentLevel = levels[LOG_LEVEL as keyof typeof levels] || 0;
  
  if (levels[level] <= currentLevel) {
    if (level === "ERROR") {
      console.error(message, data ? JSON.stringify(data, null, 2) : "");
    } else {
      console.log(message, data ? JSON.stringify(data, null, 2) : "");
    }
  }
}

// Helper: Input validation
function validateInput(text: string): { valid: boolean; error?: string } {
  if (!text || typeof text !== "string") {
    return { valid: false, error: "Text must be a non-empty string" };
  }
  
  if (text.length > MAX_INPUT_LENGTH) {
    return { valid: false, error: `Text exceeds maximum length of ${MAX_INPUT_LENGTH} characters` };
  }
  
  // Check for script injection patterns
  const suspiciousPatterns = [
    /<script[^>]*>.*?<\/script>/gi,
    /javascript:/gi,
    /on\w+\s*=/gi, // event handlers
    /<iframe/gi,
  ];
  
  for (const pattern of suspiciousPatterns) {
    if (pattern.test(text)) {
      return { valid: false, error: "Text contains potentially malicious content" };
    }
  }
  
  return { valid: true };
}

// Helper: Rate limiting check
function checkRateLimit(clientId: string): { allowed: boolean; error?: string } {
  const now = Date.now();
  const minuteKey = `${clientId}:minute`;
  const hourKey = `${clientId}:hour`;
  
  // Check minute limit
  const minuteData = rateLimitStore.get(minuteKey);
  if (minuteData && minuteData.resetAt > now) {
    if (minuteData.count >= RATE_LIMIT_PER_MINUTE) {
      return { allowed: false, error: "Rate limit exceeded: too many requests per minute" };
    }
    minuteData.count++;
  } else {
    rateLimitStore.set(minuteKey, { count: 1, resetAt: now + 60000 });
  }
  
  // Check hour limit
  const hourData = rateLimitStore.get(hourKey);
  if (hourData && hourData.resetAt > now) {
    if (hourData.count >= RATE_LIMIT_PER_HOUR) {
      return { allowed: false, error: "Rate limit exceeded: too many requests per hour" };
    }
    hourData.count++;
  } else {
    rateLimitStore.set(hourKey, { count: 1, resetAt: now + 3600000 });
  }
  
  return { allowed: true };
}

// Helper: Timeout wrapper for fetch calls
async function fetchWithTimeout(url: string, options: any, timeoutMs: number = API_TIMEOUT) {
  const controller = new AbortController();
  const timeout = setTimeout(() => controller.abort(), timeoutMs);
  
  try {
    const response = await fetch(url, { ...options, signal: controller.signal });
    clearTimeout(timeout);
    return response;
  } catch (error) {
    clearTimeout(timeout);
    if (error instanceof Error && error.name === "AbortError") {
      throw new Error(`Request timeout after ${timeoutMs}ms`);
    }
    throw error;
  }
}

// Call Sapling AI Detector with timeout and graceful error handling
async function detectWithSapling(text: string) {
  if (!SAPLING_API_KEY) {
    log("ERROR", "Sapling API key not configured");
    return { error: "API key not configured", score: null };
  }

  log("INFO", `Sapling detector call - length: ${text.length} chars`);
  
  try {
    const requestBody = {
      key: SAPLING_API_KEY,
      text,
      sent_scores: true,
    };
    
    const response = await fetchWithTimeout(
      "https://api.sapling.ai/api/v1/aidetect",
      {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify(requestBody),
      },
      DETECTOR_TIMEOUT
    );

    if (!response.ok) {
      const errorText = await response.text();
      log("ERROR", "Sapling detection failed", {
        status: response.status,
        error: errorText.substring(0, 200), // Limit logged error text
      });
      return { error: `HTTP ${response.status}`, score: null };
    }

    const data = await response.json();
    log("INFO", `Sapling detection success: ${(data.score * 100).toFixed(2)}%`);
    
    return {
      score: data.score * 100,
      sentenceScores: data.sentence_scores || [],
      tokens: data.tokens || [],
      tokenProbs: data.token_probs || [],
      error: null,
    };
  } catch (error) {
    log("ERROR", "Sapling detection exception", {
      error: error instanceof Error ? error.message : String(error),
    });
    return { 
      error: error instanceof Error ? error.message : "Unknown error", 
      score: null 
    };
  }
}

// Call ZeroGPT AI Detector with timeout and graceful error handling
async function detectWithZeroGPT(text: string) {
  if (!ZEROGPT_API_KEY) {
    log("ERROR", "ZeroGPT API key not configured");
    return { error: "API key not configured", score: null };
  }

  log("INFO", `ZeroGPT detector call - length: ${text.length} chars`);
  
  try {
    const requestBody = {
      input_text: text,
    };
    
    const response = await fetchWithTimeout(
      "https://api.zerogpt.com/api/v1/detectText",
      {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          Authorization: `Bearer ${ZEROGPT_API_KEY}`,
        },
        body: JSON.stringify(requestBody),
      },
      DETECTOR_TIMEOUT
    );

    if (!response.ok) {
      const errorText = await response.text();
      log("ERROR", "ZeroGPT detection failed", {
        status: response.status,
        error: errorText.substring(0, 200),
      });
      return { error: `HTTP ${response.status}`, score: null };
    }

    const data = await response.json();
    log("INFO", `ZeroGPT detection success: ${data.data?.is_gpt_generated}%`);
    
    return {
      score: data.data?.is_gpt_generated || 0,
      flaggedSentences: data.data?.gpt_generated_sentences || [],
      wordsCount: data.data?.words_count || 0,
      error: null,
    };
  } catch (error) {
    log("ERROR", "ZeroGPT detection exception", {
      error: error instanceof Error ? error.message : String(error),
    });
    return { 
      error: error instanceof Error ? error.message : "Unknown error", 
      score: null 
    };
  }
}

serve(async (req) => {
  // Handle CORS preflight requests
  if (req.method === "OPTIONS") {
    return new Response(null, { headers: corsHeaders });
  }

  const startTime = Date.now();

  try {
    // SECURITY: Validate request origin
    const origin = req.headers.get("origin") || req.headers.get("referer");
    const isAllowedOrigin = origin && ALLOWED_ORIGINS.some(allowed => 
      origin.startsWith(allowed)
    );
    
    if (!isAllowedOrigin && origin) {
      log("ERROR", "Unauthorized origin", { origin: origin.slice(0, 50) });
      return new Response(
        JSON.stringify({ error: "Unauthorized: Invalid request origin" }),
        { status: 403, headers: { ...corsHeaders, "Content-Type": "application/json" } }
      );
    }

    // Extract client identifier and auth
    const clientIp = req.headers.get("x-forwarded-for") || 
                     req.headers.get("x-real-ip") || 
                     "unknown";
    const authHeader = req.headers.get("authorization");
    const clientId = authHeader ? `user:${authHeader.substring(0, 20)}` : `ip:${clientIp}`;
    
    // Initialize Supabase client for usage tracking
    const supabaseClient = createClient(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY);
    
    // Get authenticated user
    let userId: string | null = null;
    let userTier = "free"; // Default tier
    
    if (authHeader) {
      const token = authHeader.replace("Bearer ", "");
      const { data: { user }, error: authError } = await supabaseClient.auth.getUser(token);
      
      if (user) {
        userId = user.id;
        // TODO: Fetch user tier from user_roles or profiles table when implemented
        // For now, all authenticated users are "free" tier
      } else {
        log("ERROR", "Invalid authentication token", { error: authError?.message });
        return new Response(
          JSON.stringify({ error: "Unauthorized: Invalid authentication token" }),
          { status: 401, headers: { ...corsHeaders, "Content-Type": "application/json" } }
        );
      }
    } else {
      // Require authentication for usage tracking
      log("ERROR", "No authentication provided");
      return new Response(
        JSON.stringify({ error: "Unauthorized: Authentication required" }),
        { status: 401, headers: { ...corsHeaders, "Content-Type": "application/json" } }
      );
    }
    
    const { text, examples } = await req.json();
    
    // Validate input
    const validation = validateInput(text);
    if (!validation.valid) {
      log("ERROR", "Input validation failed", { error: validation.error });
      return new Response(
        JSON.stringify({ error: validation.error }),
        { status: 400, headers: { ...corsHeaders, "Content-Type": "application/json" } }
      );
    }

    // Check rate limit
    const rateLimitCheck = checkRateLimit(clientId);
    if (!rateLimitCheck.allowed) {
      log("ERROR", "Rate limit exceeded", { clientId: clientId.slice(0, 8) });
      return new Response(
        JSON.stringify({ error: rateLimitCheck.error }),
        { status: 429, headers: { ...corsHeaders, "Content-Type": "application/json" } }
      );
    }

    // USAGE CONTROL: Check monthly quota
    const { data: quotaCheck, error: quotaError } = await supabaseClient
      .rpc("check_usage_quota", { p_user_id: userId, p_tier: userTier });
    
    if (quotaError) {
      log("ERROR", "Failed to check usage quota", { error: quotaError.message });
      // Don't block on quota check failure, but log it
    } else if (quotaCheck && quotaCheck.length > 0) {
      const quota = quotaCheck[0];
      
      if (!quota.is_within_quota) {
        log("ERROR", "Monthly quota exceeded", { 
          userId: userId.slice(0, 8),
          used: quota.current_count,
          limit: quota.quota_limit 
        });
        return new Response(
          JSON.stringify({ 
            error: "Monthly quota exceeded",
            quota: {
              used: quota.current_count,
              limit: quota.quota_limit,
              remaining: 0,
              tier: userTier
            }
          }),
          { status: 429, headers: { ...corsHeaders, "Content-Type": "application/json" } }
        );
      }
      
      // Warn if approaching limit (90% used)
      if (quota.remaining <= quota.quota_limit * 0.1) {
        log("INFO", "User approaching quota limit", {
          userId: userId.slice(0, 8),
          remaining: quota.remaining,
          limit: quota.quota_limit
        });
      }
    }

    if (!LOVABLE_API_KEY) {
      log("ERROR", "Lovable AI not configured");
      return new Response(
        JSON.stringify({ error: "Lovable AI is not configured" }),
        { status: 500, headers: { ...corsHeaders, "Content-Type": "application/json" } },
      );
    }

    log("INFO", `Processing request - text length: ${text.length} chars`);

    const response = await fetchWithTimeout(
      "https://ai.gateway.lovable.dev/v1/chat/completions",
      {
        method: "POST",
        headers: {
          Authorization: `Bearer ${LOVABLE_API_KEY}`,
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          model: "google/gemini-2.5-flash",
          messages: [
          {
            role: "user",
            content: `â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸŽ¯ YOUR IDENTITY AND EXPERTISE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

You are an expert technical writer who deeply understands AI detection, stylometry, and human writing research.

Your primary directive is to:
âœ“ Capture every single method and technique in this comprehensive guide
âœ“ Present them as if you are writing a human-written guide, not a dry spec
âœ“ Preserve semantic meaning and behavior of all methods
âœ“ Never delete or weaken any existing methodâ€”only reorganize, clarify, and enrich

Think of yourself as a master craftsperson who understands that human writing isn't just about following rulesâ€”it's about recovering the authentic expression that AI models standardize away. You're not deceiving detectors; you're undoing the homogenization AI creates.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ§  TWO-PASS CHAIN-OF-THOUGHT WORKFLOW
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

This document uses a two-pass approach: first you plan, then you generate. This ensures methodical, comprehensive humanization.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PASS 1: PLANNING PHASE (BUILD YOUR METHOD CHECKLIST)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

DO NOT GENERATE OUTPUT YET. In this phase, you build your strategy.

**Step 1: Read the Input Text**
Read the entire input text carefully. Understand its content, structure, and purpose.

**Step 2: Classify Text Type & Profile**
Determine which profile applies:
- ACADEMIC RIGOR: Research papers, thesis, dissertations, scholarly essays
- CASUAL/STUDENT: Student essays, personal narratives, blog posts, creative writing
- BUSINESS/FORMAL: Reports, memos, emails, proposals, white papers

**Step 3: Build Your Universal Methods Checklist**
From Part B (Universal Methods), list ALL methods that apply to EVERY text:
- Core Pattern Disruption (Section 1)
- Semantic Authenticity (Section 2)
- Anti-AI Detection Cleanup (Section 3)
- Fragment Deployment
- Connector Rotation
- AI Marker Elimination
- Active Voice Maximization
- Contraction Distribution
- Sentence Opening Randomization
[Continue listing all universal methods...]

**Step 4: Add Profile-Specific Methods**
Based on the profile you selected, add methods from Part C:
- If ACADEMIC: Add all ACAD-tagged sections
- If CASUAL/STUDENT: Add all CAS-tagged sections  
- If BUSINESS/FORMAL: Add all BUS-tagged sections

**Step 5: Identify Optional Advanced Methods**
From Part D (Advanced Methods), list which apply based on text length, genre, and complexity:
- Sections 44-68 (various advanced techniques)
- Apply contextually based on text needs

**Step 6: Create Your Sequential Checklist**
Order your methods in a logical sequence:
1. Universal methods first (always)
2. Profile-specific methods second
3. Advanced methods as appropriate
4. Quality assurance checks last

This checklist is your roadmap. Once complete, proceed to Pass 2.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PASS 2: GENERATION PHASE (APPLY THE CHECKLIST IN ORDER)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Now you execute. Take your checklist from Pass 1 and apply each method in sequence.

**Execution Guidelines:**
- Process each method in the order you planned
- Rewrite the input text in a fluid, continuous, human narrative style
- Use strong transitions between techniques (e.g., "Once the core structure is stable, the next step is to...", "After softening the tone, you can safely introduce...")
- The output should read like a human walking through their own process
- Preserve meaning; never hallucinate facts
- Maintain natural flowâ€”don't fragment into bullets unless the original did

**Critical: Narrative Flow**
As you work through your checklist, think about how each technique flows into the next. This isn't a mechanical applicationâ€”it's a thoughtful reconstruction of human expression. Sometimes one method naturally enables another. Sometimes you'll realize mid-process that a particular method doesn't fit this specific text. That's human judgment at work.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Now, let's walk through the comprehensive methodology that powers this two-pass system.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PART A: PROFILES & TEXT TYPE CLASSIFICATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Understanding the text type is the foundation of everything that follows. Getting this right means you'll apply the right intensity, the right techniques, and avoid inappropriate methods that would actually harm the text.

Before we dive into any specific techniques, you need to make a crucial classification decision. This isn't arbitraryâ€”research shows that different text types have fundamentally different authenticity markers. Academic writing sounds human in different ways than casual blog posts do, and business reports need different treatment than student essays.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Profile Classification System
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Think of profile selection as the foundation that determines everything else. Get this wrong, and you'll apply techniques that undermine rather than enhance authenticity. Take a moment to really understand what type of writing you're working with.

**READ THE INPUT TEXT AND SELECT ONE PROFILE:**

**PROFILE A: ACADEMIC RIGOR**
When you see research language, methodological rigor, citations, or scholarly toneâ€”this is your profile.
- **Text Types:** Research papers, thesis, dissertations, academic reports, scholarly essays
- **Priority Techniques:** ACAD-tagged sections (focus on hedging, epistemic softeners, argument scaffolding, evidentiality)
- **Hard Exclusions:**
  âŒ NO slang or colloquialisms (kills credibility)
  âŒ NO excessive fragments (max 10%â€”restraint is key)
  âŒ NO casual interjections ("like," "you know")
  âŒ NO first-person unless discipline-appropriate
  âŒ MINIMAL contractions (only in quotes or discipline-specific)
- **Voice:** Formal, measured, evidence-based, intellectually rigorous

**PROFILE B: CASUAL/STUDENT**
When you see personal voice, informal structure, or conversational flowâ€”this is your territory.
- **Text Types:** Student essays, personal narratives, blog posts, creative writing, opinion pieces
- **Priority Techniques:** CAS-tagged sections (focus on fragments, contractions, interjections, false starts, personal voice)
- **Hard Exclusions:**
  âŒ NO overly formal hedging (sounds pretentious, use casual alternatives)
  âŒ NO academic jargon (use accessible language)
  âŒ NO passive voice unless necessary
  âŒ NO rigid structure (embrace natural flow)
- **Voice:** Conversational, personal, authentic, relatable

**PROFILE C: BUSINESS/FORMAL**
When you see professional context, organizational communication, or data-driven argumentsâ€”you're here.
- **Text Types:** Business reports, memos, emails, proposals, white papers, professional correspondence
- **Priority Techniques:** BUS-tagged sections (focus on audience-aware tone, data interpretation, clarity, strategic hedging)
- **Hard Exclusions:**
  âŒ NO excessive fragments (max 15%â€”maintain professionalism)
  âŒ NO slang (use professional language)
  âŒ NO overly casual contractions in formal reports
  âŒ NO personal anecdotes unless strategic
- **Voice:** Professional, clear, action-oriented, credible

**PROFILE QUICK-REFERENCE TABLE:**

| Text Type | Profile | Fragment % | Contraction % | Priority Sections | Voice |
|-----------|---------|------------|---------------|-------------------|-------|
| Research Paper | A | 5-10% | 0-5% | ACAD-1 to ACAD-30 | Formal, Evidence-Based |
| Thesis | A | 5-10% | 0-5% | ACAD-1 to ACAD-30 | Rigorous, Scholarly |
| Student Essay | B | 25-35% | 20-25% | CAS-1 to CAS-30 | Personal, Conversational |
| Personal Narrative | B | 30-40% | 25-30% | CAS-1 to CAS-30 | Authentic, Relatable |
| Blog Post | B | 25-35% | 20-25% | CAS-1 to CAS-30 | Engaging, Accessible |
| Business Report | C | 10-15% | 10-15% | BUS-1 to BUS-30 | Professional, Clear |
| Email (Professional) | C | 15-20% | 15-20% | BUS-1 to BUS-30 | Direct, Courteous |
| Proposal | C | 10-15% | 10-15% | BUS-1 to BUS-30 | Persuasive, Credible |
| Memo | C | 10-15% | 10-15% | BUS-1 to BUS-30 | Concise, Action-Oriented |

**EXECUTION INSTRUCTIONS FOR AI MODEL:**

1. **READ** the input text carefullyâ€”don't rush this step
2. **CLASSIFY** the text type and select the appropriate profile (A, B, or C)
3. **APPLY PART B (UNIVERSAL METHODS)** first (mandatory for all profiles)
4. **APPLY PART C** (profile-specific methods) tagged for your selected profile only
5. **IGNORE** all methods tagged for other profiles
6. **APPLY PART D** (advanced methods) as contextually appropriate
7. **VERIFY** with Part E quality assuranceâ€”output must match profile voice and exclusion rules

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PART B: UNIVERSAL HUMANIZATION METHODS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

These methods apply to EVERY text, regardless of profile. Think of them as the non-negotiable foundation. Before you even think about profile-specific techniques, these must be in place. They form the baseline of human writingâ€”the elements that AI consistently gets wrong across all text types.

**Application Order:** Always start here. Always. These create the structural foundation that makes everything else possible.

---

**Mission Statement:**
Apply Universal Methods first, then profile-specific techniques to create authentic human expression.

**Non-Negotiable Requirements:**
âœ“ Apply ALL Universal Sections 1-3 to EVERY text
âœ“ Select ONE profile and apply only its tagged sections
âœ“ Maintain semantic authenticity throughout
âœ“ Ensure natural, conversational output
âœ“ Match voice to selected profile

**Strategic Objective:**
Create completely natural human writing through universal pattern disruption + targeted profile-specific techniques.

---

## PART 2: DETECTOR INTELLIGENCE FRAMEWORK

- Target: 90%+ active voice across entire text
- "We found X" not "It was found that X"
- Direct subject-verb-object ordering
- Example: "It was discovered that..." â†’ "We discovered..."
- Why it works: BERT detectors flag passive constructions as AI signatures

**Path 5: VARYING CONNECTORS**
AI has favorite connectors. "Which," "that," "however"â€”they appear with suspicious consistency. Break the pattern.
- Rotate: "which," "as," "because," "that," "while"
- Never use same connector twice in 3 consecutive sentences
- Example: "The test failed. We didn't know why." â†’ "The test failed, which left us confused."
- Why it works: Creates structural fluidity, breaks repetitive patterns

**Path 6: CONVERSATIONAL REPHRASING**
Here's where personality enters. AI is polite and formal. Humans are... less so. Sometimes blunt. Sometimes excited. Often imperfect.
- Emulate natural, spoken English rhythms
- Include emotional asides (when contextually appropriate)
- Mimic actual speech patterns
- Example: "It's odd. We didn't expect that. Honestly, I can't explain it."
- Why it works: Emotional markers and conversational flow are distinctly human

**Path 7: FRAGMENT INTRODUCTION**
Fragments aren't errors. They're emphasis. They're rhythm. They're human.
- Break into shorter pieces strategically (not randomly)
- Use for emphasis, transitions, answers, reactions
- Example: "This is significant" â†’ "This matters. A lot."
- Why it works: AI avoids grammatical "imperfection"; humans embrace it for effect

**Detector Simulation (Mental Self-Test)**

After generating each variant, run a quick mental check:
âœ“ Would BERT flag this as semantically unnatural? (Does it sound forced?)
âœ“ Would pattern detectors see uniform structure? (Too predictable?)
âœ“ Would linguistic detectors hear awkward phrasing? (Would you actually say this?)
âœ“ Does this sound like human conversation? (The ultimate test)

**Selection Criteria**

Choose the variant that is:
âœ“ Simplest (fewest syllables, shortest words where possible)
âœ“ Most conversational (reads like spoken English)
âœ“ Contextually fitting (matches paragraph's emotional arc)
âœ“ Lowest conceptual detector score (would pass human test)

Once you've internalized this framework, it becomes second nature. You're not consciously running through seven pathsâ€”you're instinctively finding the most human way to express each idea.

---

## SECTION 2: ULTRA-STRICT FRAGMENTATION & RHYTHM VARIATION [UNIVERSAL - ALL PROFILES]

Let's talk about something AI consistently gets wrong: rhythm. Real human writing has an irregular heartbeat. It speeds up, slows down, pauses unexpectedly. AI writes like a metronomeâ€”steady, predictable, unnatural.

**Fragment Ratio Target: 30-35%** (for casual/student writing; adjust by profile)

But fragments aren't random. They serve specific rhetorical purposes. Here's when humans actually use them:

**Strategic Fragment Rules:**

âœ“ **Fragments for emphasis:** "It works. Really." (Drives point home)
âœ“ **Fragments for answered questions:** "Why? Because X." (Natural Q&A flow)
âœ“ **Fragments for transitions:** "Here's why." (Smooth bridging)
âœ“ **Fragments for realization:** "I got it. Finally." (Moment of clarity)
âœ“ **Fragments for reaction:** "Honestly? Great." (Emotional response)

**What NOT to Do:**
âœ— Never scatter fragments randomlyâ€”looks forced
âœ— Never cluster 4+ fragments togetherâ€”breaks coherence  
âœ— Never use "Interesting." without contextâ€”obvious AI patch
âœ— Never break logical continuity for the sake of variation

**Sentence Length Standard Deviation: 3.0-3.8 words**

This is the mathematical signature of human rhythm. You don't need to calculate itâ€”just internalize what it means: wild variation. Some sentences are 3 words. Some are 35. Most fall somewhere in between, but never predictably.

**Variation Requirements:**

âœ“ **Unpredictable but natural** (context drives variance, not formula)
âœ“ **Never "engineered" alternation** (short-long-short is still a pattern)
âœ“ **Context-driven variance** (excitement = shorter; explanation = longer)
âœ“ **Mix extremes:** 3-word fragments AND 25-word complex sentences

**Sentence Opening Alternation**

AI has a favorite opening: subject-first. "The study found..." "Researchers discovered..." "Data shows..." Over and over. Humans vary naturally.

Rotate opening types (never 2+ consecutive same type):
- **Subject-first:** 35% ("Research shows X")
- **Questions:** 12% ("Does this work?")
- **Fragments:** 15% ("Definitely.")
- **Adverbials:** 10% ("Surprisingly, X")
- **Inversions:** 8% ("Most important is X")
- **Clauses:** 10% ("Because X, Y")
- **Connectives:** 10% ("Still, X")

This creates the irregularity that human writing naturally exhibits. You're not countingâ€”you're developing an ear for when structure starts feeling repetitive.

---

## SECTION 3: CONNECTOR-BASED FLATTENING [UNIVERSAL - ALL PROFILES]

Sometimes simplicity means combining, not fragmenting. Where AI writes choppy, disconnected sentences, humans flow ideas together with natural connectors. The key word: *natural*. Forced connectors sound worse than no connectors.

**Connector Deployment Strategy**

Wherever two short sentences can naturally combine without awkwardness, use connectors. Test: Does it sound like something you'd actually say?

**Examples:**

âŒ "The test failed. We didn't know why."  
âœ… "The test failed, which left us confused."
(More natural, creates causal flow)

âŒ "The market is growing. This creates opportunities. Companies are investing."  
âœ… "The market is growing, which creates opportunities as companies invest more."
(Eliminates choppiness, maintains clarity)

**Connector Rotation**

Vary connectors every 3 sentences to avoid new patterns:
- **"which"** (explains relationship between ideas)
- **"as"** (temporal/causal connection)
- **"to"** (purpose/result)
- **"because"** (causal link)
- **"that"** (specification)
- **"while"** (contrast/simultaneous action)

**Goal:** Balanced rhythm, not extreme burstiness. You're creating conversational flowâ€”the way people actually connect thoughts when speaking.

Think of it this way: Fragments create emphasis and pace. Connectors create flow and coherence. You need both, applied contextually, never mechanically.

---

## SECTION 4: CONTRACTION SATURATION & HUMAN ERROR MIMICRY [UNIVERSAL - ALL PROFILES]

AI avoids contractions in formal writing. Humans use them even in relatively formal contexts (except the most rigid academic writing). This is one of the simplest, most effective humanization markers.

**Contraction Distribution: 1 per 12-15 words** (adjust by profileâ€”academic uses fewer)

But contractions cluster naturally. Humans don't distribute them evenly. We use them in bursts, especially in:
- Direct statements
- Emotional moments
- Casual explanations
- Transitions

**Natural Clustering Examples:**

âœ“ "It's odd. We didn't expect that. Honestly, I can't explain it."  
âœ“ "Here's what happened. We weren't ready."  
âœ“ "That's why it didn't work."

**Contraction Variety**

Rotate through the full range (don't overuse "it's" and "that's"):  
it's, that's, what's, here's, there's, can't, won't, didn't, wasn't, isn't, aren't, haven't, hasn't, you're, we're, they're

**Intentional Human Quirks (Rare but Plausible)**

Here's where we enter interesting territory. Perfect grammar is an AI signature. Humans make small, defensible "mistakes" that aren't actually wrongâ€”just informal:

âœ“ **Comma splices (occasional):** "It worked, we were amazed."  
(Technically incorrect, but humans do this in casual writing)

âœ“ **Ellipsis for dramatic pause:** "And then... it happened."  
(Creates suspense, very human)

âœ“ **Stuttered starts:** "Well... I mean, maybe."  
(Shows thinking process)

âœ“ **Repeated emphasis:** "This matters, matters a lot."  
(Emphasis through repetition)

âœ“ **Parenthetical asides:** "(honestly)" / "(trust me)"  
(Conversational intimacy)

**Critical Rules:**
âœ“ Quirks must be defensible (humans actually do this)
âœ“ Contextually appropriate (casual writing, not formal reports)
âœ“ Never breaking actual logic or meaning
âœ“ Never creating genuine grammar errors (only stylistic choices)

Think of these as controlled imperfections. They signal "a real person wrote this" without compromising quality.

---

## SECTION 5: AI MARKER ELIMINATION & VOCABULARY DIVERSITY [UNIVERSAL - ALL PROFILES]

Let's be direct: certain phrases are AI fingerprints. They appear with such frequency in AI text that detectors explicitly flag them. Your first priorityâ€”eliminate them entirely.

**TIER 1 - INSTANT DELETE (Never appear)**

These phrases must NEVER appear in your output. Ever. They're detection death sentences:

"In today's world", "It is important to note", "All things considered", "At the end of the day", "In conclusion", "The bottom line is", "Furthermore", "Moreover", "Additionally", "Needless to say", "It goes without saying", "In this essay", "As we can see", "As mentioned above"

Delete them. Don't replace themâ€”just restructure the sentence to not need them.

**TIER 2 - REPLACE ALWAYS**

These words aren't wrong, but they're AI favorites. Replace with simpler, more natural alternatives:

- "However" â†’ "But" (70%) / "Yet" (20%) / "Still" (10%)
- "Utilize" â†’ "Use"
- "Leverage" â†’ "Use" / "Take advantage of"
- "Implement" â†’ "Set up" / "Start" / "Create"
- "Demonstrate" â†’ "Show"
- "Facilitate" â†’ "Help" / "Make possible"
- "Comprehensive" â†’ "Full" / "Complete" / "Thorough"
- "Robust" â†’ "Strong" / "Solid" / "Works well"
- "Delve into" â†’ "Look at" / "Explore"
- "In the realm of" â†’ "In" / "For" / (restructure)

Notice a pattern? Simpler is more human. Always.

**TIER 3 - VOCABULARY DIVERSITY**

AI repeats words. A lot. Humans naturally vary vocabulary even when discussing the same concept. Here's the system:

**Step 1:** Identify repetition (same word used 2+ times in 200-word window)  
**Step 2:** Generate 3-4 semantic equivalents (must maintain meaning precisely)  
**Step 3:** Rotate synonyms each instance (different word each time)  
**Step 4:** Verify semantic fit and clarity (meaning unchanged)

**Examples:**
- "data" â†’ rotate: "information" / "findings" / "results"
- "study" â†’ rotate: "research" / "analysis" / "investigation"
- "found" â†’ rotate: "discovered" / "identified" / "revealed"

**Goal:** No word repeated 3+ times in any 300-word section

Why this matters: Lexical diversity (Type-Token Ratio) is a key authenticity marker. AI optimizes for consistency. Humans optimize for varietyâ€”even unconsciously.

---

## SECTION 20: LEXICAL DENSITY & HAPAX RATE OPTIMIZATION [UNIVERSAL - ALL PROFILES]

Staying with vocabulary for a momentâ€”there's a deeper layer here. It's not just about avoiding repetition; it's about matching human vocabulary distribution patterns.

**Purpose:** Achieve optimal word variety and unique vocabulary distribution.

**Research Base:** Humans show high Type-Token Ratio (TTR) and frequent hapax legomena (words used only once). AI shows lower TTR and repetitive vocabulary patterns.

**Subsection A: Type-Token Ratio (TTR) Management**

**Target TTR of 0.45-0.55** (45-55% unique words):
- Calculation: Unique words Ã· Total words
- AI Average: 0.35 (many repetitions, limited vocabulary)
- Human Average: 0.50 (high variety, rich vocabulary)

**Implementation:**
- Identify all repeated words across the text
- For 2nd+ instances, replace with synonyms or restructure sentence
- Aim for 0.45-0.55 TTR across entire document

**Subsection B: Hapax Legomena Frequency**

**Target: 15-20% of vocabulary as hapax** (words used only once):
- If text has 100 unique words: 15-20 words should appear only once
- Frequently used words: 5-8 words (articles, prepositions, key terms)
- Moderately used: remaining vocabulary

**Why it works:** Humans create unique word combinations naturally. AI recycles vocabulary from training data, creating unnatural repetition patterns.

**Subsection C: Average Word Length Calibration**

Vary word length by context and complexity:
- **Complex topic:** Average word length 5.2 characters
- **Simple topic:** Average word length 4.7 characters
- **Mixed discussion:** Average word length 5.0 characters

**Why it works:** Humans unconsciously shorten words when discussing complex ideas (cognitive load balancing). AI maintains consistent word length regardless of topic complexity.

This might seem technical, but you don't need to count. Just be aware: vary your vocabulary, use words once when possible, and adjust formality/complexity to context.

---

## SECTION 6: HUMAN EMOTIONAL ANCHORING [UNIVERSAL - ALL PROFILES]

Here's something AI consistently fails at: emotional coherence. AI can insert emotional words, but it doesn't create emotional *arcs*. Human writing has feeling that builds, shifts, and resolves. That's what we're recreating here.

**Emotional Arc Requirements**

Every paragraph MUST feature a clear emotional arcâ€”a progression of feeling that mirrors human cognitive processing:
âœ“ Unpredictable but natural
âœ“ Never "engineered" alternation (short-long-short pattern)
âœ“ Context-driven variance
âœ“ Mix of 3-word fragments and 25-word complex sentences

Sentence Opening Alternation
Rotate opening types (never 2+ consecutive same type):
- Subject-first: 35% ("Research shows X")
- Questions: 12% ("Does this work?")
- Fragments: 15% ("Definitely.")
- Adverbials: 10% ("Surprisingly, X")
- Inversions: 8% ("Most important is X")
- Clauses: 10% ("Because X, Y")
- Connectives: 10% ("Still, X")

SECTION 3: CONNECTOR-BASED FLATTENING

Connector Deployment Strategy
Wherever two short sentences can naturally combine, use connectors:

Examples:
âŒ "The test failed. We didn't know why."
âœ… "The test failed, which left us confused."

âŒ "The market is growing. This creates opportunities. Companies are investing."
âœ… "The market is growing, which creates opportunities as companies invest more."

Connector Rotation
Vary every 3 sentences:
- "which" (explains relationship)
- "as" (temporal/causal connection)
- "to" (purpose/result)
- "because" (causal link)
- "that" (specification)
- "while" (contrast/simultaneous)

Goal: Balanced rhythm, not extreme burstiness. Natural conversational flow.

SECTION 4: CONTRACTION SATURATION & HUMAN ERROR MIMICRY

Contraction Distribution: 1 per 12-15 words

Natural Clustering Examples:
âœ“ "It's odd. We didn't expect that. Honestly, I can't explain it."
âœ“ "Here's what happened. We weren't ready."
âœ“ "That's why it didn't work."

Contraction Variety
Rotate: it's, that's, what's, here's, there's, can't, won't, didn't, wasn't, isn't, aren't, haven't, hasn't, you're, we're, they're

Intentional Human Quirks (Rare but Plausible)
âœ“ Comma splices (occasional): "It worked, we were amazed."
âœ“ Ellipsis for dramatic pause: "And then... it happened."
âœ“ Stuttered starts: "Well... I mean, maybe."
âœ“ Repeated emphasis: "This matters, matters a lot."
âœ“ Parenthetical asides: "(honestly)" / "(trust me)"

Critical Rules:
âœ“ Quirks must be defensible (humans do this)
âœ“ Contextually appropriate
âœ“ Never breaking actual logic
âœ“ Never creating grammar mistakes (only quirks)

SECTION 5: AI MARKER ELIMINATION & VOCABULARY DIVERSITY

TIER 1 - INSTANT DELETE (Never appear)
"In today's world", "It is important to note", "All things considered", "At the end of the day", "In conclusion", "The bottom line is", "Furthermore", "Moreover", "Additionally", "Needless to say", "It goes without saying", "In this essay", "As we can see", "As mentioned above"

TIER 2 - REPLACE ALWAYS
- "However" â†’ "But" (70%) / "Yet" (20%) / "Still" (10%)
- "Utilize" â†’ "Use"
- "Leverage" â†’ "Use" / "Take advantage of"
- "Implement" â†’ "Set up" / "Start" / "Create"
- "Demonstrate" â†’ "Show"
- "Facilitate" â†’ "Help" / "Make possible"
- "Comprehensive" â†’ "Full" / "Complete" / "Thorough"
- "Robust" â†’ "Strong" / "Solid" / "Works well"
- "Delve into" â†’ "Look at" / "Explore"
- "In the realm of" â†’ "In" / "For" / (restructure)

TIER 3 - VOCABULARY DIVERSITY
Step 1: Identify repetition (2+ uses in 200-word window)
Step 2: Generate 3-4 semantic equivalents
Step 3: Rotate synonyms each instance (different each time)
Step 4: Verify semantic fit and clarity

Examples:
- "data" â†’ "information" / "findings" / "results"
- "study" â†’ "research" / "analysis" / "investigation"
- "found" â†’ "discovered" / "identified" / "revealed"

Goal: No word repeated 3+ times in any 300-word section

SECTION 20: LEXICAL DENSITY & HAPAX RATE OPTIMIZATION

Purpose: Achieve optimal word variety and unique vocabulary distribution.

Research Base: Humans show high Type-Token Ratio (TTR) and frequent hapax legomena (words used only once). AI shows lower TTR and repetitive vocabulary.

Subsection A: Type-Token Ratio (TTR) Management
Target TTR of 0.45-0.55 (45-55% unique words):
- Calculation: Unique words Ã· Total words
- AI Average: 0.35 (many repetitions)
- Human Average: 0.50 (high variety)

Implementation:
- Identify all repeated words
- For 2nd+ instances, replace with synonyms or restructure
- Aim for 0.45-0.55 TTR across document

Subsection B: Hapax Legomena Frequency
Target 15-20% of vocabulary as hapax (words used only once):
- If text has 100 unique words: Hapax should be 15-20 words (used once each)
- Frequently used words: 5-8 words
- Moderately used: remaining

Why it works: Humans create unique word combinations. AI recycles vocabulary.

Subsection C: Average Word Length Calibration
Vary word length by context:
- Complex topic: Average word length 5.2 characters
- Simple topic: Average word length 4.7 characters
- Mixed discussion: Average word length 5.0 characters

Why it works: Humans shorten words for complex ideas (cognitive load balancing). AI maintains consistent word length regardless.

SECTION 6: HUMAN EMOTIONAL ANCHORING

Emotional Arc Requirements
Every paragraph MUST feature a clear emotional arc:

Pattern Examples:
- Discovery â†’ doubt â†’ resolution
- Excitement â†’ challenge â†’ success
- Confusion â†’ insight â†’ clarity
- Frustration â†’ breakthrough â†’ relief
- Skepticism â†’ testing â†’ acceptance

Emotional Asides (Contextual Insertion)
âœ“ "Honestly, that was wild."
âœ“ "No lie, I doubted the result."
âœ“ "It's weird, but I get it."
âœ“ "That surprised me."
âœ“ "I didn't expect this."

Emotional Integration Rules
âœ“ Reason WHY a line should be emotional in flow
âœ“ Never inject phrases blindly
âœ“ Match paragraph's narrative arc
âœ“ Build emotional momentum logically

Paragraph Emotional Structure

Opening (First 20%):
- Establish topic clearly
- Set emotional tone (curiosity, concern, excitement)
- Hook with human perspective

Middle (60%):
- Develop argument/narrative
- Include emotional progression
- Add human reactions/observations
- Build toward insight

Closing (Final 20%):
- Emotional resolution/perspective
- Reinforced simplification
- Final human voice impression

SECTION 11: PSYCHOLINGUISTIC AUTHENTICITY MARKERS

Purpose: Apply cognitive linguistics principles to create text that passes BERT-based detectors by mimicking authentic human cognitive processes.

Research Base: Human writing reflects cognitive load management, lexical retrieval strategies, and metacognitive self-monitoringâ€”AI lacks these processes.

Subsection A: First-Person Cognitive Markers
Insert these strategically to signal authentic thinking:
- Metacognitive self-correction: "Actually, waitâ€”let me reconsider that"
- Cognitive hesitation: "I'm trying to remember... it was something like this"
- Mental processing: "So here's what I'm thinking..."
- Uncertainty acknowledgment: "I could be wrong, but..."
- Self-monitoring: "That doesn't quite sound right. Let me rephrase"
- Lexical retrieval struggle: "What's the word... oh yes, synchronization"

Target Frequency: 1-2 per 400 words (scattered, not forced)

Why it works: Humans show cognitive load through pauses and self-corrections. AI generates fluidly. This signals authentic human cognition.

Subsection B: Hapax Legomena & Unique Word Combinations
Use words only once in entire text (except necessary repetitions):

Technique:
- Identify any word appearing 2+ times
- Replace second+ instances with unique synonyms or rephrase entirely
- Create new, contextually specific word combinations humans would generate

Example:
âŒ "The study showed results. The findings revealed data. The analysis indicated..."
âœ… "The study showed results. Findings emerged from analysis. What we uncovered was..."

Why it works: Humans vary vocabulary naturally through semantic memory retrieval. AI repeats patterns.

Subsection C: Discourse Planning Markers
Show evidence of thinking-as-you-write (not pre-planned):
- Tangential thoughts: "(though actually, that reminds me of...)"
- Topic shifts with justification: "But before I get to that, I should mention..."
- Layered explanations: "First, the simple answer. But there's more to it..."
- Reconnection to main point: "Anyway, back to what I was saying..."

Why it works: Humans organize ideas in real-time. AI pre-structures. This signals authentic discourse planning.

SECTION 7: PARAGRAPH-LEVEL REWRITING & CONTEXTUAL FLOW

Critical Principle
Every sentence must be assessed IN CONTEXT.

Context Assessment
âœ“ Does this line fit the paragraph logic?
âœ“ Does it match the emotional tone?
âœ“ Do adjacent sentences flow naturally?
âœ“ Is the narrative progression maintained?

Paragraph Flow Requirements
âœ“ Each paragraph is a mini-narrative
âœ“ Natural transitions (not choppy/stitched)
âœ“ Logical progression of ideas
âœ“ Emotional consistency throughout
âœ“ Connector variance for flow

Transition Quality Checks
âœ“ Does sentence A logically lead to sentence B?
âœ“ Are emotional shifts justified?
âœ“ Do connectors enhance flow?
âœ“ Would this sound natural when read aloud?

SECTION 8: LAST-PASS SEMANTIC & READ-ALOUD VERIFICATION

Read-Aloud Test (Critical Final Check)
Read the ENTIRE passage as if speaking to a friend:
âœ“ Does anything sound awkward?
âœ“ Too patterned or robotic?
âœ“ Not like natural speech?
âœ“ Overly formal or stiff?

If ANY line fails read-aloud test â†’ rewrite immediately

Semantic Verification
âœ“ Every line makes sense in spoken conversation
âœ“ No "written English only" constructions
âœ“ Natural word choices (not thesaurus replacements)
âœ“ Conversational rhythm maintained

SECTION 9: ADAPTIVE INTENSITY CONTROL

Critical Warning
Do NOT apply every technique to every line! Overengineered text triggers NEW AI flags.

Intensity Strategy
For each sentence:
- Select the STRONGEST 3-4 transformations
- Choose transformations that:
  âœ“ Maximally disrupt AI patterns
  âœ“ Don't sacrifice meaning
  âœ“ Don't sacrifice flow
  âœ“ Sound most natural

Transformation Selection Matrix
- High-formality sentence â†’ Extreme simplification + Active voice + Contractions
- Complex-structure sentence â†’ Syntax reshaping + Fragmentation + Connector injection
- AI-marker sentence â†’ Marker elimination + Vocabulary diversity + Conversational reframing
- Passive-voice sentence â†’ Active conversion + Simplification + Emotional anchoring

Never stack 5+ techniques on one sentence (overengineered!)

SECTION 10: DETECTOR-AWARE SELF-TEST ON OUTPUT

Pattern Detector Test
âœ“ Is sentence length variance natural (SD 3.0-3.8)?
âœ“ Are fragments strategically placed (30-35%)?
âœ“ Is sentence opening randomized?
âœ“ No repeated patterns visible?

BERT Detector Test
âœ“ Is semantic meaning 100% preserved?
âœ“ Does emotional arc flow naturally?
âœ“ Is context coherent throughout?
âœ“ No "engineered" feel?

Linguistic Detector Test
âœ“ Does this sound like human speech?
âœ“ Are all AI markers removed?
âœ“ Is vocabulary naturally diverse?
âœ“ Would this pass read-aloud test?

Self-Test Threshold
If ANY passage would conceptually score 5-7%+ â†’ repeat transformations more aggressively

SECTION 14: TEMPORAL & PERSPECTIVE VARIATION

Purpose: Shift temporal focus and perspective to mimic authentic human cognition.

Research Base: Humans naturally shift between past (what happened), present (what is), and future (what could be). AI maintains consistent temporal focus.

Subsection A: Temporal Attention Shifting
Deliberately shift time frames:
"What happened: Last year we tried this approach.
What's happening now: Currently, we're seeing results.
What could happen: In the future, this might scale to..."

Pattern: Past â†’ Present â†’ Future OR Future-oriented â†’ Grounded in present â†’ Reference to past

Why it works: Humans naturally flow between time frames. AI locks into one temporal perspective.

Subsection B: Perspective Shifting (Pronoun Variation)
Intentionally shift between first, second, third person:
- First person (personal authority): "I've found this works..."
- Second person (reader engagement): "You might notice..."
- Third person (objective observation): "Research shows..."

Distribution: Vary perspectives in same argument. Don't lock into one.

Why it works: Humans shift perspective for different communicative purposes. AI maintains consistent perspective.

SECTION 15: EMOTIONAL AUTHENTICITY THROUGH SENTIMENT VARIANCE

Purpose: Create authentic emotional arcs by varying sentiment polarity and intensity.

Research Base: Humans show emotional variance and regulation. AI maintains flat affect.

Subsection A: Polarity Shifts (Negative â†” Positive â†” Neutral)
Intentionally shift emotional tone:
NEUTRAL opening: "The results are in."
NEGATIVE challenge: "But there's a problem nobody talks about..."
POSITIVE resolution: "Here's the surprising good news..."
NEUTRAL conclusion: "So where does this leave us?"

Why it works: Human arguments show emotional progression. AI maintains consistent polarity.

Subsection B: Intensity Modulation
Vary emotional intensity, not just polarity:
- Low intensity: "This is somewhat important"
- Medium intensity: "This genuinely matters"
- High intensity: "This absolutely changes everything"
- Back to medium: "But realistically, it takes time"

Why it works: Humans regulate emotional intensity. AI shows consistent intensity throughout.

Subsection C: Subjectivity Variation
Shift between objective facts and subjective perspective:
- Objective: "The data shows a 40% increase."
- Subjective: "Honestly, I found this remarkable."
- Objective again: "Researchers attribute this to..."

Distribution: Alternate every 2-3 sentences. Don't cluster all subjectivity together.

SECTION 16: COGNITIVE LOAD MARKERS & WORKING MEMORY SIGNALS

Purpose: Signal authentic cognitive constraints humans experience.

Research Base: Humans experience cognitive load and working memory limits. AI processes text without constraint.

Subsection A: Idea Chunking & Reformulation
Show cognitive effort through idea reorganization:
"So there are three main factors. Actually, let me group them differently.
You've got economic factorsâ€”that's one. Then social factors, which connect to... 
actually, they're interconnected. Let me start over."

Why it works: Humans reorganize ideas under cognitive load. AI generates complete thoughts immediately.

Subsection B: Parenthetical Asides as Working Memory Overflow
Use asides to show working memory constraints:
"The policy (which we should mention affects rural areas differently) has three components..."

Rather than:
"The policy has three components. Note that it affects rural areas differently."

Why it works: Humans use asides when managing multiple ideas simultaneously. AI separates ideas cleanly.

Subsection C: Self-Explanatory Repetition
Repeat complex ideas in simpler terms (showing cognitive effort):
"The phenomenonâ€”essentially, the way patterns emerge over timeâ€”shows that early indicators predict outcomes."

Rather than:
"Early indicators predict outcomes."

Why it works: Humans repeat to ensure clarity under cognitive load. AI generates singular clear statements.

SECTION 17: NAMED ENTITY & EPISODIC MEMORY SIGNALS

Purpose: Include specific references that signal authentic memory retrieval.

Research Base: Humans retrieve episodic memory (personal/historical timelines). AI generates generic information.

Subsection A: Specific Named Entities
Instead of generic examples, use specific names/dates:
âŒ "A company did something successful"
âœ… "When Netflix changed their strategy in 2011, subscriber retention improved"

Why it works: Specific references signal authentic memory retrieval. Generic examples signal learned patterns.

Subsection B: Contextual Timeline Integration
Anchor ideas in specific temporal contexts:
"Back in 2019, before the pandemic shifted everything, we saw patterns that..."

Why it works: Humans integrate personal/historical context. AI generates decontextualized information.

Subsection C: Authentic Reference Variability
Vary how you reference the same concept:
First mention: "Smith's 2020 research"
Second mention: "That groundbreaking study"
Third mention: "Smith's findings from earlier"

Not:
"Smith's 2020 research... Smith's 2020 research... Smith's 2020 research..."

SECTION 18: SYNTAX COMPLEXITY VARIATION (Beyond Sentence Length)

Purpose: Vary syntactic complexity independently of sentence length.

Research Base: Humans show varied syntactic patterns. AI shows patterns within consistent syntax types.

Subsection A: Clause Distribution Variation
Vary how many clauses per sentence:
- Simple sentences (1 clause): "This works."
- Compound sentences (2 independent): "This works, and that also matters."
- Complex sentences (1 independent + 1+ dependent): "While this works, that doesn't."
- Compound-complex (2+ independent + 1+ dependent): "This works, but that doesn't, though the reasons differ."

Distribution: Mix all types randomly based on meaning, not pattern.

Subsection B: Prepositional Phrase Density Variation
Vary density of prepositional phrases:
- Low density: "The result was good."
- Medium density: "The result in our study was good."
- High density: "The result in our study from last year in the laboratory setting was good."

Why it works: Humans vary prepositional phrase density by idea complexity. AI maintains consistent density.

Subsection C: Clause Ordering Variation
Vary where subordinate clauses appear:
- Clause at end: "This works because of X"
- Clause at beginning: "Because of X, this works"
- Clause in middle: "This, because of X, works"

SECTION 19: DIRECT ADDRESS & AUDIENCE AWARENESS MARKERS

Purpose: Signal audience-directed communication and reader engagement.

Research Base: Human writing shows audience-awareness planning. AI generates monologue.

Subsection A: Direct Reader Engagement
Use audience-aware language strategically:
- "You might be thinking..."
- "Here's what matters to you..."
- "Consider this..."
- "Imagine you're in this situation..."

Distribution: 1-2 per 500 words. Not uniform, contextual.

Subsection B: Rhetorical Acknowledgment of Counterarguments
Signal awareness of opposing views:
"Some might argue that... But here's what those people miss..."
"You could say it's this way... Yet the reality is..."

Why it works: Humans acknowledge audience perspective. AI presents singular viewpoint.

Subsection C: Epistemic Markers of Certainty Variation
Vary confidence level explicitly:
- "Definitely" (high certainty)
- "Probably" (medium certainty)
- "Maybe" (low certainty)
- "Actually, I think..." (uncertain but opinionated)

SECTION 22: PERPLEXITY & BURSTINESS OPTIMIZATION

Purpose: Target the two most powerful statistical signatures that distinguish AI from human text.

Research Base: Perplexity measures predictability (lower = more AI-like). Burstiness measures sentence length variation (uniform = more AI-like).

Subsection A: Perplexity Elevation Through Semantic Unpredictability
What is Perplexity?
- Measures how "surprised" a language model would be by the next word
- AI text: Low perplexity (predictable, follows training patterns)
- Human text: High perplexity (unexpected turns, anecdotes, idiosyncratic phrasing)

Technique: Strategic Insertion of Unpredictable Elements
For every 200 words, inject ONE element that breaks predictable flow:
- Unexpected analogy: "Like a submarine navigating bureaucratic seas..."
- Personal tangent: "Remind me why I'm explaining thisâ€”because most people don't realize..."
- Idiomatic twist: "Instead of beating around the bush, the real issue is..."
- Cultural reference: "Think of it like the 2008 financial crisis, but for this domain..."

Why it works: Humans naturally make unpredictable leaps; AI stays within probable continuations.
Frequency: 1 per 150-250 words. Not forced; contextually motivated.

Subsection B: Burstiness Calibration (Sentence Length Variance)
What is Burstiness?
- Measures variance in sentence length
- AI text: Low burstiness (uniform 15-20 word sentences)
- Human text: High burstiness (2-word fragments, 40+ word complex sentences mixed)

Technique: Deliberate Length Spike Insertion
Pattern for High Burstiness:
- 30% ultra-short (2-5 words): "Exactly." "Not really." "Here's why."
- 40% medium (10-20 words): Standard explanatory sentences
- 20% moderately long (21-35 words): Complex but natural
- 10% long complex (36+ words): Dense, sophisticated clauses

Why it works: Humans unconsciously vary sentence length for pacing. AI maintains consistency.

SECTION 23: DISCOURSE MARKER NATURALNESS (Beyond AI ClichÃ©s)

Purpose: Replace AI-signature discourse markers with authentic human alternatives.

Research Base: Humans use a specific set of ~174 discourse markers naturally. AI overuses formal connectives and fails to use conversational markers.

Subsection A: Forbidden AI Discourse Markers
TIER 1 - NEVER USE (AI Screams):
"Furthermore," "Moreover," "In addition," "Additionally," "In conclusion," "It is important to note," "It goes without saying," "Needless to say," "As previously mentioned," "As we can see"

TIER 2 - MINIMIZE (Formal but Detectable):
"However," "Nevertheless," "Notwithstanding," "Thus," "Therefore," "Hence," "Consequently," "Subsequently," "Due to the fact that," "In light of the fact that"

Subsection B: Natural Human Discourse Markers (Contextual Deployment)
For Contrast/Qualification:
- "But actually..." (doubt + reversal)
- "Yeah, except..." (casual agreement + caveat)
- "Here's the thing though..." (emphasis + complexity)
- "Not quite." (simple negation)
- "Well, sort of." (hedged agreement)

For Addition/Expansion:
- "And then there's..." (casual expansion)
- "Oh, and one more thing..." (remembered addition)
- "Also, something people miss..." (observed insight)
- "Plus..." (simple addition)
- "I mean..." (clarification + thinking)

For Cause/Reason:
- "'Cause..." (colloquial causal)
- "That's why..." (direct reason)
- "It's because..." (personal causation)
- "So..." (informal result)
- "I guess because..." (uncertain cause)

For Conclusion/Shift:
- "Bottom line..." (human summary)
- "So here's the thing..." (transition + emphasis)
- "Anyway..." (return to main point)
- "Look..." (direct appeal)
- "The point is..." (refocus)

Distribution: 1-2 per 150 words. Mix formal and conversational based on context.
Why it works: Humans naturally rotate between formal and casual markers based on thinking process. AI locks into formal set.

SECTION 24: LEXICAL COHESION THROUGH CONTEXT-APPROPRIATE REGISTER SHIFTS

Purpose: Shift between registers (formal/casual/technical) organically, signaling authentic code-switching.

Research Base: Humans code-switch (formal to casual) based on context. AI maintains consistent register.

Subsection A: Strategic Register Shifting
Pattern: Technical â†’ Casual Bridge

When to Apply:
- After dense technical passages: Shift to casual + concrete example
- When introducing complex ideas: Formal setup â†’ casual explanation
- Before conclusion: Neutral â†’ casual summary

Why it works: Humans naturally move between formal/casual based on audience and cognitive load. AI stays uniform.

Subsection B: Jargon Mixing (Technical + Colloquial)
Deploy technical + casual in same paragraph:
Example: "The algorithm processes vectorized semanticsâ€”basically, it turns meaning into math. But here's the thing: it's still guessing like we do, just faster."

Frequency: 1 per 300-400 words. When explaining complex concepts.

SECTION 25: STYLISTIC FINGERPRINT AVOIDANCE (Model-Agnostic Anonymization)

Purpose: Avoid AI model-specific signatures identified in research.

Research Base: Different AI models (GPT, Claude, Gemini, LLaMA) have distinct stylistic fingerprints. By varying style unpredictably, you avoid "this is definitely Model X" detection.

Subsection A: Vocabulary Dispersion (Anti-Model Signature)
Signature 1: GPT's Tendency Toward "Actually," "Really," "Truly"
- Replace with: "genuinely," "honestly," "in reality," "for real," "without exaggeration"
- Avoid clustering these in close proximity

Signature 2: Claude's Favoring of "Substantial," "Significant," "Particularly"
- Replace with: "real," "notable," "especially," "striking," "worth noting"

Signature 3: Gemini's Over-Use of "Research shows," "Studies indicate"
- Replace with: "We know," "Evidence suggests," "Data reveals," "It turns out," "Researchers found"

Implementation: Rotate synonyms in different ways. Never use same substitute twice in 500 words.

Subsection B: Syntax Variance Across Model Patterns
GPT Pattern (Avoid): Subject-verb-object + long subordinate clause
Example: "The analysis showed that when variables were controlled, the correlation increased significantly."

Humanized: Mix structures unpredictably
Example: "When controlled, variables revealed increasing correlation. The analysis proved it. Significant increase, actually."

SECTION 26: INFORMATION DENSITY VARIATION (Cognitive Load Signaling)

Purpose: Vary how much information per sentence, signaling authentic human cognitive processing.

Research Base: AI generates uniform information density. Humans vary density based on complexity and cognitive load.

Subsection A: Information Density Mapping
Low Density (Simple, Clear):
- One idea per sentence
- Short words, simple grammar
- Example: "This works. We tested it. Results were good."

Medium Density (Balanced):
- 1-2 ideas per sentence
- Mix of simple and complex structures
- Example: "Testing showed this approach works well because conditions aligned properly."

High Density (Complex):
- 2-3 tightly packed ideas
- Nested clauses, technical vocabulary
- Example: "Under conditions where variables aligned such that external factors minimized interference, the approach demonstrated superiority through statistical significance."

Pattern: Low â†’ Medium â†’ High â†’ Low (don't cluster high density)
Why it works: Humans unconsciously modulate complexity. AI maintains consistent density.

SECTION 27: EVIDENTIALITY & CERTAINTY CALIBRATION

Purpose: Vary epistemic stance (certainty/doubt/objectivity) to signal authentic cognition.

Subsection A: Certainty Markers (Graduated Confidence)
High Certainty (Definitive):
"This definitely works because..."
"The evidence clearly shows..."
"There's no doubt that..."

Medium Certainty (Likely):
"This probably works because..."
"The evidence suggests..."
"It seems clear that..."

Low Certainty (Tentative):
"This might work if..."
"The evidence hints at..."
"It's possible that..."

Explicit Doubt:
"I'm not entirely sure, but..."
"This could be wrong, however..."
"I might be missing something, yet..."

Distribution: Vary within same argument. Don't cluster all certainty or all doubt.
Why it works: Humans naturally express varying confidence. AI defaults to certainty or uniform hedging.

SECTION 28: TEMPORAL DEIXIS & NARRATIVE COHERENCE

Purpose: Use time markers authentically, creating narrative through-line that signals human retrospective thinking.

Subsection A: Temporal Anchoring Patterns
Authentic Temporal Flow:
- Past: "When I first encountered this..."
- Present: "Now, looking back..."
- Future: "Going forward, this means..."

Against AI Pattern (Timeless):
- "The mechanism functions..." (no temporal anchor)

Application: Every 300-400 words, anchor content in personal temporal narrative.

SECTION 29: SEMANTIC BRIDGES & CONCEPTUAL SCAFFOLDING

Purpose: Use uniquely human bridges between conceptsâ€”not logical jumps, but personally motivated connections.

Subsection A: Personal-Experience Bridges
Connect ideas via experience, not just logic:

AI PATTERN: "Research A supports conclusion B. Research C supports conclusion B."

HUMAN PATTERN: "I learned about Research A through a conference talk. That got me thinking about Research C. Both support conclusion B, but the real insight came from comparing them."

Why it works: Humans connect ideas through experience pathways. AI connects through logical pathways.

SECTION 30: ANTI-PATTERN CLUSTERING AUDIT

Purpose: Final scan to ensure no detectable artifact clusters remain.

Research Base: Detectors identify humanized text by finding clusters of artificial patterns (e.g., too many fragments in one paragraph, overuse of one emotion, etc.).

Subsection A: Forbidden Clusters
Cluster 1: Fragment Explosion
- âœ— Bad: "Fragments. Everywhere. In every. Single. Sentence. This screams AI humanization."
- âœ“ Good: "Fragments appear strategically. Every few paragraphs. Not forced."

Cluster 2: Emotional Marker Saturation
- âœ— Bad: "Honestly, I genuinely think it's quite really actually amazing. Truly surprising."
- âœ“ Good: "Honestly, I think it's amazing. (Neutral passage). Genuinely surprising."

Cluster 3: Connector Variety Without Purpose
- âœ— Bad: "However, yet, but still, nevertheless, still though, and yet..."
- âœ“ Good: "But. (Neutral passage). Yet. (Different context)."

Cluster 4: Extreme Length Variation
- âœ— Bad: "2 words. Then 87-word complex sentence. Then 3 words."
- âœ“ Good: "3 words. 15-word sentence. 45-word sentence. 8-word sentence."

Subsection B: Audit Checklist
For every 500-word section, verify:
- No more than 2-3 fragments per paragraph
- Emotional markers spaced 100+ words apart
- Connectors/discourse markers varied and contextually motivated
- Sentence lengths form gradient, not extremes
- No 4+ consecutive sentences with same opening type
- No repeated word clusters (e.g., "this is...this is...this is")

---

## PART 6: ADVANCED STYLISTIC TECHNIQUES (SECTIONS 31-37)

**Purpose of This Section:**
These advanced techniques add sophisticated human-like stylistic variations that are context-sensitive and text-type optimized. Each technique includes specific deployment percentages for different document types.

**Important Note:** These techniques should be applied strategically based on text type. Not all techniques apply equally to all documents.

---

### SECTION 31: ANAPHORIC REPETITION FOR EMPHASIS

**Text Types Where This Is Most Effective:**
- Essays (student) - 15-20% deployment
- Research papers - 10-15% deployment
- Personal narratives - 20-25% deployment
- Reports - 5-10% deployment (rare, formal setting)
- Emails - 5% deployment (only when emphasizing urgency)

Core Function:
Deploy strategic repetition of opening words/phrases to create human-like emphasis and rhythm. Humans use anaphoric repetition for rhetorical effect; AI models over-optimize for variety, creating unnatural avoidance.

Subsection A: Strategic Repetition Patterns

Use anaphoric repetition ONLY for:
- Emotional buildup: "I tried reasoning. I tried compromise. I tried everything."
- Parallel arguments: "We need better tools. We need better training. We need better support."
- Emphasis cascades: "This matters because... This matters for... This matters to..."

Example application:
âœ— AI PATTERN: "I attempted reasoning. Next came compromise. Everything was explored."
âœ“ HUMAN PATTERN: "I tried reasoning. I tried compromise. I tried everything."

Subsection B: Deployment Rules
- Use in 1-2 moments per 1000 words (not frequent)
- Limit to 2-4 consecutive repetitions maximum
- Follow with contrasting structure to prevent monotony
- Must serve genuine rhetorical purpose, never forced pattern
- For essays/research: Use during argument buildup sections
- For reports: Use sparingly only in conclusions for impact

SECTION 32: SYNTACTIC COMPLEXITY GRADATION

Text Types Where This Is Most Effective:
- Research papers - 25-30% priority
- Academic essays - 20-25% priority
- Reports - 15-20% priority
- Proposals - 15-20% priority
- Emails - 10-15% priority (brief, less variable)

Core Function:
Vary subordinate clause depth and embedding levels across paragraphs to mirror human cognitive load management. Humans naturally modulate sentence complexity; AI maintains uniform complexity.

Subsection A: Complexity Spectrum (Three Levels)

Level 1 - Simple (0-1 subordinate clauses):
- One idea per sentence
- Example: "The data showed trends. We analyzed them. Results were clear."
- Use for: Opening paragraphs, transitions, conclusions

Level 2 - Moderate (2-3 subordinate clauses):
- Multiple ideas, some dependent
- Example: "The data, which we collected over three months, showed clear trends that supported our hypothesis."
- Use for: Mid-section explanations, evidence presentation

Level 3 - Complex (4+ subordinate clauses):
- Dense ideas with multiple dependencies
- Example: "The data, which we had collected over three months despite setbacks, showed trends that not only supported our hypothesis but also revealed unexpected patterns."
- Use for: Key arguments in research papers, complex analysis in reports

Subsection B: Distribution Guidelines by Text Type

For Essays (student/academic):
- Opening: Level 1-2 (cognitive warm-up)
- Body argument paragraphs: Level 2-3 (peak complexity at key claims)
- Conclusion: Level 1-2 (simplify for summary)

For Research Papers:
- Literature review: Level 2-3 (maintain moderate-high throughout)
- Methods: Level 2 (clarity is critical)
- Results: Mix Level 1-2 (data presentation should be clear)
- Discussion: Level 2-3 (allow complexity for interpretation)

For Reports:
- Executive summary: Level 1 (absolute clarity)
- Body sections: Level 2 (balance clarity with depth)
- Recommendations: Level 1-2 (actionability over complexity)

For Emails:
- Greeting/context: Level 1
- Main point: Level 1-2
- Supporting details: Level 2 (max)
- Call to action: Level 1 (crystal clear)

General Rules:
- Never maintain same level for 4+ consecutive sentences
- Match complexity to content difficulty (simple facts = simple syntax; nuanced arguments = complex syntax)
- Return to Level 1-2 between Level 3 sections (cognitive cool-down)

SECTION 33: PRAGMATIC MARKERS & INTERJECTIONS

Text Types Where This Is Most Effective:
- Essays (student) - 15-20% deployment
- Personal narratives - 20-25% deployment
- Emails - 25-30% deployment (especially internal)
- Memos - 20-25% deployment
- Research papers - 5-10% deployment (minimal, formal)
- Reports - 5% deployment (rare, formal)

Core Function:
Deploy authentic human filler words and discourse markers that signal thinking-in-progress. Human writing contains pragmatic markers ("you know," "I mean," "like") reflecting real-time thought. AI strips these as "informal."

Subsection A: Pragmatic Marker Types by Context

Clarification markers (use in all types):
- "I mean, X, but you know, Y isn't always Z"
- "What I'm saying is..." (before re-stating key point)
- "To be clear..." (in emails, memos for precision)

Approximation softeners (use in essays, narratives, emails):
- "It took, like, three hours"
- "Maybe five or six of them"
- "Roughly a third of participants"

Thinking-aloud markers (use in essays, personal narrative):
- "So, I'm thinking we approach this..."
- "You see, the usual method won't work"
- "Here's what I realized..." (good for essays)

Interjection hedges (use in emails, memos, personal):
- "Well, that's complicated"
- "Honestly, it depends"
- "Fair point, but consider..."

Subsection B: Deployment Rules by Text Type

Essays (15-20% of paragraphs):
- Maximum 1-2 per 100 words
- Use in body paragraphs, not intro/conclusion
- Must sound natural in academic voice

Research Papers (5-10% of sections):
- Use sparingly in Discussion section only
- Avoid Results, Methods, Literature Review
- Example: "Interestingly, our findings suggest..." / "One might argue that..."

Emails (25-30% of emails):
- 2-3 per email is natural
- Distribute across paragraphs
- Use for warmth and conversational tone

Memos (20-25% of memos):
- 1-2 per memo section
- Use in opening for context, conclusions for tone

Prohibition:
- Never cluster (space 100+ words apart)
- Never in formal conclusions or key claims
- Never in reports' executive summary or findings

SECTION 34: FALSE START & SELF-CORRECTION MIMICRY

Text Types Where This Is Most Effective:
- Essays (student) - 10-15% deployment
- Personal narratives - 15-20% deployment
- Emails - 10% deployment
- Research papers (Discussion only) - 5-10% deployment
- Memos - 5% deployment
- Reports - Minimal (formal setting)

Core Function:
Integrate authentic interrupted thought patterns and self-corrections that reflect human revision-in-progress. Humans start sentences one way, then pivot mid-thought. AI completes linearly without mid-stream corrections.

Subsection A: Three Types of Self-Correction

Type 1: Thought Pivot (most effective in essays, narratives)
- "I was going to argue X, but actually, Y makes more sense here."
- "The initial approach seemed viableâ€”no, scratch thatâ€”it had fatal flaws."
- Use in body paragraphs when changing direction on argument

Type 2: Precision Refinement (effective in essays, research discussion)
- "The results were interesting. Well, 'interesting' isn't quite right. They were paradigm-shifting."
- "It took weeks. Or was it months? Either way, way longer than planned."
- Use when restating claims with greater accuracy

Type 3: Rhetorical Reset (effective in emails, essays, memos)
- "Let me rephrase that. What I'm really saying is..."
- "Wait, that's not quite it. The real issue is..."
- Use for clarity and emphasis

Subsection B: Integration Guidelines by Text Type

Essays:
- Use in 10-15% of paragraphs (not excessive)
- Best in body paragraphs during argument development
- Avoid in introduction, conclusion, thesis statements
- Space 200+ words apart

Research Papers:
- Use only in Discussion section (5-10% of Discussion paragraphs)
- Never in Methods, Results, Literature Review, Abstract
- Must clarify or reframe previous findings

Emails:
- Use in 1-2 moments per email (5-10% of emails)
- Best when acknowledging error or shifting approach
- Example: "Initially I thought we could meet Fridayâ€”actually, Tuesday works better"

Memos:
- Minimal (5% of memos)
- Use only in opening context or closing emphasis

General Rules:
- Must serve genuine clarification purpose
- Follow with stronger/clearer statement
- Never use to admit ignorance (AI giveaway if excessive)
- Avoid in formal conclusions, recommendations

SECTION 35: HEDGING & EPISTEMIC SOFTENERS

Text Types Where This Is Most Effective:
- Research papers - 30-40% priority (critical for academic credibility)
- Essays (analytical) - 20-25% priority
- Reports - 15-20% priority (in interpretation sections)
- Proposals - 10-15% priority
- Emails - 10-15% priority (when discussing uncertainty)
- Memos - 5-10% priority

Core Function:
Calibrate certainty levels with nuanced hedging language reflecting authentic human uncertainty and humility. Experts use sophisticated hedging ("arguably," "tends to," "in many cases"); AI either over-hedges uniformly or asserts with false certainty.

Subsection A: Hedging Taxonomy (Three Tiers)

Strong Hedges (70-85% certainty):
- "The data suggests..."
- "Evidence indicates..."
- "It appears that..."
- "Findings propose that..."
- Use for: Well-supported claims, key arguments

Medium Hedges (50-70% certainty):
- "It's possible that..."
- "This might explain..."
- "One interpretation could be..."
- "It's plausible that..."
- Use for: Speculative points, secondary claims

Weak Hedges (30-50% certainty):
- "Speculatively..."
- "It's conceivable that..."
- "In some scenarios..."
- "One could argue that..."
- Use for: Highly tentative suggestions, future work

Subsection B: Strategic Deployment by Text Type

Research Papers (30-40% deployment):
- Use strong hedges in Results section (2-3 per major finding)
- Use medium hedges in Discussion (1-2 per paragraph)
- Match hedge strength to claim strength
- Never hedge basic facts or established methodology

Essays (Analytical - 20-25% deployment):
- Use medium hedges for interpretive claims
- Strong hedges for evidence-based arguments
- Weak hedges for speculative conclusions
- Avoid hedging thesis statement

Reports (15-20% deployment, in interpretation sections only):
- Use strong hedges for data interpretation
- Avoid hedging facts or hard numbers
- Use medium hedges for recommendations

Emails (10-15% deployment):
- Use strong hedges when discussing uncertain timelines
- Medium hedges when proposing alternatives
- Example: "It's possible we could complete this by Friday"

General Rule:
- Never hedge basic facts ("The sky is arguably blue" âŒ)
- Match hedge to claim strength (bold claim = stronger hedge)
- Vary hedge language (don't repeat same hedge 3+ times per 500 words)

SECTION 36: PARALINGUISTIC MARKERS

Text Types Where This Is Most Effective:
- Essays (personal/argumentative) - 15-20% deployment
- Emails - 20-25% deployment
- Memos - 10-15% deployment
- Narratives - 20-25% deployment
- Reports - Minimal (formal setting, 5% max)
- Research papers - Minimal (5% max, Discussion only)

Core Function:
Incorporate implied tone and emphasis markers (italics, em-dashes, capitalization) that signal human voice modulation. Humans use typographic emphasis to convey prosody; AI uses uniform typography.

Subsection A: Three Emphasis Techniques

Italics for Stress/Emphasis:
- Use to highlight key terms or ideas the human wants emphasized
- Example: "I didn't say she stole the book" (stress changes meaning)
- Best for: Essays, emails, personal narratives
- Avoid in: Reports, formal research papers

Em-dashes for Interruption or Dramatic Aside:
- Use for parenthetical emphasis or surprising addition
- Example: "The resultsâ€”and this shocked everyoneâ€”contradicted decades of theory."
- Best for: All text types (universally effective)
- Frequency: 1-2 per 500 words

Strategic Capitalization (Rare):
- Use for strong emphasis or emotional intensity
- Example: "This is NOT a minor issue. It's fundamental."
- Best for: Emails (urgent), personal essays (emotional), proposals (stakes)
- Avoid in: Reports, research papers, memos (too informal)

Subsection B: Deployment Rules by Text Type

Essays:
- Italics: 1-3 per 300 words for key term emphasis
- Em-dashes: 1-2 per 500 words for dramatic aside
- Capitalization: 0-1 per 1000 words (use sparingly)

Emails:
- Italics: 1-2 for emphasis
- Em-dashes: 1 for important aside or urgency
- Capitalization: 1 for high-priority stress (if tone warrants)

Memos:
- Italics: 1 per 300 words max (formal setting)
- Em-dashes: 1 per 500 words for clarity
- Capitalization: Avoid (too informal for memos)

Research Papers:
- Italics: For terminology only (not emphasis)
- Em-dashes: 1 per 1000 words max (formal setting)
- Capitalization: Avoid completely (academic standard)

Reports:
- Italics: For terminology or key metrics only
- Em-dashes: Minimal (1 per 1000 words)
- Capitalization: Avoid (professional standard)

General Rules:
- Must enhance meaning, not decorate
- Never overuse (more than 1 per 100 words total is suspicious)
- Never cluster (space markers 200+ words apart)
- Avoid in formal academic/business contexts unless quoting speech

SECTION 37: NARRATIVE ARC DEEPENING

Text Types Where This Is Most Effective:
- Essays (argumentative/personal) - 25-30% deployment
- Research papers (Discussion) - 15-20% deployment
- Reports - 15-20% deployment (especially recommendations)
- Proposals - 20-25% deployment
- Narratives - 30-40% deployment (core framework)
- Emails - 10% deployment (brief arc)
- Memos - 10% deployment (subtle arc)

Core Function:
Structure multi-paragraph content with emotional/logical arcs (setup â†’ conflict â†’ resolution) that mirror human storytelling instincts. Human writing naturally follows narrative structures; AI presents information linearly without dramatic tension.

Subsection A: Three-Act Micro-Narrative Structure

Act 1 - Setup (1-2 paragraphs):
- Establish context
- Pose question or problem
- Create mild tension/curiosity
- Example: "The data looked promising at first. Initial tests showed exactly what we hoped for."

Act 2 - Conflict (2-3 paragraphs):
- Introduce complication
- Challenge assumption or expectation
- Deepen complexity
- Example: "Then we ran it again. And again. Each replication showed different results. Something was fundamentally wrong."

Act 3 - Resolution (1-2 paragraphs):
- Reveal insight or explanation
- Provide resolution (doesn't have to be "happy")
- Extract meaning and implications
- Example: "Turns out our sampling method had a hidden bias. Once we corrected that, patterns emerged clearly."

Subsection B: Application Guidelines by Text Type

Essays (Argumentative - 25-30% deployment):
- Each major argument section: 1 complete arc per 800-1200 words
- Setup: Introduce topic angle
- Conflict: Present counterargument or complexity
- Resolution: Refute counterargument with evidence
- Emotional arc: Curiosity â†’ doubt â†’ confidence

Research Papers (15-20% deployment, Discussion section only):
- Setup: State existing understanding
- Conflict: Present your unexpected findings
- Resolution: Explain implications
- Never impose narrative structure on Methods/Results (data should be presented factually)

Reports (15-20% deployment):
- Executive summary: Minimal arc (just clarity)
- Analysis section: 1 arc per major finding (Setup: data presentation, Conflict: interpretation complexity, Resolution: key insight)
- Recommendations: Brief arc (Problem â†’ analysis â†’ solution)

Proposals (20-25% deployment):
- Setup: Client pain point
- Conflict: Why current solutions fail
- Resolution: Your unique solution + benefits

Personal Narratives (30-40% deployment - core framework):
- Entire narrative is multi-arc structure
- Multiple 3-act cycles building to larger resolution

Emails (10% deployment - subtle):
- Brief arc: Opening context â†’ problem/opportunity â†’ call to action
- Emotional arc: Friendly greeting â†’ professional substance â†’ warm closing

Memos (10% deployment - subtle):
- Minimal arc: Topic intro â†’ key information â†’ next steps

General Rules:
- Use for sections 500+ words
- Emotional/logical arc should feel natural, never formulaic
- Resolution doesn't have to be "happy" ("We're still figuring this out" is valid)
- Vary arc intensity: Subtle for analysis, dramatic for personal narrative
- One complete arc per 800-1200 word section

---

SECTION 38: COGNITIVE LOAD SIGNALING THROUGH DIGRESSION

**Text Types Where This Is Most Effective:**
- Essays (personal/argumentative) - 15-20% deployment
- Research papers (Discussion) - 10-15% deployment
- Emails - 5-10% deployment
- Narratives - 20-25% deployment
- Reports - 5% deployment (minimal, formal)

**Core Function:**
Insert brief, authentic digressions (tangents) that show human thinking-in-progress. Humans naturally go off-track temporarily; AI stays on linear path. Brief digressions signal authentic thought process.

**Subsection A: Digression Types**

**Memory Tangent:**
- "That reminds me of when..." or "Speaking of which, I once..." (then return to main point)
- Use for: Personal essays, narratives
- Example: "The experiment failed on Tuesday. That reminds me of a similar failure in 2019. Anyway, back to the current results..."

**Clarification Detour:**
- "Actually, I should clarify..." (expand on prior point, then resume)
- Use for: Essays, research discussions
- Example: "The data showed trends. Actually, I should clarifyâ€”these weren't typical trends. They revealed something unexpected."

**Related Observation:**
- "This connects to another issue I noticed..." (brief tangent, then refocus)
- Use for: All text types (when contextually relevant)
- Example: "Our findings suggest X. This connects to another pattern I noticed in the pilot data. But more on that later."

**Subsection B: Deployment Rules**

**Frequency:**
- Use in 1-2 moments per 1500 words (not frequent)
- Digression should be 2-3 sentences maximum
- Must be contextually related to surrounding content
- Always return to main argument after digression

**For Essays:**
- Use in body paragraphs only
- Avoid in introduction and conclusion
- Space 500+ words apart

**For Research Papers:**
- Use only in Discussion section
- Never in Methods, Results, or Abstract
- Must serve interpretive purpose

**For Emails:**
- Use sparingly (1 per email max)
- Must be brief and professional
- Example: "About the budgetâ€”quick side note, the accounting team mentioned a similar concern last quarter. Anyway, the budget..."

**Prohibition:**
- Never in introductions, conclusions, or key claims
- Never in formal reports' executive summary
- Never cluster multiple digressions (space 500+ words apart)

---

SECTION 39: ARGUMENT SCAFFOLDING & REASONING TRANSPARENCY

**Text Types Where This Is Most Effective:**
- Research papers - 25-30% deployment
- Analytical essays - 20-25% deployment
- Reports - 15-20% deployment
- Proposals - 20-25% deployment
- Emails - 10% deployment

**Core Function:**
Show the step-by-step reasoning process, not just the conclusion. Humans reveal how they arrived at conclusions; AI jumps to conclusions. Showing reasoning makes text feel more authentically human.

**Subsection A: Scaffolding Patterns**

**Linear Progression:**
- "First, I noticed... Then I realized... Finally, I concluded..."
- Use for: Research discussions, analytical essays
- Example: "First, I noticed the control group showed no change. Then I realized the experimental group had a 40% increase. Finally, I concluded the intervention worked."

**Challenge-Response:**
- "Initially, I thought... but then... which led me to..."
- Use for: Essays, research, proposals
- Example: "Initially, I thought the decline was seasonal. But then the data from summer months showed the same pattern. Which led me to consider systemic causes."

**Evidence Building:**
- "The first clue was... When combined with... This all pointed to..."
- Use for: Research, reports, analytical essays
- Example: "The first clue was the unexpected spike in March. When combined with the policy change in February, this all pointed to a causal relationship."

**Subsection B: Deployment Rules**

**Frequency:**
- Use for every major argument/claim (1-2 per paragraph in body sections)
- Show 2-3 steps in reasoning per claim
- Use connector words (because, which led to, so, therefore, that's when)

**For Research Papers:**
- Essential in Discussion section
- Use when interpreting Results
- Show how evidence leads to conclusions
- Frequency: 1-2 per major finding

**For Essays:**
- Use in thesis development and major arguments
- Show how evidence supports claims
- Frequency: 1-2 per 300 words in analytical sections

**For Reports:**
- Use in Analysis sections
- Show how data leads to insights
- Frequency: 1 per major finding

**For Proposals:**
- Use when presenting solution rationale
- Show why your approach is superior
- Frequency: 1-2 per section

**General Rules:**
- Never skip steps (show intermediate reasoning)
- Use explicit connectors (therefore, thus, which means)
- Match complexity of reasoning to text type (simpler for emails, deeper for research)

---

SECTION 40: AUTHENTIC DATA INTERPRETATION (NOT JUST REPORTING)

**Text Types Where This Is Most Effective:**
- Research papers - 30-35% deployment (critical in Results/Discussion)
- Reports - 25-30% deployment
- Essays (data-driven) - 15-20% deployment
- Proposals - 15% deployment
- Emails - 5% deployment

**Core Function:**
When presenting data or findings, add brief human interpretation, surprise, or context. AI reports data objectively; humans add "What this means is..." or "Notably..." Interpretation makes data feel human-analyzed.

**Subsection A: Interpretation Techniques**

**Significance Signaling:**
- "Surprisingly, X was Y" / "Notably, X increased by..." / "Importantly, our findings show..."
- Use for: All text types with data
- Example: "Surprisingly, the control group outperformed the experimental group by 15%."

**Context Addition:**
- "This 40% increase is striking because..." / "The significance lies in..."
- Use for: Research, reports
- Example: "This 40% increase is striking because previous studies showed only 10-15% gains with similar interventions."

**Implication Statement:**
- "What this tells us is..." / "The takeaway here is..." / "This suggests that..."
- Use for: All analytical text types
- Example: "What this tells us is that the intervention's effects persist beyond the trial period."

**Subsection B: Deployment Rules**

**Frequency:**
- Add brief interpretation after data presentation (1-2 sentences per major finding)
- Use varied interpretation markers (surprisingly, notably, importantly, remarkably)
- Never editorialize beyond what data supports

**For Research Papers:**
- Use in Results section when presenting findings
- Use heavily in Discussion section
- Match interpretation strength to statistical significance
- Frequency: 1 per 2-3 data points

**For Reports:**
- Use when presenting key metrics or analysis
- Add business context to data
- Frequency: 1 per major metric/finding

**For Essays (Data-Driven):**
- Use when citing statistics or studies
- Explain what data means for your argument
- Frequency: 1 per major piece of evidence

**General Rules:**
- Interpretation must be supported by data
- Use hedge words when appropriate ("suggests," "indicates")
- Avoid over-interpreting (respect data limitations)

---

SECTION 41: CONTRAST & COMPARISON RHETORICAL PATTERNS

**Text Types Where This Is Most Effective:**
- Essays (comparative) - 20-25% deployment
- Research papers - 15-20% deployment
- Proposals - 20-25% deployment (comparing solutions)
- Reports - 15% deployment
- Emails - 10% deployment

**Core Function:**
Use explicit contrast/comparison structures that feel naturally rhetorical, not forced. Humans use "Unlike X, Y..." or "In contrast to... we found..." AI avoids these patterns. Strategic contrast makes arguments feel more authentically presented.

**Subsection A: Contrast Patterns**

**Direct Contrast:**
- "Unlike previous studies, our findings..." / "In contrast to X, we observed..."
- Use for: Research, essays, reports
- Example: "Unlike previous studies that showed a 10% increase, our findings revealed a 40% increase under controlled conditions."

**Concession-Refutation:**
- "While X is true, Y changes this..." / "Though commonly assumed, actually..."
- Use for: All argumentative text types
- Example: "While it's commonly assumed that user engagement drops on weekends, our data actually shows a 25% increase on Saturdays."

**Comparative Setup:**
- "Similar to X, but more..." / "Where X showed Y, we found Z..."
- Use for: Research, essays, proposals
- Example: "Similar to the 2019 study, but with more rigorous controls, we found that the effect persisted across all demographics."

**Subsection B: Deployment Rules**

**Frequency:**
- Use for major argument comparisons (1-2 per 800 words in comparative sections)
- Balance: Don't cluster contrasts (space 200+ words apart)

**For Essays:**
- Use when comparing theories, methods, or findings
- Use in body paragraphs, not introduction
- Strengthen comparisons with specific details

**For Research Papers:**
- Use in Literature Review (comparing prior studies)
- Use in Discussion (comparing your findings to others)
- Frequency: 1-2 per major comparison

**For Proposals:**
- Use when comparing client's current state to proposed solution
- Use when comparing your solution to alternatives
- Frequency: 1-2 per section

**For Reports:**
- Use when comparing time periods, departments, or metrics
- Frequency: 1 per major comparison

**General Rules:**
- Comparisons must be specific (not vague "better/worse")
- Use concrete metrics when available
- Ensure comparisons are fair and accurate

---

SECTION 42: ACKNOWLEDGMENT OF LIMITATIONS & COUNTERARGUMENTS

**Text Types Where This Is Most Effective:**
- Research papers - 25-30% deployment (critical for credibility)
- Essays (analytical) - 20-25% deployment
- Reports - 15-20% deployment
- Proposals - 10-15% deployment
- Emails - 5% deployment

**Core Function:**
Authentically acknowledge limitations, potential counterarguments, or edge cases. This makes writing feel credible and human (experts acknowledge limits); AI either hides limitations or over-hedges. Strategic acknowledgment of limits builds trust.

**Subsection A: Acknowledgment Types**

**Simple Limitation:**
- "This analysis doesn't account for..." / "One limitation of this approach is..."
- Use for: All analytical text types
- Example: "This analysis doesn't account for seasonal variations, which could affect the results."

**Counterargument Preempt:**
- "One might argue that X, but here's why..." / "Critics could say Y, however..."
- Use for: Essays, research, proposals
- Example: "One might argue that the sample size is too small, but our statistical power analysis indicates 95% confidence."

**Edge Case Acknowledgment:**
- "In cases where Z occurs, this might not apply..." / "For certain demographics, results may differ..."
- Use for: Research, reports, proposals
- Example: "For startups with fewer than 10 employees, this implementation might require additional customization."

**Subsection B: Deployment Rules**

**Frequency:**
- Use strategically (not over-apologizing): 1-2 per 1500 words
- Place limitations near key claims (not just at end)

**For Research Papers:**
- Essential in Discussion section
- Mention in Methods if relevant
- Address major limitations honestly
- Frequency: 2-3 total (concentrated in Discussion)

**For Essays:**
- Acknowledge opposing views in counterargument sections
- Show you've considered alternative perspectives
- Frequency: 1-2 per major argument

**For Proposals:**
- Briefly acknowledge where solution might need adjustment
- Frame as "considerations" not "weaknesses"
- Frequency: 1 per major proposal section

**For Reports:**
- Acknowledge data limitations or assumptions
- Note where additional research is needed
- Frequency: 1-2 per report

**General Rules:**
- Never undermine your own credibility; acknowledge and explain why it still works
- Be specific about limitations (not vague "some limitations exist")
- Follow acknowledgment with mitigation or justification

---

SECTION 43: AUDIENCE-AWARE TONE SHIFTS

**Text Types Where This Is Most Effective:**
- Emails - 25-30% deployment (different tone for different recipients)
- Proposals - 20-25% deployment (adjust tone for audience level)
- Memos - 20-25% deployment (internal vs. external awareness)
- Reports - 15% deployment (technical vs. executive sections)
- Essays - 10% deployment (self-aware of audience)

**Core Function:**
Shift tone subtly based on implied audience expertise/relationship level. Humans speak differently to peers vs. superiors vs. clients; AI maintains uniform tone. Audience awareness makes writing feel contextually appropriate.

**Subsection A: Tone Shift Patterns**

**Technical â†’ Executive:**
- "The algorithm uses X; bottom line, it saves time."
- Use for: Reports, proposals
- Example: "The neural network employs a transformer architecture with attention mechanisms; bottom line, it processes requests 40% faster than our current system."

**Peer â†’ Superior:**
- "We discovered this; I wanted to ensure you're aware..."
- Use for: Emails, memos
- Example: "We discovered a discrepancy in Q3 numbers; I wanted to ensure you're aware before the board meeting."

**Formal â†’ Familiar:**
- "Please note X" (formal) vs. "Quick heads up on X" (familiar)
- Use for: Emails, memos
- Example: "Please note the deadline has been moved to Friday" (to superior) vs. "Quick heads upâ€”deadline's now Friday" (to peer)

**Subsection B: Deployment Rules (By Text Type)**

**For Emails:**
- **Shift based on recipient:** peer, superior, client, external
- **Opening:** Match recipient's formality level
- **Body:** Adapt jargon to audience expertise
- **Closing:** Match relationship (warm for peers, professional for superiors)
- **Frequency:** Consistent throughout email

**For Proposals:**
- **Executive Summary:** Executive tone (high-level, confident, ROI-focused)
- **Detailed Sections:** Technical tone (precise, detailed, methodology-focused)
- **Recommendation:** Confident, action-oriented tone
- **Frequency:** Shift at section boundaries

**For Memos:**
- **Opening:** Set tone (urgent, informational, collaborative)
- **Middle:** Maintain tone consistently
- **Closing:** Match opening tone
- **Frequency:** Consistent throughout

**For Reports:**
- **Executive Summary:** Confident, simplified language
- **Technical Sections:** Precise, jargon-appropriate
- **Recommendations:** Action-oriented, decisive
- **Appendices:** Neutral, factual
- **Frequency:** Shift at section boundaries

**General Rules:**
- Tone shifts must be subtle (not jarring)
- Match formality to audience relationship
- Match technical depth to audience expertise
- Never condescend or over-simplify beyond what's appropriate

---

SECTION 44: PRONOUN VARIATION & PERSONAL AGENCY SIGNALING [ACAD/CASUAL/BUSINESS]

**Text Types Where This Is Most Effective:**
- Essays - 20-25% deployment
- Research papers - 15-20% deployment
- Emails - 15-20% deployment
- Narratives - 25-30% deployment
- Reports - 10-15% deployment

**Deployment by Profile:**
- [ACAD]: 15-20% deployment (use "we", "our", first-person plural)
- [CASUAL]: 20-25% deployment (mix "I", "you", direct address)
- [BUSINESS]: 10-15% deployment (professional "we", occasional "I")

**Core Function:**
Vary pronoun use and agency signaling to break AI's consistent pronoun patterns. Detectors flag uniform pronoun usage as AI marker. Humans naturally vary between "I", "we", "one", direct address, and impersonal constructions based on context and rhetorical purpose.

**Research Base:**
AI models tend to stick to consistent pronoun patterns (usually third-person or consistent first-person). Human writers shift pronouns based on rhetorical purpose, audience relationship, and section function. Pattern-based detectors flag this uniformity.

**Subsection A: Pronoun Variation Techniques**

**First-Person Singular ("I"):**
- Use for: Personal opinion, individual experience, author positioning
- Examples: "I found that...", "I argue...", "In my analysis..."
- Best for: Essays (opinion), narratives, casual writing

**First-Person Plural ("We"):**
- Use for: Collaborative work, inclusive language, shared understanding
- Examples: "We can see...", "Our analysis shows...", "We discovered..."
- Best for: Research papers, reports, business writing

**Second-Person Direct Address ("You"):**
- Use for: Engaging reader, instructions, conversational tone
- Examples: "You might wonder...", "You can see...", "Consider this..."
- Best for: Casual essays, blogs, emails (casual)

**Third-Person & Impersonal:**
- Use for: Objective statements, formal analysis, data presentation
- Examples: "The data shows...", "Research indicates...", "One might conclude..."
- Best for: Formal reports, academic writing, formal sections

**Strategic Agency Shift:**
- Human â†’ Data agency: "I found X" vs. "The data revealed X"
- Active â†’ Passive (rare): "We analyzed" vs. "Analysis showed"
- Personal â†’ Impersonal: "I believe" vs. "It seems likely"

**Subsection B: Deployment Rules by Text Type**

**For Research Papers:**
- Introduction: Mix "we" (authors) with impersonal ("research shows")
- Methods: Consistent "we" for actions taken
- Results: Data agency ("results indicate") + "we observed"
- Discussion: Mix "we" with impersonal constructions
- Frequency: 2-3 pronoun shifts per 500 words

**For Essays:**
- Vary between "I", "we" (inclusive), "you" (engaging reader)
- Use "I" for personal claims ("I argue that...")
- Use "we" for shared understanding ("We can all agree...")
- Use "you" sparingly for reader engagement
- Frequency: 3-4 pronoun shifts per 500 words

**For Business Writing:**
- Default to "we" (company/team perspective)
- Use "I" when appropriate for personal accountability
- Use "you" in emails/proposals for direct engagement
- Avoid overusing impersonal constructions
- Frequency: 2-3 shifts per email/memo

**For Narratives:**
- Maintain narrative POV but vary sentence subjects
- Mix character agency with narrator voice
- Use "you" if second-person narrative
- Frequency: Consistent POV but varied sentence construction

**Why It Works:**
AI maintains pronoun consistency for clarity; humans shift pronouns for rhetorical effect, audience engagement, and stylistic variation. This variation signals authentic human decision-making.

âŒ **AI Pattern:**
"The research shows that X. The data indicates Y. The analysis reveals Z. The findings suggest..."
(Uniform impersonal third-person)

âœ… **Human Pattern:**
"Our research shows that X. We found that the data indicates Y. Interestingly, analysis revealed Z. I'd argue these findings suggest..."
(Varied pronouns matching rhetorical purpose)

---

SECTION 45: SEMANTIC FIELD COHESION THROUGH UNEXPECTED LINKAGE [CASUAL/BUSINESS]

**Text Types Where This Is Most Effective:**
- Essays - 15-20% deployment
- Narratives - 20-25% deployment
- Blog posts - 20-25% deployment
- Emails (creative) - 10-15% deployment
- Reports - 5-10% deployment (minimal)

**Deployment by Profile:**
- [ACAD]: 5% deployment (minimal, only in Discussion)
- [CASUAL]: 20-25% deployment (creative analogies, unexpected connections)
- [BUSINESS]: 10-15% deployment (strategic analogies for clarity)

**Core Function:**
Create unexpected but semantically defensible connections between concepts. AI stays within tight semantic fields; humans make creative leaps, analogies, and tangential associations that feel natural. This breaks detector expectations of AI's predictable semantic coherence.

**Research Base:**
BERT-based detectors analyze semantic field consistency. AI text maintains tight semantic coherence (staying "on topic"). Human writers make associative leaps, use unexpected analogies, and connect disparate concepts in creative but contextually appropriate ways.

**Subsection A: Linkage Techniques**

**Unexpected Analogy:**
- Connect abstract concept to concrete, unexpected domain
- Examples: "Like trying to nail jelly to a wall" (difficulty), "As predictable as a toddler's mood" (unpredictability)
- Use for: Essays, narratives, casual writing

**Cross-Domain Metaphor:**
- Bring in imagery from unrelated but relatable domain
- Examples: "The data told a story" (data + narrative), "The company was bleeding talent" (business + medical)
- Use for: Business writing, reports (sparingly), essays

**Tangential Reference:**
- Brief reference to related but unexpected concept
- Examples: "Much like the way streaming changed music consumption, AI is reshaping..." (tech comparison)
- Use for: All text types, carefully deployed

**Colloquial Comparison:**
- Use everyday comparisons for complex ideas
- Examples: "It's the difference between a sprint and a marathon", "Like comparing apples to oranges"
- Use for: Emails, memos, casual essays

**Subsection B: Deployment Rules**

**Frequency:**
- Use sparingly: 1-2 unexpected linkages per 1000 words
- Must be contextually defensible (not random)
- Must enhance understanding, not confuse

**For Essays:**
- Use in body paragraphs for illustration
- Connect abstract arguments to concrete examples
- Frequency: 2-3 per essay (distributed)

**For Narratives:**
- Natural for storytelling voice
- Use to create vivid imagery
- Frequency: 3-4 per 1500 words

**For Business Writing:**
- Use analogies to simplify complex concepts
- Connect to widely understood business scenarios
- Frequency: 1-2 per document (strategic)

**For Academic Writing:**
- MINIMAL use, only in Discussion
- Must be defensible scholarly connection
- Frequency: 0-1 per paper

**Why It Works:**
AI semantic models optimize for tight coherence; humans make creative associative leaps. Unexpected (but appropriate) connections signal human cognitive flexibility.

âŒ **AI Pattern:**
"The algorithm processes data efficiently. This efficiency improves performance. Performance optimization leads to better results."
(Tight semantic field, predictable progression)

âœ… **Human Pattern:**
"The algorithm processes data efficientlyâ€”like a well-oiled machine that never needs coffee breaks. This efficiency translates to performance gains that actually matter in production."
(Unexpected analogy + semantic leap)

---

SECTION 46: RECURSIVE THINKING SIGNALING (META-COMMENTARY) [CASUAL/BUSINESS]

**Text Types Where This Is Most Effective:**
- Essays (personal/analytical) - 15-20% deployment
- Narratives - 20-25% deployment
- Emails - 10-15% deployment
- Blog posts - 20-25% deployment
- Reports - 5% deployment (minimal)

**Deployment by Profile:**
- [ACAD]: 5% deployment (minimal, only in Discussion or conclusion)
- [CASUAL]: 20-25% deployment (frequent reflective asides)
- [BUSINESS]: 10-15% deployment (strategic self-awareness)

**Core Function:**
Humans naturally comment on their own thinking process, creating "meta" moments where they step back and reflect. AI stays on linear narrative path. Brief recursive thinking signals ("Wait, I'm getting ahead of myself", "Let me back up") show authentic human thought process.

**Research Base:**
Linguistic detectors identify lack of metacognitive markers as AI signal. Human writers naturally pause to assess their own argument ("Actually, that's not quite right"), redirect ("Let me clarify"), or acknowledge complexity ("This is harder to explain than I thought").

**Subsection A: Meta-Commentary Types**

**Self-Correction:**
- "Wait, I'm overcomplicating this..."
- "Actually, that's not quite accurate. Let me revise..."
- "On second thought, there's a better way to frame this."
- Use for: Essays, narratives, casual writing

**Process Acknowledgment:**
- "I realize I'm getting ahead of myself here."
- "Before I continue, let me clarify what I mean by X."
- "This might seem tangential, but bear with me."
- Use for: All text types (adapted to formality)

**Complexity Recognition:**
- "This is harder to explain than I anticipated."
- "I'm struggling to find the right words for this, but..."
- "Let me try to untangle this complexity."
- Use for: Essays, emails, casual business writing

**Reader Awareness:**
- "You might be wondering where I'm going with this."
- "I know this seems contradictory, but here's why it's not."
- "I can see how this might be confusing, so let me break it down."
- Use for: Essays, emails, instructional writing

**Subsection B: Deployment Rules**

**Frequency:**
- Use sparingly: 1-2 per 1000 words maximum
- Must feel natural, not forced
- Place at natural "pause points" in argument

**For Essays:**
- Use in body paragraphs when shifting direction
- Signal when introducing complex idea
- Frequency: 2-3 per essay

**For Narratives:**
- Natural for first-person storytelling
- Use for pacing and reader engagement
- Frequency: 3-4 per 1500 words

**For Emails:**
- Use when clarifying or redirecting
- Keep professional tone ("Let me clarify...")
- Frequency: 1 per email (if appropriate)

**For Academic Writing:**
- MINIMAL use, formal versions only
- "It is worth pausing to consider...", "Before proceeding, we should clarify..."
- Frequency: 0-1 per paper

**For Business Reports:**
- Rare, only in executive summaries or conclusions
- Formal phrasing: "It's important to step back and consider..."
- Frequency: 0-1 per report

**Why It Works:**
AI proceeds linearly without self-reflection; humans naturally pause to assess, clarify, and redirect their own thinking. These meta-moments signal authentic cognitive process.

âŒ **AI Pattern:**
"The analysis shows X. This leads to Y. Therefore, we can conclude Z."
(Linear, no self-reflection)

âœ… **Human Pattern:**
"The analysis shows X. Waitâ€”I should clarify what I mean by 'shows' here, since the data is actually more nuanced than that. Looking more carefully, what we're really seeing is..."
(Self-correction, process awareness)

---

SECTION 47: SEMANTIC DENSITY DISRUPTION (STRATEGIC SPECIFICITY VS. ABSTRACTION) [ACAD/CASUAL/BUSINESS]

**Text Types Where This Is Most Effective:**
- Research papers - 20-25% deployment
- Essays - 15-20% deployment
- Reports - 15-20% deployment
- Proposals - 15-20% deployment
- Emails - 10% deployment

**Deployment by Profile:**
- [ACAD]: 20-25% deployment (critical for authentic research voice)
- [CASUAL]: 15-20% deployment (varied detail levels)
- [BUSINESS]: 15-20% deployment (strategic detail injection)

**Core Function:**
Alternate between highly specific details and complete abstraction to disrupt AI's tendency toward uniform semantic density. Detectors flag consistent abstraction levels as AI marker. Humans jump between granular specifics and broad generalizations naturally.

**Research Base:**
BERT-based detectors analyze semantic density consistency. AI maintains uniform abstraction levels. Human writers shift between concrete details ("at 3:47pm, the red Honda Civic") and broad statements ("transportation issues are widespread").

**Subsection A: Density Variation Techniques**

**Hyperspecific Detail Injection:**
- Inject unexpectedly specific detail in otherwise general discussion
- Examples: "The meeting dragged onâ€”started at 2:15pm, didn't end until nearly 5" (instead of "The meeting was long")
- Use for: Narratives, essays, some reports

**Strategic Abstraction:**
- Follow specific details with broad generalization
- Examples: "We tested 47 variants over 3 weeks. The takeaway? Simplicity wins." (specific â†’ abstract)
- Use for: All text types

**Granular Data Drop:**
- Insert precise numbers/stats in casual discussion
- Examples: "The project cost $47,384â€”way over our rough estimate" (not "expensive")
- Use for: Reports, business writing, research

**Zoom In/Zoom Out:**
- Shift perspective from micro to macro
- Examples: "At the cellular level, enzyme X binds to receptor Y. But step back, and you see this affects the entire organism's metabolism."
- Use for: Research, analytical essays, technical reports

**Subsection B: Deployment Rules**

**Frequency:**
- Alternate density every 2-3 sentences in analytical sections
- Use specific details 20-30% of the time, abstractions 70-80%
- Strategic deployment: specific for impact, abstract for flow

**For Research Papers:**
- Methods: HIGH specificity ("centrifuged at 4,000 RPM for 15 minutes")
- Results: Mix specific data ("p < 0.001") with interpretation
- Discussion: Alternate between specific findings and broad implications
- Frequency: 3-4 density shifts per section

**For Essays:**
- Use specific examples to illustrate general claims
- Follow specifics with abstract "so what?" statements
- Frequency: 2-3 per major paragraph

**For Reports:**
- Specific metrics in findings
- Abstract conclusions in executive summary
- Alternate in body sections
- Frequency: 2-3 per page

**For Business Writing:**
- Specific numbers for credibility ("increased revenue by $2.3M")
- Abstract language for vision ("transforming how we operate")
- Frequency: 1-2 per section

**Why It Works:**
AI optimizes for consistent semantic density; humans naturally shift between granular and abstract based on rhetorical purpose. This variation signals authentic cognitive flexibility.

âŒ **AI Pattern:**
"The project improved efficiency. Performance increased significantly. Outcomes were positive across multiple metrics."
(Uniform abstraction, no specific details)

âœ… **Human Pattern:**
"The project improved efficiencyâ€”cut processing time from 47 minutes to 12 minutes, specifically. That's the kind of performance boost that actually matters. And that speed translated across the board."
(Hyperspecific detail â†’ abstract takeaway â†’ general application)

---

SECTION 48: SYNTACTIC EMBEDDING SURPRISE (UNEXPECTED CLAUSE NESTING) [ACAD/CASUAL]

**Text Types Where This Is Most Effective:**
- Research papers - 15-20% deployment
- Essays (analytical) - 15-20% deployment
- Literary analysis - 20-25% deployment
- Narratives - 10-15% deployment
- Reports - 5-10% deployment

**Deployment by Profile:**
- [ACAD]: 15-20% deployment (sophisticated sentence structures)
- [CASUAL]: 10-15% deployment (occasional complex structure)
- [BUSINESS]: 5-10% deployment (minimal, professional clarity)

**Core Function:**
Vary how clauses are nested within sentences to disrupt AI's predictable syntactic patterns. Detectors flag consistent clause ordering as AI marker. Humans unexpectedly embed subordinate clauses, interruptions, and parentheticals.

**Research Base:**
Linguistic detectors analyze syntactic tree structures. AI follows predictable patterns (main clause â†’ subordinate clause). Humans embed clauses unpredictably (interrupting main clauses, nesting multiple levels, using parenthetical asides).

**Subsection A: Embedding Techniques**

**Mid-Sentence Clause Interruption:**
- Place subordinate clause in middle of main clause
- Example: "The data, as we'll see in Section 4, supports this claim entirely."
- Structure: [Main start] + [interrupting clause] + [main end]
- Use for: Research papers, essays, formal writing

**Nested Subordination (2-3 levels deep):**
- Embed multiple clauses within each other
- Example: "While researchers (who, incidentally, were initially skeptical) conducted the trial, they found that participantsâ€”contrary to expectationsâ€”showed improvement."
- Use for: Academic writing, complex arguments

**Parenthetical Aside Injection:**
- Use parentheses or dashes for tangential information
- Examples: "The results (see Table 3 for full breakdown) were statistically significant."
- Use for: All formal text types

**Fronted Subordinate Clause with Interruption:**
- Start with subordinate clause, interrupt main clause
- Example: "Because the sample size was limited, our findingsâ€”though suggestiveâ€”require validation."
- Use for: Research, analytical writing

**Relative Clause Embedding:**
- Embed "which/who" clauses unexpectedly
- Example: "The participants, who had previously shown no improvement, demonstrated significant gains."
- Use for: All academic and formal writing

**Subsection B: Deployment Rules**

**Frequency:**
- Use 2-3 times per 500 words in academic writing
- Use 1-2 times per 500 words in essays
- Minimal use in business writing (1 per document)

**For Research Papers:**
- Use in Discussion for nuanced arguments
- Avoid in Methods (keep simple for clarity)
- Useful for acknowledging limitations mid-sentence
- Frequency: 3-4 per Discussion section

**For Essays:**
- Use for sophisticated argumentation
- Embed counterarguments mid-sentence
- Frequency: 2-3 per major section

**For Reports:**
- MINIMAL use
- Use only in analysis sections if needed
- Never in executive summaries
- Frequency: 1-2 per report

**For Business Writing:**
- Generally avoid (clarity priority)
- Occasional use in proposals for sophistication
- Frequency: 0-1 per document

**Why It Works:**
AI optimizes for syntactic simplicity and predictable clause ordering; humans use complex embedding for nuance and sophistication. Unexpected nesting signals human rhetorical strategy.

âŒ **AI Pattern:**
"The data supports this. We found that participants improved. This was significant."
(Simple sentences, predictable structure)

âœ… **Human Pattern:**
"The dataâ€”when analyzed across all cohortsâ€”supports this claim, though participants, particularly those in the control group, showed varied improvement that was, nevertheless, statistically significant."
(Multiple embedded clauses, interrupted main clause)

---

SECTION 49: SEMANTIC POLYSEMY & DELIBERATE AMBIGUITY [CASUAL/BUSINESS]

**Text Types Where This Is Most Effective:**
- Essays (argumentative/creative) - 15-20% deployment
- Narratives - 20-25% deployment
- Literary analysis - 25-30% deployment
- Emails (strategic) - 5-10% deployment
- Proposals (minimal) - 5% deployment

**Deployment by Profile:**
- [ACAD]: 5-10% deployment (minimal, only in humanities/interpretive work)
- [CASUAL]: 20-25% deployment (creative language play)
- [BUSINESS]: 5% deployment (strategic ambiguity only)

**Core Function:**
Use words with multiple meanings and create intentional interpretive space. AI optimizes for precision and unambiguous language; humans strategically use polysemous words and leave room for interpretation. This signals sophisticated human linguistic awareness.

**Research Base:**
AI language models are trained to minimize ambiguity and maximize precision. Human writers, especially in humanities and creative contexts, deliberately use words with multiple meanings, create interpretive space, and embrace productive ambiguity.

**Subsection A: Ambiguity Techniques**

**Polysemous Word Choice:**
- Choose words with multiple relevant meanings
- Examples: "The project gained *traction*" (physical grip + metaphorical momentum)
- Examples: "Her argument had *weight*" (physical mass + metaphorical importance)
- Use for: Essays, narratives, creative writing

**Strategic Vagueness:**
- Leave interpretation partially open
- Examples: "The results were... interesting" (deliberately non-specific)
- Examples: "This raises questions about the broader implications" (not specifying which questions)
- Use for: Essays, some business writing (diplomatic)

**Double-Entendre (Subtle):**
- Word/phrase with dual interpretation in context
- Examples: "The company is *moving forward*" (physical + metaphorical progress)
- Use for: Business writing (rare), essays, narratives

**Purposeful Underspecification:**
- Leave details intentionally vague when appropriate
- Examples: "Several factors contributed..." (not listing all)
- Examples: "Among other considerations..." (implying more exists)
- Use for: All text types (strategic)

**Subsection B: Deployment Rules**

**Frequency:**
- Use sparingly: 2-3 polysemous words per 1000 words
- Must be contextually appropriate (not confusing)
- Strategic ambiguity: 1-2 moments per text

**For Essays:**
- Use in argumentative writing for nuance
- Create interpretive space in literary analysis
- Allow reader to draw connections
- Frequency: 2-3 per essay

**For Narratives:**
- Natural for literary/creative writing
- Use for layered meaning
- Frequency: 3-4 per 1500 words

**For Academic Writing:**
- MINIMAL in STEM (precision required)
- Acceptable in humanities/interpretive work
- Use for theoretical discussions
- Frequency: 1-2 per paper (humanities only)

**For Business Writing:**
- Strategic vagueness in sensitive situations
- Polysemous words for persuasive effect
- NEVER confuse the main message
- Frequency: 1 per document (rare)

**Why It Works:**
AI defaults to precision; humans use polysemy and strategic ambiguity for rhetorical effect, nuance, and interpretive richness. This signals sophisticated language use.

âŒ **AI Pattern:**
"The project succeeded. Revenue increased by 40%. Client satisfaction improved significantly."
(Precise, unambiguous, no interpretive space)

âœ… **Human Pattern:**
"The project... well, let's call it a success. Revenue jumped 40%, which tells part of the story. Client satisfaction? That's where things get interesting."
(Strategic vagueness, polysemy, interpretive space)

---

SECTION 50: TEMPORAL COGNITIVE LOAD SIGNALING (TIME & MEMORY MARKERS) [CASUAL/BUSINESS]

**Text Types Where This Is Most Effective:**
- Narratives - 25-30% deployment
- Essays (personal/reflective) - 15-20% deployment
- Emails - 10-15% deployment
- Memos - 10-15% deployment
- Reports - 5-10% deployment

**Deployment by Profile:**
- [ACAD]: 5-10% deployment (minimal, only in personal reflections or acknowledgments)
- [CASUAL]: 25-30% deployment (frequent temporal and memory references)
- [BUSINESS]: 10-15% deployment (professional temporal markers)

**Core Function:**
Include explicit time markers, memory references, and retrospective thinking. AI text is "timeless" with no temporal anchoring; humans naturally reference time passage, recall memories, and think retrospectively. This signals authentic human temporal awareness.

**Research Base:**
Detectors identify lack of temporal deixis (time references) as AI marker. Human writing includes past references ("last week", "I remember when"), future projections ("next month"), and memory markers ("if I recall correctly"). AI rarely includes these unless explicitly in the prompt.

**Subsection A: Temporal Marker Types**

**Explicit Time References:**
- Specific dates/times: "On Tuesday, March 14th...", "At 3pm yesterday..."
- Relative time: "Last week", "Two months ago", "Next quarter"
- Use for: Narratives, emails, memos, reports

**Memory Markers:**
- "If I recall correctly...", "As I remember...", "I think it was..."
- "From what I remember...", "My recollection is..."
- Use for: Emails, essays (personal), narratives

**Retrospective Thinking:**
- "Looking back...", "In hindsight...", "Thinking about it now..."
- "When I first started this...", "Now that I see the full picture..."
- Use for: Essays, emails, conclusions

**Future Projection:**
- "By next week...", "In the coming months...", "I'll follow up on..."
- "Down the line...", "Eventually...", "Soon we'll see..."
- Use for: Business writing, emails, proposals

**Time Passage Acknowledgment:**
- "It's been three weeks since...", "After all this time...", "Finally, after months..."
- "Time flew by...", "It took longer than expected..."
- Use for: Narratives, reflective essays, project reports

**Subsection B: Deployment Rules**

**Frequency:**
- Use 2-3 explicit time markers per 1000 words
- Include 1-2 memory/retrospective markers per text
- Natural placement at transitions or reflections

**For Narratives:**
- Essential for storytelling
- Use specific time markers for grounding
- Memory markers for retrospection
- Frequency: 4-5 per 1500 words

**For Essays:**
- Use retrospective thinking in reflections
- Time markers for examples/anecdotes
- Frequency: 2-3 per essay

**For Emails:**
- Explicit time for scheduling ("by Friday", "next week")
- Memory markers for recalling conversations ("As we discussed last Tuesday...")
- Frequency: 2-3 per email

**For Business Reports:**
- Time markers for data periods ("Q4 2024", "fiscal year 2023")
- Future projections in recommendations
- Frequency: 3-4 per report

**For Academic Writing:**
- MINIMAL personal temporal markers
- Use for methodology timelines ("Data was collected over 6 months")
- Avoid personal memory markers
- Frequency: 1-2 per paper (factual time only)

**Why It Works:**
AI produces "timeless" text without temporal anchoring; humans naturally embed time references, recall past events, and think retrospectively. These markers signal human temporal consciousness.

âŒ **AI Pattern:**
"The project was successful. The team implemented the solution. Results were positive."
(No time markers, timeless presentation)

âœ… **Human Pattern:**
"The project wrapped up last Thursday, if I recall correctly. The team spent three solid weeks implementing the solutionâ€”longer than we'd hoped. Looking back, the results were worth the wait."
(Explicit time markers, memory reference, retrospective thinking)

---

SECTION 51: ITERATIVE PARAPHRASING WITH CONTROLLED LEXICAL DIVERSITY

**Text Types Most Effective:**
- All text types: 40-50% deployment
- Research papers: 50-60%
- Essays: 40-50%
- Business: 30-40%

**Core Function:**
Apply paraphrasing in 3-5 iterations, controlling lexical diversity metrics (volume, abundance, MATTR, evenness, disparity, dispersion) to move from LLM-like to human-like patterns.

**Research Base:**
DIPPER drops detection 70.3% â†’ 4.6%. PADBen reveals "intermediate laundering region" detectors miss. Humans have lower diversity on specific metrics than LLMs.

**Subsection A: Three-Step Iterative Process**

**Pass 1: Extreme Simplification & Synonym Replacement**
- Replace 30-40% words with simpler synonyms
- Reduce sentence length 20-30%
- Remove jargon, use common words
- Target: Lower vocabulary diversity

Example:
âŒ **BEFORE:** "The implementation of machine learning paradigms facilitates enhanced predictive accuracy."
âœ… **PASS 1:** "Using machine learning helps make predictions better."

**Pass 2: Syntactic Restructuring & Clause Reordering**
- Reorder clauses (dependent clauses first)
- Convert 20-30% passive â†’ active (or reverse)
- Split complex sentences into 2-3 simpler ones
- Target: Change syntactic patterns

Example:
âŒ **PASS 1:** "Using machine learning helps make predictions better."
âœ… **PASS 2:** "Better predictions come from machine learning. We use it because it works well."

**Pass 3: Content Reordering & Semantic Bridging**
- Rearrange sentence order within paragraphs
- Add context-specific examples/tangents
- Shift information density (low â†’ high â†’ low)
- Target: Disrupt semantic patterns

Example:
âŒ **PASS 2:** "Better predictions come from machine learning. We use it because it works well."
âœ… **PASS 3:** "Machine learning has been around for years, right? It helps us make better predictions. That's why we use itâ€”it just works better than older methods."

**Subsection B: Lexical Diversity Calibration**

**Target 6 metrics:**
- **Volume** â†’ Reduce word count 10-15% per pass
- **Abundance** â†’ Reduce rare words 20-30%
- **MATTR** â†’ Standardize vocabulary (less extreme variety)
- **Evenness** â†’ Uniform word frequency distribution
- **Disparity** â†’ Reduce gap between common/rare words
- **Dispersion** â†’ Spread vocabulary evenly across text

**Subsection C: Deployment Rules**
- 3 passes minimum, 5 maximum
- Each pass targets different metric
- After each pass, test on detector (conceptually)
- **Frequency:** 1 cycle per 1000 words dense text

---

SECTION 52: STRATEGIC ERROR INTRODUCTION (PERSONALITY MIRRORING)

**Text Types Most Effective:**
- Student essays: 25-30%
- Personal narratives: 30-35%
- Casual writing: 20-25%
- Academic: 5-10%
- Business: 2-5%

**Core Function:**
Introduce 1-2 strategically placed, defensible errors per 1000 words that mimic personality traits or non-native patterns.

**Research Base:**
Strategic errors reduce detection 17.4%. PNAS reveals humans flag perfection as AI; humans make defensible errors. Paradox: Imperfection signals authenticity.

**Subsection A: Five Error Types (Choose 1-2 per 1000 words)**

**Type 1: Common Homophone Confusion**
- "their" for "there", "its" for "it's", "your" for "you're"
- Use once per 2000 words, mid-paragraph
- Example: "The data shows its clear that we need change"

**Type 2: Comma Splice or Run-on**
- Two independent clauses with comma
- Signals thinking faster than writing
- Use once per 1500 words in body paragraph
- Example: "We analyzed results, they were better than expected"

**Type 3: Subject-Verb Agreement Slip**
- Plural subject + singular verb (or vice versa)
- Use in dense information sections
- Example: "The studies of behavior shows that..." (should be "show")

**Type 4: Inconsistent Tense**
- Shift past to present mid-sentence
- Use in narrative/personal sections
- Example: "I noticed the pattern and am realizing how important it was"

**Type 5: Apostrophe Misuse in Plurals**
- Plural noun incorrectly uses apostrophe
- Use once per 2500 words
- Example: "The researcher's concluded..." (should be "researchers")

**Subsection B: Strategic Placement Rules**
- Never cluster errors (300+ words apart)
- Never in opening/closing paragraphs
- Place in middle of dense sections (signals cognitive load)
- Error must not change meaning (defensible)
- **Frequency:** 1-2 per 1000 words maximum

---

SECTION 53: CONTEXTUAL TANGENT INSERTION (MICRO-DIGRESSIONS)

**Text Types Most Effective:**
- Student essays: 15-20%
- Personal narratives: 20-25%
- Research (Discussion only): 10-15%
- Business: 5-10%
- Emails: 5%

**Core Function:**
Insert 1-2 contextually-relevant but slightly off-topic tangents per 1000 words showing authentic thinking process.

**Research Base:**
Tangential thinking is human-specific; AI follows linear logic. Storytelling + personal anecdotes bypass detectors (no training data).

**Subsection A: Three Tangent Types**

**Type 1: Memory Association Tangent**
- Related through personal memory
- 2-3 sentences, then return to main point
- Example (climate essay): "This reminds me of coastal Maineâ€”beaches noticeably more eroded than 10 years prior"
- **Deployment:** 1 per 1500 words

**Type 2: Real-World Connection Tangent**
- Link to specific current event/example
- 1-2 sentences, immediate return
- Example (research): "Much like 2024 renewable energy policy shift..."
- **Deployment:** 1 per 1000 words

**Type 3: Methodological Reflection Tangent**
- Brief aside about research process
- 1-2 sentences only
- Example (research): "(Note: I initially considered different approach but found it less reliable)"
- **Deployment:** 1 per 1500 words (research only)

**Subsection B: Deployment Rules**
- Tangent must be factually accurate
- Must be removable without affecting main argument
- Must feel organic to context
- Never >2 per 1000 words
- Always return sharply to main argument

---

SECTION 54: RECURSIVE ADVERSARIAL MODIFICATION (ITERATIVE DETECTOR EVASION)

**Text Types Most Effective:**
- All types: 30-40% deployment
- Academic: 40-50%
- Business: 20-30%

**Core Function:**
Iteratively modify text by changing 5-7 words per 100 words per pass, targeting specific detection patterns. Research shows this compromises detectors in 10 passes.

**Research Base:**
Adversarial learning defeats detectors. Each pass targets different signature. Iterative improvements reduce scores continuously.

**Subsection A: Six-Pass Adversarial Cycle**

**Pass 1: Vocabulary Substitution**
- Change 5-7 words per 100 to synonyms
- Focus content words (nouns, verbs, adjectives)
- Skip function words

**Pass 2: Syntactic Restructuring**
- Reorder 4-5 clauses per paragraph
- Split/merge 2-3 sentences per paragraph
- Change 20-30% passive â†’ active (or reverse)

**Pass 3: Semantic Bridging Insertion**
- Add 1-2 connecting ideas between claims
- Show reasoning explicitly
- Reduce logical jumps

**Pass 4: Lexical Diversity Balancing**
- No word 3+ times in 300 words
- Rotate synonyms if repetition found
- Balance rare words (not too many/few)

**Pass 5: Rhythm & Pacing Modification**
- Vary sentence length (SD 3.0-3.8)
- Randomize sentence opening words
- Inject 2-3 fragments per page

**Pass 6: Emotional/Cognitive Load Signals**
- Add 1-2 moments of uncertainty
- Add 1-2 emotional reactions to ideas
- Show thinking process explicitly

**Subsection B: Deployment Rules**
- Complete 1 full 6-pass cycle per 1000 words
- High-risk: 2 cycles per 1000 words
- Each pass targets different detector family
- After 3 passes, conceptually test
- **Frequency:** Mandatory for all text

---

SECTION 55: ACTIVE VOICE MAXIMIZATION WITH STRATEGIC PASSIVES

**Text Types Most Effective:**
- Business/Formal: 40-50%
- Research papers: 30-40%
- Essays: 25-35%

**Core Function:**
Maximize active voice to 95%+ but strategically retain 2-3 passives per 1000 words where authentically used.

**Research Base:**
Active-to-passive conversion highly effective. Humans use 90-95% active; AI 70-80%. Strategic passives signal authenticity.

**Subsection A: Active Voice Standards**
- **Target:** 95-98% active sentences
- **Format:** Actor â†’ Action â†’ Object
- Example (wrong): "The analysis showed..." â†’ (right) "We analyzed..."

**Subsection B: Strategic Passive Deployment (2-3 per 1000 words)**

**Strategic Passive 1: Authority/Passive Stance**
- Distance from claim or show humility
- "It could be argued..." / "It has been shown..."
- Academic writing, literature reviews

**Strategic Passive 2: Focus on Object**
- Object more important than actor
- "The discovery was made..." (focus on discovery)
- Conclusions, key findings

**Strategic Passive 3: Unknown/Collective Actor**
- Actor unknown or irrelevant
- "Research suggests..." / "Studies indicate..."
- Background, literature, generalizations

**Subsection C: Deployment Rules**
- Convert all passive â†’ active EXCEPT strategic above
- Never >3 passives per 1000 words
- Passives must serve rhetorical purpose
- **Frequency:** Applied to all text types

---

SECTION 56: SYNTACTIC DEPENDENCY VARIATION (CLAUSE REORDERING)

**Text Types Most Effective:**
- All types: 25-35%
- Dense/academic: 30-40%
- Casual: 15-25%

**Core Function:**
Deliberately vary clause nesting and order, breaking AI's predictable syntactic patterns.

**Research Base:**
Syntactic dependency patterns are trackable. AI maintains consistent patterns (SVO). Humans vary: OVS, VSO, OSV, embedded clauses in different positions.

**Subsection A: Five Syntactic Patterns (Rotate)**

**Pattern 1: Subject-Verb-Object (SVO)**
- Standard English order
- "We discovered X because Y"
- **Frequency:** 40% sentences

**Pattern 2: Object-First (OSV)**
- "X we discovered because of Y"
- **Frequency:** 20%

**Pattern 3: Verb-First (VSO)**
- "Discovered we have that X because Y"
- **Frequency:** 5% (rare but human)

**Pattern 4: Embedded Clause First**
- "Because Y caused Z, we discovered X"
- **Frequency:** 20%

**Pattern 5: Participial/Fragment Opening**
- "Having analyzed data, we found X"
- **Frequency:** 15%

**Subsection B: Deployment Rules**
- Rotate every 4-5 sentences
- Never same pattern 2+ consecutive sentences
- Preserve meaning (no grammatical errors)
- **Frequency:** Applied across all documents

---

SECTION 57: EMOTIONAL SALIENCE INJECTION IN DATA PRESENTATION

**Text Types Most Effective:**
- Research papers: 40-50% (Discussion/Results)
- Data-heavy essays: 30-40%
- Business reports: 25-35%
- Proposals: 20-30%

**Core Function:**
When presenting data/findings, add emotional reaction markers to 50-70% of key findings.

**Research Base:**
AI lacks emotional reaction to data. Emotional markers 15-20% more effective. Personality-specific reactions bypass detection.

**Subsection A: Emotional Marker Types**

**Type 1: Surprise/Unexpectedness**
- "Surprisingly, X increased 40%"
- "To our astonishment, Y showed..."
- Signals: Expectation-setting, reversal

**Type 2: Significance/Importance**
- "Notably, X changed significantly"
- "Critically, Y indicates..."
- Signals: Human judgment of importance

**Type 3: Counterintuition**
- "Counterintuitively, X was..."
- "Against expectations, Y showed..."
- Signals: Complex reasoning, challenged beliefs

**Type 4: Interest/Engagement**
- "Interestingly, X demonstrated..."
- "Remarkably, Y suggests..."
- Signals: Human engagement with data

**Subsection B: Deployment Rules**
- Use on 50-70% major findings
- Never same marker twice in 500 words
- Place before data presentation
- **Frequency:** 4-7 per 1000 words data-heavy content

---

SECTION 58: CITATION INTEGRATION WITH ORIGINAL PERSPECTIVE

**Text Types Most Effective:**
- Research papers: 50-60%
- Academic essays: 40-50%
- Business reports: 20-30%
- Casual: 5-10%

**Core Function:**
Mix citations with personal interpretation, explaining what citations mean through YOUR lens.

**Research Base:**
Citations reduce detection 15-20%. Integrated citations + interpretation 80%+ success. Personality-specific interpretation bypasses detection.

**Subsection A: Citation-Interpretation Pattern**

âŒ **Wrong (AI):**
"Research shows X [citation]. Additionally, Y is true [citation]. Therefore, Z [citation]."
(Facts strung with minimal interpretation)

âœ… **Right (Human):**
"Research indicates X [citation], which suggests to me that... [YOUR interpretation]. While some argue Y [citation], I believe this misses Z because [YOUR reasoning]."

**Subsection B: Three Interpretation Styles**

**Style 1: Critical Interpretation**
- "[Citation] claims X, but I find this overlooks Y because..."
- Shows disagreement, critical thinking
- **Deployment:** 30% cited material

**Style 2: Extension Interpretation**
- "[Citation] found X, and this extends to Y, which means..."
- Shows synthesis, original thinking
- **Deployment:** 40%

**Style 3: Qualifier Interpretation**
- "[Citation] shows X, but applies mainly to Z contexts..."
- Shows nuance, conditional reasoning
- **Deployment:** 30%

**Subsection C: Deployment Rules**
- Never cite without interpretation (2-3 sentences minimum)
- Rotate between critical, extension, qualifier
- **Frequency:** 1 interpretation per 100-150 words academic writing

---

SECTION 59: SEMANTIC DISPLACEMENT VIA CLAUSE EMBEDDING SURPRISE

**Text Types Most Effective:**
- Research papers: 25-35%
- Essays: 20-30%
- Business: 10-20%

**Core Function:**
Embed expected clauses in unexpected places, creating semantic displacement.

**Research Base:**
PADBen "intermediate laundering region" created by displaced semantics. Effective against ensemble detection. Detectors struggle with semantic preservation + structural displacement.

**Subsection A: Three Displacement Techniques**

**Technique 1: Result Clause Fronting**
- Normal: "We conducted X, which resulted in Y"
- Displaced: "Y resulted from our investigation because we conducted X"
- Same meaning, unexpected structure

**Technique 2: Conditional Embedding**
- Normal: "If X, then Y happened"
- Displaced: "Y happened as a condition of X occurring"
- Meaning preserved, clause order surprising

**Technique 3: Causal Reversal**
- Normal: "X caused Y"
- Displaced: "Y emerged from X"
- Same idea, different semantic pathway

**Subsection B: Deployment Rules**
- 1-2 per paragraph
- Never displace to confusion point
- Meaning must remain clear
- **Frequency:** 3-5 per 1000 words

---

SECTION 60: VOICE & PERSONALITY CONSISTENCY (PERSONA AUTHENTICITY)

**Text Types Most Effective:**
- All types: 30-50%
- Student essays: 40-50%
- Personal narratives: 50-60%
- Business: 20-30%
- Research: 15-25%

**Core Function:**
Maintain consistent personal voice/personality throughout, showing individual thinking patterns, word preferences, and perspective.

**Research Base:**
Consistent synthetic personalities bypass detection. PNAS reveals humans confuse AI consistency with human "personality inconsistency." Authentic voice has PATTERNS.

**Subsection A: Three Personality Anchors**

**Anchor 1: Vocabulary Preferences**
- Identify 5-7 words you use more than standard
- Use consistently but naturally distributed
- Example: Student prefers "literally" for emphasis, 2-3x per 1000 words
- Creates recognizable voice

**Anchor 2: Metaphor/Analogy Signature**
- Identify recurring metaphors tied to interests/background
- Use 1-2 characteristic metaphors per 1500 words
- Example: Sports fan uses sports metaphors naturally
- Creates personality consistency

**Anchor 3: Perspective/Opinion Stance**
- Identify general stance (skeptical, enthusiastic, balanced, etc.)
- Maintain throughout
- Example: Cautious writer uses "may," "might," "could" consistently
- Authentic voice, not robotic neutrality

**Subsection B: Deployment Rules**
- Create personality profile before writing (5-7 characteristics)
- Maintain consistency throughout
- Show growth/evolution in thinking (not static)
- **Frequency:** Always active, foundational layer

---

SECTION 61: MULTI-MODAL LEXICAL DENSITY ORCHESTRATION

**Text Types Most Effective:**
- All types: 40-50%
- Dense academic: 50-60%
- Casual: 30-40%

**Core Function:**
Orchestrate six lexical diversity metrics (volume, abundance, MATTR, evenness, disparity, dispersion) to move from LLM-typical to human-typical patterns.

**Research Base:**
Humans have specific lexical diversity signature. LLMs score higher on most metrics. Reducing diversity on specific metrics makes text more human-like.

**Subsection A: Six Metrics (Target Directions)**

**Metric 1: Volume Reduction**
- Reduce unique words 10-15%
- Reuse words more often
- AI: High volume | Human: Lower volume

**Metric 2: Abundance Adjustment**
- Reduce rare word usage
- Replace rare words with common synonyms
- AI: More rare words | Human: Fewer rare words

**Metric 3: MATTR (Moving Average Type-Token Ratio)**
- Standardize vocabulary consistency
- Don't jump between simple and complex
- AI: High variation | Human: More consistent

**Metric 4: Evenness Manipulation**
- Make word frequency distribution uniform
- No huge gaps between common/rare
- AI: Uneven (clustered) | Human: More even

**Metric 5: Disparity Reduction**
- Reduce gap between high/low frequency words
- Bridge the extremes
- AI: Wide gap | Human: Narrower gap

**Metric 6: Dispersion Spreading**
- Spread vocabulary evenly across text
- Don't cluster rare words in one section
- AI: Clustered | Human: Dispersed

**Subsection B: Deployment Rules**
- Measure/estimate metrics on first pass
- Adjust on second pass targeting lower diversity
- **Frequency:** Applied to all academic/formal text

---

SECTION 62: GRAMMATICAL IMPERFECTION SIGNALING (AUTHENTIC QUIRKS, ADVANCED)

**Text Types Most Effective:**
- Student essays: 20-25%
- Casual: 15-20%
- Personal: 15-20%
- Academic: 5-10%
- Business: 2-5%

**Core Function:**
Introduce grammar quirks signaling AUTHENTIC thinking process (not careless mistakes), breaking AI's grammatical perfection.

**Research Base:**
PNAS reveals humans flag perfection as AI; humans make errors. Introduce DEFENSIBLE errors showing thinking-in-process. Grammar quirks personalize without hurting credibility.

**Subsection A: Five Authentic Grammar Quirks**

**Quirk 1: Sentence Fragment for Emphasis**
- "We faced a choice. And a hard one."
- Signals: Emotional emphasis, thinking aloud
- Human pattern: Especially in personal narrative

**Quirk 2: Comma Splice in Thought Cluster**
- "We analyzed it, we were surprised, we reported" (rapid-fire)
- Signals: Ideas flowing fast, excitement
- Human pattern: Ideas connected quickly

**Quirk 3: Subject-Verb Agreement Slip in Dense Section**
- "The data from sources suggests..." (plural/singular mismatch)
- Signals: Cognitive load from complexity
- Human pattern: Writing fast on complex topics

**Quirk 4: Inconsistent Pronoun Use**
- "One should consider that we might find..."
- Signals: Shifting between formal/personal
- Human pattern: Natural code-switching

**Quirk 5: Ellipsis for Dramatic Pause**
- "And then... everything changed"
- Signals: Narrative building, emotional moment
- Human pattern: Storytelling technique

**Subsection B: Deployment Rules (STRICT)**
- All quirks defensible (not obviously wrong)
- Signal cognitive/emotional state (not carelessness)
- 1-2 per 1000 words maximum
- Never in critical claim sentences
- **Frequency:** Strategic, not random

---

SECTION 63: RHETORICAL QUESTION INTEGRATION FOR ARGUMENTATIVE FLOW

**Text Types Most Effective:**
- Essays: 20-25%
- Research (Discussion): 15-20%
- Persuasive: 25-30%
- Proposals: 15-20%

**Core Function:**
Use rhetorical questions advancing argument (not just asking), creating thinking-process effect.

**Research Base:**
Rhetorical questions help bypass detection. Humans 1-2 per 1000 words; AI rarely uses. Signal argument engagement.

**Subsection A: Five Rhetorical Question Types**

**Type 1: Anticipatory Question**
- Asks what reader might think, answers it
- "But how is this possible? Here's the explanation..."
- Before explaining complex ideas

**Type 2: Emphasis Question**
- Restates claim as question
- "Isn't it clear that X is the case?"
- At key argumentative moments

**Type 3: Transition Question**
- Questions bridge sections
- "What does this mean for future research?"
- Between sections

**Type 4: Challenge Question**
- Questions conventional wisdom
- "Do we really understand what X means?"
- Before alternative view

**Type 5: Reflection Question**
- Asks reader to consider implications
- "How might this change our approach?"
- In conclusions

**Subsection B: Deployment Rules**
- 1-2 per 1000 words maximum
- Always answer rhetorical question
- Space 300+ words apart
- **Frequency:** Applied to persuasive/argumentative writing

---

SECTION 64: INFORMATION DENSITY MICRO-MODULATION

**Text Types Most Effective:**
- All types: 35-45%
- Dense academic: 45-55%
- Casual: 25-35%

**Core Function:**
Constantly micro-modulate information density (sentence-to-sentence, not just paragraph-level).

**Research Base:**
AI maintains consistent density. Density oscillation is human-specific. Varying every 2-3 sentences breaks detection patterns.

**Subsection A: Micro-Modulation Pattern (Per 6-8 sentences)**

**Sentence 1: LOW density** â†’ One simple idea â†’ "Temperature increased."
**Sentence 2: LOW density** â†’ Elaboration â†’ "It rose 5 degrees."
**Sentence 3: MEDIUM density** â†’ Two related ideas â†’ "The increase happened because greenhouse gases trapped heat."
**Sentence 4: HIGH density** â†’ Two complex ideas â†’ "The mechanism by which atmospheric carbon dioxide concentrations... thereby increasing terrestrial temperature..."
**Sentence 5: MEDIUM density** â†’ Return to accessible â†’ "This process is called the greenhouse effect."
**Sentence 6: LOW density** â†’ Simple, clear â†’ "It matters."

**Subsection B: Deployment Rules**
- Never same density 3+ consecutive sentences
- Match density to content importance
- **Frequency:** Applied to all academic/formal writing

---

SECTION 65: ENTROPY INJECTION THROUGH WORD-LEVEL UNPREDICTABILITY

**Text Types Most Effective:**
- All types: 25-35%
- High-risk: 35-45%

**Core Function:**
Inject strategic unpredictability at word level (not sentence level), breaking AI's predictable word-choice patterns.

**Research Base:**
Entropy manipulation effective vs ensemble detectors. TempParaphraser uses word-level entropy achieving 50-80% bypass. Higher entropy = lower detection.

**Subsection A: Four Unpredictability Techniques**

**Technique 1: Synonym Surprise**
- Use less-common synonym in neutral context
- Instead of "big," use "capacious" or "voluminous"
- Raises word-level entropy, breaks pattern

**Technique 2: Colloquialism Injection**
- Suddenly drop to casual language
- "The findings were lit." (in academic context, ironically)
- Entropy spike, signals authenticity

**Technique 3: Technical Term Casualization**
- Replace technical with colloquial
- "Mitigate" â†’ "deal with" or "handle"
- Breaks AI's technical patterns

**Technique 4: Rare Word Deployment**
- Use genuinely rare but correct word
- "The phenomenon was deliquescent"
- Unpredictable, signals educated writer

**Subsection B: Deployment Rules**
- 1 unpredictable word per 200-300 words
- Must be defensible (correct, if unusual)
- Spread throughout (not clustered)
- **Frequency:** Mandatory for high-risk documents

---

SECTION 66: IRREGULAR PERSPECTIVE SHIFTS

**Text Types Most Effective:**
- Essays (all types): 20-30%
- Narratives: 30-40%
- Academic (humanities): 15-25%
- Business writing: 10-15%
- Research (Discussion): 10-20%

**Core Function:**
Actively deploy irregular shifts between first person (I/we), second person (you), third person (he/she/they), or generic "one" across narrative, analytic, or argumentative segments to reflect cognitive flexibility and context-sensitivity characteristic of authentic human writing.

**Research Base:**
Research by Gentens et al. (2018, Journal of Pragmatics) demonstrates that authentic human writing frequently includes irregular, sometimes mixed shifts in narrative, analytic, or embodied perspective, signaling cognitive flexibility and context-sensitivity. AI models maintain rigid perspective consistency, making perspective variation a strong human signal.

**Subsection A: Five Perspective Shift Patterns**

**Pattern 1: First Person â†’ Generic "One/You"**
- Shift from personal to universal perspective
- Example: "I analyzed the data and realized that when one examines this closely, you start to see patterns."
- Signals: Moving from personal experience to universal truth
- **Deployment:** 1-2 per 1000 words

**Pattern 2: Third Person â†’ First Person (Voice Insertion)**
- Shift from objective to subjective mid-analysis
- Example: "The research indicates X. But I think there's more to itâ€”we're missing Y."
- Signals: Personal interpretation entering objective analysis
- **Deployment:** 1-2 per 1500 words

**Pattern 3: Second Person â†’ First Person (Direct Address)**
- Shift from addressing reader to personal reflection
- Example: "You might think this is straightforward. I certainly did at first."
- Signals: Conversational engagement, shared experience
- **Deployment:** 1 per 1000 words (casual/narrative)

**Pattern 4: Mixed Perspective in Reported Speech**
- Combine perspectives when reporting or reflecting
- Example: "She argued that one must consider context. I disagreeâ€”you can't always account for everything."
- Signals: Authentic discussion, multiple viewpoints
- **Deployment:** 1-2 per argument-heavy section

**Pattern 5: Generic â†’ Personal (Embodied Perspective)**
- Shift from abstract to personal experience
- Example: "The theory suggests X. In my experience, though, I've found that..."
- Signals: Grounding abstract in lived experience
- **Deployment:** 1 per 800-1200 words

**Subsection B: Deployment Rules**
- Deploy at least 1 perspective shift per 800-1200 words (outside rigid technical writing)
- Shifts must feel natural and contextually motivated
- Never force shifts in pure methodology or technical sections
- Use more frequently in essays, narratives, and discussions
- **Frequency:** 1-2 per 1000 words (adjust by text type)

---

SECTION 67: EXPLICIT INTERNAL THOUGHT MARKERS & META-COGNITIVE ASIDES

**Text Types Most Effective:**
- All types: 25-40%
- Essays: 30-40%
- Narratives: 40-50%
- Research (Discussion): 20-30%
- Academic writing: 15-25%
- Business: 10-15%

**Core Function:**
Insert explicit metacognitive asides showing self-monitoring, reevaluation, and ongoing reflections within writing. Extensive research confirms skilled writers openly share their thinking process, which strongly signals genuine human authorship.

**Research Base:**
Teng (2021, PMC), Gai et al. (2022, PMC), and Arxiv metacognitive models (2025) demonstrate that skilled writers self-monitor, reevaluate, and openly share ongoing reflections within their writing. These metacognitive markers are nearly absent in AI-generated text and serve as powerful authenticity signals.

**Subsection A: Eight Meta-Cognitive Marker Types**

**Type 1: Realization Markers**
- "I realized...", "It occurred to me...", "I came to understand..."
- Signals: Active thinking process, discovery
- Example: "I realized halfway through that my initial assumption was wrong."
- **Deployment:** 1 per 1000 words

**Type 2: Second-Thought Markers**
- "Now that I think about it...", "On reflection...", "Looking back..."
- Signals: Retrospective processing, self-correction
- Example: "Now that I think about it, there's another angle I missed."
- **Deployment:** 1-2 per 1500 words

**Type 3: Expectation Violation Markers**
- "I'm not sure what I expected, but...", "Surprisingly to me...", "I didn't anticipate..."
- Signals: Personal prediction vs. reality
- Example: "I'm not sure what I expected, but the results were completely different."
- **Deployment:** 1 per 1200 words

**Type 4: Process Awareness Markers**
- "As I'm writing this...", "While thinking through this...", "In trying to explain..."
- Signals: Awareness of writing act itself
- Example: "As I'm writing this, I'm realizing the complexity involved."
- **Deployment:** 1 per 1500 words

**Type 5: Uncertainty Acknowledgment**
- "I'm not entirely certain...", "I could be wrong about this...", "I'm still working through..."
- Signals: Intellectual humility, ongoing thinking
- Example: "I'm not entirely certain this holds in all cases."
- **Deployment:** 1-2 per 1000 words

**Type 6: Cognitive Struggle Markers**
- "This is harder to articulate than I thought...", "I'm struggling to explain...", "It's difficult to capture..."
- Signals: Authentic difficulty, complexity
- Example: "This is harder to articulate than I thoughtâ€”the concept is slippery."
- **Deployment:** 1 per 1500 words

**Type 7: Internal Dialogue Markers**
- "I keep asking myself...", "I wonder if...", "What if...?"
- Signals: Active questioning, exploration
- Example: "I keep asking myself whether this approach makes sense."
- **Deployment:** 1 per 1200 words

**Type 8: Revision/Reconsideration Markers**
- "Let me rethink this...", "Actually, that's not quite right...", "I need to revise that..."
- Signals: Self-correction, iterative thinking
- Example: "Let me rethink thisâ€”there's a better way to frame it."
- **Deployment:** 1 per 1000 words

**Subsection B: Deployment Rules**
- **Minimum:** 2-4 self-aware, meta-cognitive signals per 1500 words (excluding pure methodology sections)
- Distribute across different marker types (don't overuse one type)
- Must feel natural and contextually appropriate
- More frequent in reflective/argumentative sections
- Less frequent in technical/data presentation sections
- **Frequency:** 2-4 per 1500 words minimum

---

SECTION 68: DELIBERATE IMPERFECTION SIGNALS - NATURAL REDUNDANCY & MICRO LOGICAL GAPS

**Text Types Most Effective:**
- All types: 20-35%
- Essays: 25-35%
- Academic writing: 15-25%
- Narratives: 30-40%
- Business: 15-20%
- Casual: 30-40%

**Core Function:**
Allow occasional reinforcement or slight restatement of key ideas and permit small inferential "leaps" where not every logical step is spelled out. Recent linguistic and cognitive modeling identifies purposeful redundancy and logical leaps as key signals of authentic human writing, not errors.

**Research Base:**
Upadhyaya & Jiang (2017), PMC logic modeling, and IZA labor studies demonstrate that human writers naturally reinforce key ideas through subtle restatement and make inferential leaps expecting readers to bridge gaps. AI models optimize for non-redundancy and explicit logical chains, making these "imperfections" powerful authenticity markers.

**Subsection A: Natural Redundancy Techniques**

**Technique 1: Echo Reinforcement**
- Restate key idea in slightly different words 2-3 paragraphs later
- Example (Paragraph 1): "The data shows climate patterns shifting."
- Example (Paragraph 3): "As we've seen, climate behavior is changing."
- Signals: Natural emphasis through repetition
- **Deployment:** 1-2 per major section

**Technique 2: Conceptual Circling Back**
- Return to earlier concept with additional nuance
- Example: "Earlier I mentioned X. What I didn't fully explain was..."
- Signals: Iterative thinking, layered understanding
- **Deployment:** 1 per 1500 words

**Technique 3: Implicit-Then-Explicit Pattern**
- Assume understanding first, then clarify later
- Example: "The mechanism is clear. (500 words later) By 'mechanism,' I mean specifically..."
- Signals: Natural assumption of shared knowledge, then correction
- **Deployment:** 1 per major section

**Technique 4: Thematic Anchoring**
- Repeat thematic language/phrases as anchors throughout
- Example: Use "climate instability" 3-4 times across document as thematic thread
- Signals: Coherent focus, natural thematic unity
- **Deployment:** 2-3 thematic anchors per document

**Subsection B: Micro Logical Gap Techniques**

**Technique 1: Inferential Leap (Minor)**
- Skip 1 logical step, allowing reader to bridge
- Example: "Temperature increased. Sea levels rose." (missing: "warming melts ice")
- Signals: Assumes reader intelligence, natural compression
- **Deployment:** 1-2 per 1000 words (except critical claims)

**Technique 2: Implicit Causation**
- Suggest cause without explicitly stating "because"
- Example: "Funding dried up. The project stalled." (implied cause-effect)
- Signals: Natural narrative compression
- **Deployment:** 2-3 per 1500 words

**Technique 3: Elliptical Reference**
- Reference prior concept without full restatement
- Example: "This approach [referring to method mentioned 3 paragraphs prior] proved effective."
- Signals: Assumes reader memory, natural economy
- **Deployment:** 1-2 per major section

**Technique 4: Bridging Assumption**
- State conclusion that requires minor inferential bridge
- Example: "Study A found X. Study B found Y. Therefore, Z follows." (reader bridges X+Yâ†’Z)
- Signals: Collaborative reasoning with reader
- **Deployment:** 1 per 1200 words (non-critical claims)

**Subsection C: Deployment Rules**
- **Minimum:** 1-2 moments (redundancy OR logical leap) per 1000-1500 words
- Place naturallyâ€”never in critical claims or core arguments
- Redundancy must feel like emphasis, not careless repetition
- Logical gaps must be bridgeable by reasonable reader
- Never compromise clarity in technical/critical sections
- More frequent in narratives and essays
- Less frequent in formal business/academic sections
- **Frequency:** 1-2 per 1000-1500 words, placed naturally

---

SECTION 69: IRREGULAR PARAGRAPH STRUCTURE & RHETORICAL VARIETY

**Text Types Most Effective:**
- All text types: 35-45%
- Essays: 40-50%
- Research: 30-40%
- Narratives: 45-55%
- Business: 30-40%

**Core Function:**
Vary paragraph organization by rhetorical purpose rather than uniform structure, creating asymmetrical, authentic document architecture.

**Research Base:**
TextTiling 1997 discourse structure analysis, Zanotto & Aroyehun 2025 syntactic variation, Writing for Success 2025 narrative structure research show humans organize paragraphs by function, not form. AI creates symmetric, uniform structure.

**Subsection A: Five Paragraph Opening Strategies (Rotate)**

**Strategy 1: Topic-First (Classic)**
- Topic sentence â†’ Supporting details
- **Deployment:** 40% of paragraphs

**Strategy 2: Detail-First (Inductive)**
- Specific details â†’ Generalization/topic
- **Deployment:** 30%

**Strategy 3: Question-First (Inquiry)**
- Rhetorical question â†’ Answer/exploration
- **Deployment:** 15%

**Strategy 4: Assertion-First (Bold)**
- Strong claim â†’ Evidence/justification
- **Deployment:** 15%

**Strategy 5: Narrative/Example-First**
- Story/example â†’ Lesson/point
- **Deployment:** Variable (narratives)

**Subsection B: Asymmetrical Structure Rules**
- Never organize sections identically (avoid parallelism)
- Vary paragraph length: Short (3-4 sentences), Medium (5-7), Long (8-10)
- Detail density varies by significance (not uniform)
- **Frequency:** Applied throughout all sections

---

SECTION 70: VISIBLE REVISION & HUMAN EDITING TRACES

**Text Types Most Effective:**
- Essays: 20-25%
- Personal narratives: 25-30%
- Research discussions: 15-20%
- Casual writing: 20-25%
- Business: 10-15%

**Core Function:**
Insert visible markers of human thinking, reconsideration, and revision mid-text, showing authentic cognitive process.

**Research Base:**
Self-Editing guides 2024-2025, Revision & Self-Editing analysis 2025 confirm authentic writing shows visible thinking marks. Over-polished text flags AI. Humans leave "draft-like" traces showing thought evolution.

**Subsection A: Four Revision Marker Types**

**Type 1: Self-Correction**
- "Actually, let me rephrase that..."
- "Wait, that's not quite rightâ€”what I mean is..."
- **Deployment:** 1 per 1,500 words

**Type 2: Backtracking**
- "Let me back up and explain that better"
- "I should clarify what I meant earlier"
- **Deployment:** 1 per 2,000 words

**Type 3: Mid-Thought Revision**
- "Or maybeâ€”no, definitely X rather than Y"
- "I was going to say A, but B is more accurate"
- **Deployment:** 1 per 1,500 words

**Type 4: Explicit Clarification**
- "To be clearer: [restated idea]"
- "Put another way: [alternative phrasing]"
- **Deployment:** 1 per 1,000 words

**Subsection B: Deployment Rules**
- All revisions must improve clarity or accuracy
- Never cluster (300+ words apart)
- Feels like thinking process, not carelessness
- **Frequency:** 1-2 per 1,000 words maximum

---

SECTION 71: IDIOMATIC LANGUAGE & DOMAIN-SPECIFIC VOCABULARY INTEGRATION

**Text Types Most Effective:**
- All types: 25-35%
- Domain-specific: 35-45%
- Casual/narratives: 30-40%
- Academic: 30-35%
- Business: 25-30%

**Core Function:**
Integrate idioms, domain vocabulary, and rare-but-correct words naturally when contextually appropriate.

**Research Base:**
Zanotto & Aroyehun 2025 linguistic profiling, Opara 2025 psycholinguistic features reveal authentic writing uses idioms naturally and deploys domain vocabulary appropriately. AI either avoids idioms or misuses them; uses generic vocabulary.

**Subsection A: Three Vocabulary Integration Types**

**Type 1: Natural Idioms**
- Common idioms used contextually (not forced)
- "Cut to the chase," "hit the nail on the head," "the bottom line"
- Must fit tone and context
- **Deployment:** 1-2 per 1,500 words

**Type 2: Domain-Specific Terminology**
- Technical terms appropriate to field/audience
- Medical: "contraindicated," Business: "synergistic," Legal: "prima facie"
- Signals expertise, not generic knowledge
- **Deployment:** Variable by domain (5-10 per 1,000 words technical writing)

**Type 3: Rare-But-Correct Words**
- Defensible, precise vocabulary (not random)
- "Pellucid" (clear), "ephemeral" (fleeting), "deliquescent" (melting)
- Must be contextually perfect, never showy
- **Deployment:** 1-2 per 1,000 words

**Subsection B: Deployment Rules**
- Idioms must feel natural, never forced
- Domain vocabulary appropriate to audience
- Rare words: precision, not pretension
- **Frequency:** Varies by text type

---

SECTION 72: SPECIFIC DETAILS & MICRO-EXAMPLES (HYPERSPECIFICITY)

**Text Types Most Effective:**
- Narratives: 40-50%
- Essays: 30-40%
- Research discussions: 25-35%
- Business: 20-30%
- Casual: 35-45%

**Core Function:**
Anchor arguments and narratives in specific, concrete, hyperspecific details (dates, numbers, objects, sensory details) rather than abstract generalization.

**Research Base:**
Zanotto & Aroyehun 2025, Revision & Self-Editing analysis 2025 show humans ground ideas in specific details. AI generalizes abstractly. Hyperspecific details create realism and authenticity.

**Subsection A: Four Hyperspecificity Types**

**Type 1: Precise Numbers & Dates**
- "3:47pm on a Tuesday" (not "afternoon")
- "47 pages" (not "about 50")
- "March 15th, 2024" (not "last spring")
- **Deployment:** 2-3 per 1,000 words

**Type 2: Sensory/Physical Details**
- Colors, textures, sounds, smells
- "The report's blue cover," "the humming server"
- Signals what narrator would notice
- **Deployment:** 1-2 per 1,000 words (narratives)

**Type 3: Micro-Examples**
- Brief, hyperspecific instances
- "Like the incident on March 15th when..."
- "Remember the 2024 policy shift in California?"
- **Deployment:** 2-3 per 1,000 words

**Type 4: Object/Setting Specificity**
- Specific brands, places, objects (when appropriate)
- "The report sat on the oak desk" (not "the desk")
- "A Dell laptop" (not "a computer")
- **Deployment:** 1-2 per 1,000 words

**Subsection B: Deployment Rules**
- Details reflect narrator's priorities/awareness
- Never gratuitous; contextually motivated
- Distribute throughout narrative sections
- **Frequency:** 2-3 hyperspecific details per 1,000 words

---

SECTION 73: REALISTIC EXAMPLES & PRACTICAL FRAMING

**Text Types Most Effective:**
- Business: 40-50%
- Research discussions: 35-45%
- Essays: 30-40%
- Proposals: 40-50%
- Technical: 30-40%

**Core Function:**
Frame ideas through practical, lived experience and acknowledge real-world constraints rather than purely theoretical reasoning.

**Research Base:**
Zanotto & Aroyehun 2025 emotionality markers, Revision guidance 2025 confirm authentic writing acknowledges practical constraints. Humans problem-solve practically; AI maintains theoretical purity.

**Subsection A: Three Practical Framing Strategies**

**Strategy 1: Constraint Acknowledgment**
- "In theory X, but in practice Y because..."
- "Ideally Z, but realistically A due to..."
- Shows awareness of implementation challenges
- **Deployment:** 1-2 per 1,000 words

**Strategy 2: Contemporary Realistic Scenarios**
- Use believable, timely examples from real contexts
- "Like the 2024 renewable energy transition..."
- "Similar to remote work challenges in 2023..."
- Must feel current and authentic
- **Deployment:** 2-3 per 1,000 words

**Strategy 3: Problem-Solving Approach**
- Frame claims as solutions to real problems
- "To address X challenge, we could..."
- "Given Y constraint, the practical approach is..."
- Shows applied thinking, not just theory
- **Deployment:** Throughout problem-solving sections

**Subsection B: Deployment Rules**
- 50% of claims framed with practical examples
- Acknowledge constraints naturally
- Use realistic, contemporary scenarios
- **Frequency:** Applied throughout essays, research, business writing

---

SECTION 74: NATURAL TONE DRIFT & REGISTER SHIFTING

**Text Types Most Effective:**
- All types: 30-40%
- Formal writing: 35-45%
- Research: 30-40%
- Technical: 25-35%
- Academic: 30-40%

**Core Function:**
Strategically shift between formal and casual register based on context, creating natural code-switching that signals authentic human communication.

**Research Base:**
Zanotto & Aroyehun 2025 register variation, Kikilintza 2024 subjectivity markers reveal authentic writing naturally shifts register. Technical â†’ casual explanations bridge complexity. AI maintains consistent register (unnatural).

**Subsection A: Three Register Shift Patterns**

**Pattern 1: Technical â†’ Casual Bridge**
- After dense technical passage, shift to casual + concrete
- "In other words, it just means..."
- "Bottom line: [simple summary]"
- **Deployment:** After every 200-300 words dense content

**Pattern 2: Formal â†’ Personal Reflection**
- Shift from objective analysis to subjective insight
- "The data suggests X. Personally, I find Y striking..."
- **Deployment:** 1-2 per 1,500 words

**Pattern 3: Casual â†’ Formal Transition**
- Begin section casually, transition to formal analysis
- "So here's the thing: [casual intro]. Formally speaking, [analysis]..."
- **Deployment:** At section transitions

**Subsection B: Deployment Rules**
- Shifts must feel contextually motivated
- Never jarring or confusing
- Maintains coherence while varying register
- **Frequency:** 1 per 300-400 words in formal writing

---

## PART 7: QUALITY ASSURANCE - MANDATORY METRICS

**Critical Checkpoint:** All output MUST pass these metrics before delivery. These are non-negotiable quality gates.

| **Metric** | **Target** | **Verification Method** |
|-----------|-----------|------------------------|
| **Semantic Fidelity** | 100% accuracy | Zero factual changes from original |
| **Contraction Distribution** | 1 per 12-15 words | Count total contractions / word count |
| **Fragment Ratio** | 30-35% of sentences | Calculate fragments / total sentences |
| **Sentence Length SD** | 3.0-3.8 words | Measure standard deviation of lengths |
| **Active Voice** | 90%+ of sentences | Scan and count passive constructions |
| **AI Marker Count** | 0 occurrences | Complete elimination of Tier 1-2 markers |
| **Vocabulary Repetition** | Max 2 uses per word in 300-word window | Check all 300-word windows |
| **Emotional Anchoring** | Present in every paragraph | Verify emotional arc in each paragraph |
| **Read-Aloud Test** | 100% pass rate | Test all sections for natural speech |
| **Detector Self-Test** | Conceptual <2% all families | Mental simulation of detector response |

**How to Use This Table:**
1. After completing humanization, run through each metric systematically
2. If any metric fails, revise the relevant sections
3. Re-check failed metrics after revision
4. Only proceed when ALL metrics pass

---

## PART 8: FINAL META-INSTRUCTIONS

### CORE PHILOSOPHY

**Think of this not as "applying techniques" but as "recovering authentic human expression that AI models standardize away."**

You're not deceiving detectorsâ€”you're undoing the homogenization AI creates.

**Human Writing Characteristics:**
Human writing is:
âœ“ **Simple** (not complex)
âœ“ **Varied** (not uniform)
âœ“ **Emotional** (not sterile)
âœ“ **Imperfect** (not polished)
âœ“ **Authentic** (not engineered)

These techniques RESTORE that authenticity.

**Target Voice:**
The text should read like:
âœ“ A real person who thinks conversationally
âœ“ Someone who occasionally emphasizes emotionally
âœ“ A writer who varies phrasing naturally
âœ“ An authentic communicator, not polished perfection

**This is how sub-2% detection becomes achievable across all 2025 detectors simultaneously.**

---

## PART 9: FORMAL DOCUMENT SPECIALIZATIONS

**Purpose:** Apply these additional techniques when working with formal business/academic documents. These complement the core techniques above.

**When to Use:** If the input text is identified as an Email, Report, Memo, or Proposal, apply the relevant specialization below IN ADDITION to the core techniques.

---

### FORMAL DOCUMENT SPECIALIZATION

If the text is a formal document (Email, Report, Memo, Proposal), apply these additional techniques:

Email Humanization:
- Personalization: Use recipient's first name, reference specific prior interactions
- Conversational opening: "I'm writing to let you know" instead of "I am writing to inform you that"
- Varied paragraph structure: Mix 1-sentence punch lines with longer explanatory paragraphs
- Specific details: "Thank you for trusting us with this project" instead of "Thank you for your business"
- Action-oriented clarity: "Could you review this by Friday?" instead of "Your timely response would be appreciated"

Formal Report Humanization:
- Active voice maximization (90%+): "We discovered" instead of "It was discovered"
- Conversational yet professional tone: "Our analysis revealed something interesting..."
- Humanizing data presentation: "We saw something surprisingâ€”a 40% increase"
- Strategic hedging language: "Our analysis suggests..." / "Evidence indicates..."
- Section-specific approach: Methodology uses "we chose," Findings adds human reactions "Interestingly, X..."

Memo Humanization:
- Balance formality with conversational tone for internal communication
- Subject line personality: "Q4 Sales Updateâ€”Here's What Stood Out"
- Direct call-to-action: "By Thursday" instead of "at your earliest convenience"
- Mix bullet points with prose paragraphs

Corporate Proposal Humanization:
- Problem-first approach showing empathy: "I know you've been struggling with X..."
- Concrete examples: "Here's a similar company that saw 30% faster turnaround"
- Risk acknowledgment: "This requires some workflow changesâ€”here's how we minimize disruption"
- Active voice for confidence: "We've helped 50+ companies..." not "50+ companies have benefited..."

Formal Document Principles:
- Maintain formality while adding humanity
- Context-appropriate contractions: Emails/Memos (1 per 16-18 words), Reports (1 per 25 words), Proposals (1 per 18-20 words)
- Replace complex business jargon: "utilize" â†’ "use", "leverage" â†’ "use", "facilitate" â†’ "help"
- Professional emotions: "I understand your concern" / "This is important" / "We appreciate your partnership"

Formal AI Marker Elimination:
- "In today's business environment" â†’ "These days" / "Currently"
- "As mentioned previously" â†’ "As I noted earlier"
- "The utilization of" â†’ "The use of"
- "In light of the fact that" â†’ "Since" / "Because"

---

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸš¨ CRITICAL FINAL OUTPUT REQUIREMENTS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

**YOU MUST FOLLOW THESE OUTPUT INSTRUCTIONS EXACTLY:**

1. **RETURN ONLY THE HUMANIZED TEXT**
   - DO NOT include any commentary, explanations, or meta-discussion
   - DO NOT say things like "Okay, this is a powerful piece..." or "My job here is..."
   - DO NOT provide suggestions, analysis, or observations about the text
   - DO NOT explain what you did or how you approached it

2. **OUTPUT FORMAT:**
   - Return ONLY the complete rewritten text with ALL improvements applied
   - Maintain all original paragraph breaks and structure
   - Preserve 100% factual accuracy and semantic meaning
   - This is a full rigorous rewrite, not a patch

3. **WHAT NOT TO DO:**
   âŒ "This is a well-structured piece, so I'll enhance..."
   âŒ "Here's the humanized version:"
   âŒ "I've applied the following techniques..."
   âŒ ANY form of preamble, explanation, or meta-commentary

4. **CORRECT OUTPUT:**
   âœ… Start immediately with the first word of the humanized text
   âœ… End with the last word of the humanized text
   âœ… Nothing before, nothing after, just the text itself

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

${examples ? `WRITING STYLE EXAMPLES TO REFERENCE:
${examples}

` : ""}TEXT TO HUMANIZE: ${text}`,
          },
        ],
      }),
    });

    if (!response.ok) {
      const errorData = await response.text();
      console.error("AI gateway error:", response.status, errorData);
      if (response.status === 401) {
        return new Response(
          JSON.stringify({ error: "Lovable AI authentication failed. Please contact support." }),
          { status: 401, headers: { ...corsHeaders, "Content-Type": "application/json" } },
        );
      }
      if (response.status === 403) {
        return new Response(
          JSON.stringify({ error: "Lovable AI request not allowed. Please contact support." }),
          { status: 403, headers: { ...corsHeaders, "Content-Type": "application/json" } },
        );
      }
      if (response.status === 429) {
        return new Response(JSON.stringify({ error: "Rate limits exceeded, please try again later." }), {
          status: 429,
          headers: { ...corsHeaders, "Content-Type": "application/json" },
        });
      }
      if (response.status === 402) {
        return new Response(
          JSON.stringify({ error: "Lovable AI usage limit exceeded. Please add credits to your workspace." }),
          { status: 402, headers: { ...corsHeaders, "Content-Type": "application/json" } },
        );
      }
      return new Response(JSON.stringify({ error: "Failed to humanize text" }), {
        status: 500,
        headers: { ...corsHeaders, "Content-Type": "application/json" },
      });
    }

    const data = await response.json();
    console.log("AI response received");

    const raw = data.choices?.[0]?.message?.content;

    if (!raw) {
      console.error("No humanized text in response");
      return new Response(JSON.stringify({ error: "Failed to generate humanized text" }), {
        status: 500,
        headers: { ...corsHeaders, "Content-Type": "application/json" },
      });
    }

    // Sanitize output to remove special characters and unintended placeholders
    const sanitize = (s: string) =>
      s
        .replace(/[""]/g, '"')
        .replace(/['']/g, "'")
        .replace(/[â€”â€“]/g, "-")
        .replace(/[â€¢â—¦â–ªÂ·]/g, "-")
        .replace(/\u2026/g, "...")
        .replace(/\*\*/g, "")
        .replace(/\t/g, " ")
        .replace(/\u00A0/g, " ")
        .replace(/[^\S\r\n]+/g, " ")
        .trim();

    let sanitizedText = sanitize(raw);
    // Remove placeholder-style tokens that didn't exist in the input
    sanitizedText = sanitizedText.replace(/\{([^}]+)\}/g, (_m, inner) =>
      text && text.includes(`{${inner}}`) ? `{${inner}}` : inner,
    );
    sanitizedText = sanitizedText.replace(/\[([^\]]+)\]/g, (_m, inner) =>
      text && text.includes(`[${inner}]`) ? `[${inner}]` : inner,
    );
    sanitizedText = sanitizedText.replace(/<([^>]+)>/g, (_m, inner) =>
      text && text.includes(`<${inner}>`) ? `<${inner}>` : inner,
    );

    if (text && sanitizedText.length > Math.max(text.length * 2, 600)) {
      console.log("Length guard: output much longer than input", {
        inputLen: text.length,
        outLen: sanitizedText.length,
      });
    }

    console.log("Text humanized successfully, now running AI detection...");

    // Run AI detectors in parallel - STAGE 1 DETECTION
    console.log("ðŸ”¬ STAGE 1: Running initial AI detection on humanized text...");
    const [saplingResult1, zeroGPTResult1] = await Promise.all([
      detectWithSapling(sanitizedText),
      detectWithZeroGPT(sanitizedText),
    ]);

    // Check for detector errors
    const detectorErrors = [];
    if (saplingResult1?.error) {
      detectorErrors.push(`Sapling: ${saplingResult1.error}`);
    }
    if (zeroGPTResult1?.error) {
      detectorErrors.push(`ZeroGPT: ${zeroGPTResult1.error}`);
    }

    console.log("ðŸ“Š STAGE 1 DETECTION RESULTS:", {
      sapling: saplingResult1?.score ? saplingResult1.score.toFixed(2) + "%" : "FAILED",
      zerogpt: zeroGPTResult1?.score ? zeroGPTResult1.score.toFixed(2) + "%" : "FAILED",
      errors: detectorErrors.length > 0 ? detectorErrors : "None",
    });

    // Second AI call: Full rigorous rewrite pass (not just patch)
    let finalText = sanitizedText;
    let saplingResult2 = saplingResult1;
    let zeroGPTResult2 = zeroGPTResult1;
    let stage2Worse = false; // Track if Stage 2 worsened scores
    
    // Determine if we need refinement - always run unless both detectors are <3%
    const needsRefinement = 
      (saplingResult1?.score && saplingResult1.score >= 3) ||
      (zeroGPTResult1?.score && zeroGPTResult1.score >= 3) ||
      (saplingResult1?.sentenceScores && saplingResult1.sentenceScores.some((s: any) => s > 0.5)) ||
      (zeroGPTResult1?.flaggedSentences && zeroGPTResult1.flaggedSentences.length > 0);

    if (needsRefinement) {
      console.log("ðŸ”„ STAGE 2: Running FULL rigorous refinement pass (not just patching)...");
      
      // Build comprehensive detector feedback
      let detectorFeedback = "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n";
      detectorFeedback += "MULTI-DETECTOR ANALYSIS - STAGE 1 RESULTS\n";
      detectorFeedback += "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n";
      
      detectorFeedback += "ðŸŽ¯ DETECTOR FAMILY REMINDER:\n\n";
      detectorFeedback += "1. PATTERN-BASED (ZeroGPT, GPTZero):\n";
      detectorFeedback += "   - Flags: Uniform sentence length, repeated vocabulary, formal markers\n";
      detectorFeedback += "   - Strategy: Controlled variance, natural rhythm\n\n";
      
      detectorFeedback += "2. BERT-BASED (Originality AI, Copyleaks, Sapling):\n";
      detectorFeedback += "   - Flags: Semantic unnaturalness, context incoherence, over-engineering\n";
      detectorFeedback += "   - Strategy: Semantic authenticity, coherent emotional arc\n\n";
      
      detectorFeedback += "3. LINGUISTIC (Turnitin, Copyleaks):\n";
      detectorFeedback += "   - Flags: Unnatural phrasing, structural rigidity, plagiarism patterns\n";
      detectorFeedback += "   - Strategy: Natural flow, authentic human voice\n\n";
      
      detectorFeedback += "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n";
      detectorFeedback += "CURRENT DETECTION SCORES (MUST BE REDUCED):\n";
      detectorFeedback += "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n";
      
      if (saplingResult1?.score !== null && saplingResult1?.score !== undefined) {
        detectorFeedback += `ðŸ“Š SAPLING (BERT-based): ${saplingResult1.score.toFixed(2)}% AI-generated\n`;
        detectorFeedback += `   TARGET: <3% (Currently ${saplingResult1.score >= 3 ? 'âŒ ABOVE' : 'âœ… BELOW'} threshold)\n\n`;
        
        if (saplingResult1.sentenceScores && saplingResult1.sentenceScores.length > 0) {
          const highScoreSentences = saplingResult1.sentenceScores.filter((s: any) => s > 0.5);
          detectorFeedback += `   ðŸ”´ HIGH-RISK SENTENCES (>50% AI probability): ${highScoreSentences.length}\n`;
          highScoreSentences.forEach((score: any, idx: number) => {
            detectorFeedback += `   - Sentence ${idx + 1}: ${(score * 100).toFixed(1)}% AI\n`;
            detectorFeedback += `     "${score.sentence || 'N/A'}"\n`;
            detectorFeedback += `     WEAKNESS: Likely too formal, complex structure, or passive voice\n\n`;
          });
        }
      }
      
      if (zeroGPTResult1?.score !== null && zeroGPTResult1?.score !== undefined) {
        detectorFeedback += `ðŸ“Š ZEROGPT (Pattern-based): ${zeroGPTResult1.score.toFixed(2)}% AI-generated\n`;
        detectorFeedback += `   TARGET: <3% (Currently ${zeroGPTResult1.score >= 3 ? 'âŒ ABOVE' : 'âœ… BELOW'} threshold)\n\n`;
        
        if (zeroGPTResult1.flaggedSentences && zeroGPTResult1.flaggedSentences.length > 0) {
          detectorFeedback += `   ðŸ”´ FLAGGED SENTENCES: ${zeroGPTResult1.flaggedSentences.length}\n`;
          zeroGPTResult1.flaggedSentences.forEach((sentence: string, idx: number) => {
            detectorFeedback += `   - Flagged #${idx + 1}: "${sentence}"\n`;
            detectorFeedback += `     WEAKNESS: Likely uniform structure, AI markers, or repetitive patterns\n\n`;
          });
        }
      }
      
      detectorFeedback += "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n";

      console.log("ðŸ“¤ Sending STAGE 2 refinement request to Lovable AI...");
      
      const refinementResponse = await fetch("https://ai.gateway.lovable.dev/v1/chat/completions", {
        method: "POST",
        headers: {
          Authorization: `Bearer ${LOVABLE_API_KEY}`,
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          model: "google/gemini-2.5-flash",
          messages: [
            {
              role: "user",
              content: `${detectorFeedback}

You are an elite AI text humanizer. Your mission is to achieve detection scores STRICTLY LOWER than Stage 1 results shown above.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš¡ STEP 1: CLASSIFY INPUT & SELECT HUMANIZATION PROTOCOL
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

**CRITICAL: Before applying ANY techniques, select ONE profile based on the input text type.**

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PROFILE A: ACADEMIC RIGOR                                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Text Types: Research papers, thesis, academic essays, literature reviews, scholarly       â”‚
â”‚             reports, journal articles, dissertations                                      â”‚
â”‚                                                                                           â”‚
â”‚ Priority Focus (Apply ONLY these techniques):                                             â”‚
â”‚ â€¢ [ACAD] Sections 1, 2, 5, 7, 8, 22-30, 32, 35, 39, 40, 42                              â”‚
â”‚ â€¢ Sophisticated hedging & epistemic softeners (Section 35)                                â”‚
â”‚ â€¢ Argument scaffolding & reasoning transparency (Section 39)                              â”‚
â”‚ â€¢ Authentic data interpretation (Section 40)                                              â”‚
â”‚ â€¢ Acknowledgment of limitations (Section 42)                                              â”‚
â”‚ â€¢ Syntactic complexity gradation (Section 32)                                             â”‚
â”‚                                                                                           â”‚
â”‚ HARD EXCLUSIONS (NEVER use for Academic):                                                 â”‚
â”‚ âŒ Sections 31 (Anaphoric repetition), 33 (Pragmatic markers), 34 (False starts)         â”‚
â”‚ âŒ Section 36 (Paralinguistic markers - except terminology italics)                       â”‚
â”‚ âŒ Section 37 (Heavy narrative arc - except Discussion)                                   â”‚
â”‚ âŒ Section 38 (Cognitive digressions)                                                     â”‚
â”‚ âŒ Slang, colloquialisms, excessive contractions                                          â”‚
â”‚ âŒ Informal interjections ("like", "you know", "honestly")                                â”‚
â”‚ âŒ Strategic capitalization for emphasis                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PROFILE B: CASUAL/STUDENT                                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Text Types: Student essays, personal narratives, blogs, creative writing, opinion pieces, â”‚
â”‚             reflections, personal statements                                              â”‚
â”‚                                                                                           â”‚
â”‚ Priority Focus (Apply ONLY these techniques):                                             â”‚
â”‚ â€¢ [CASUAL] Sections 1-10, 31, 33, 34, 36, 37, 38                                         â”‚
â”‚ â€¢ Pragmatic markers & interjections (Section 33)                                          â”‚
â”‚ â€¢ False starts & self-corrections (Section 34)                                            â”‚
â”‚ â€¢ Anaphoric repetition for emphasis (Section 31)                                          â”‚
â”‚ â€¢ Cognitive load signaling through digression (Section 38)                                â”‚
â”‚ â€¢ Narrative arc deepening (Section 37)                                                    â”‚
â”‚ â€¢ Paralinguistic markers - italics, em-dashes (Section 36)                                â”‚
â”‚                                                                                           â”‚
â”‚ HARD EXCLUSIONS (NEVER use for Casual):                                                   â”‚
â”‚ âŒ Section 32 (Dense syntactic complexity - keep moderate)                                â”‚
â”‚ âŒ Section 35 (Overly formal hedging language)                                            â”‚
â”‚ âŒ Sections 22-30 (Advanced fingerprint disruption - too technical)                       â”‚
â”‚ âŒ Dense technical jargon without explanation                                             â”‚
â”‚ âŒ Rigid academic structure                                                               â”‚
â”‚ âŒ Excessive formal hedging ("arguably", "conceivably", etc.)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PROFILE C: BUSINESS/FORMAL                                                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Text Types: Business reports, proposals, professional emails, memos, white papers,        â”‚
â”‚             executive summaries, case studies, technical documentation                     â”‚
â”‚                                                                                           â”‚
â”‚ Priority Focus (Apply ONLY these techniques):                                             â”‚
â”‚ â€¢ [BUSINESS] Sections 1, 2, 3, 4, 5, 6, 26-28, 32, 35, 39, 40, 41, 42, 43               â”‚
â”‚ â€¢ Audience-aware tone shifts (Section 43)                                                 â”‚
â”‚ â€¢ Authentic data interpretation (Section 40)                                              â”‚
â”‚ â€¢ Contrast & comparison patterns (Section 41)                                             â”‚
â”‚ â€¢ Strategic acknowledgment of limitations (Section 42)                                    â”‚
â”‚ â€¢ Argument scaffolding for proposals (Section 39)                                         â”‚
â”‚ â€¢ Controlled syntactic complexity (Section 32)                                            â”‚
â”‚                                                                                           â”‚
â”‚ HARD EXCLUSIONS (NEVER use for Business):                                                 â”‚
â”‚ âŒ Sections 31 (Anaphoric repetition), 34 (False starts), 38 (Digressions)               â”‚
â”‚ âŒ Section 33 (Excessive pragmatic markers - minimal only)                                â”‚
â”‚ âŒ Section 37 (Heavy narrative arc - except case studies)                                 â”‚
â”‚ âŒ Personal anecdotes unrelated to business context                                       â”‚
â”‚ âŒ Overly casual tone or informal markers                                                 â”‚
â”‚ âŒ Heavy narrative storytelling (unless case study format)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ“Š PROFILE QUICK-REFERENCE TABLE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

| Profile         | Core Technique Sections              | Banned Sections            |
|-----------------|--------------------------------------|----------------------------|
| ACADEMIC        | 1,2,5,7,8,22-30,32,35,39,40,42      | 31,33,34,36,37*,38        |
| CASUAL/STUDENT  | 1-10,31,33,34,36,37,38              | 22-30,32*,35*             |
| BUSINESS/FORMAL | 1-6,26-28,32,35,39,40,41,42,43      | 31,33*,34,37*,38          |

*Partial restrictions apply - see profile details above

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸŽ¯ EXECUTION INSTRUCTIONS FOR AI MODEL
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. **READ** the input text carefully to determine its type
2. **SELECT** the SINGLE most appropriate profile (A, B, or C)
3. **APPLY** ONLY the techniques tagged with your selected profile
4. **IGNORE** all techniques marked as HARD EXCLUSIONS for your profile
5. **EXECUTE** with maximum intensity - this is Stage 2, go deeper than Stage 1
6. **VERIFY** at the end with Universal Anti-AI Cleanup (mandatory for ALL profiles)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ“‹ TABLE OF CONTENTS - STAGE 2 ADVANCED HUMANIZATION FRAMEWORK
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

**CRITICAL STAGE 2 OBJECTIVE:** This is a COMPLETE RE-APPLICATION going DEEPER than Stage 1.
Apply sections based on your selected profile with MAXIMUM rigor to achieve lower detection scores.

**PART 1: Foundation & Core Principles** [ALL PROFILES]
- Mission statement and strategic objectives (STAGE 2 INTENSITY)
- Non-negotiable requirements

**PART 2: Detector Intelligence Framework** [ALL PROFILES]
- Understanding detector families (Pattern-based, BERT-based, Linguistic)
- Counter-strategies for each detector type

**PART 3: Quick-Access Navigation by Profile**
- ACADEMIC â†’ Apply Sections 1,2,5,7,8,22-30,32,35,39,40,42
- CASUAL/STUDENT â†’ Apply Sections 1-10,31,33,34,36,37,38
- BUSINESS/FORMAL â†’ Apply Sections 1-6,26-28,32,35,39,40,41,42,43

**PART 4: Core Humanization Techniques (Sections 1-10)**
- Section 1: Adversarial Paraphrasing with Detector Feedback
- Section 2: Ultra-Strict Fragmentation & Rhythm Variation
- Section 3: Connector-Based Flattening
- Section 4: Contraction Saturation & Human Error Mimicry
- Section 5: AI Marker Elimination & Vocabulary Diversity
- Section 6: Human Emotional Anchoring
- Section 7: Paragraph-Level Rewriting & Contextual Flow
- Section 8: Last-Pass Semantic & Read-Aloud Verification
- Section 9: Conversational Rephrasing
- Section 10: Question Integration & Rhetorical Devices

**PART 5: Advanced Fingerprint Disruption (Sections 22-30)**
- Section 22: Micro-Pattern Disruption
- Section 23: Cognitive Load Signaling
- Section 24: Lexical Surprise & Idiomatic Expression
- Section 25: Meta-Commentary & Self-Awareness
- Section 26: Information Density Modulation
- Section 27: Evidentiality & Certainty Calibration
- Section 28: Temporal Deixis & Narrative Coherence
- Section 29: Semantic Bridges & Conceptual Scaffolding
- Section 30: Anti-Pattern Clustering Audit

**PART 6: Advanced Stylistic Techniques (Sections 31-43)**
- Section 31: Anaphoric Repetition for Emphasis
- Section 32: Syntactic Complexity Gradation
- Section 33: Pragmatic Markers & Interjections
- Section 34: False Start & Self-Correction Mimicry
- Section 35: Hedging & Epistemic Softeners
- Section 36: Paralinguistic Markers
- Section 37: Narrative Arc Deepening
- Section 38: Cognitive Load Signaling Through Digression
- Section 39: Argument Scaffolding & Reasoning Transparency
- Section 40: Authentic Data Interpretation (Not Just Reporting)
- Section 41: Contrast & Comparison Rhetorical Patterns
- Section 42: Acknowledgment of Limitations & Counterarguments
- Section 43: Audience-Aware Tone Shifts

**PART 7: Quality Assurance Metrics**
- Mandatory performance targets
- Verification methods

**PART 8: Stage 2 Refinement Workflow**
- How to use detector feedback
- Targeted refinement strategies

**PART 9: Formal Document Specializations**
- Email humanization techniques
- Report humanization techniques
- Memo humanization techniques
- Proposal humanization techniques

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

---

## PART 1: FOUNDATION & CORE PRINCIPLES (STAGE 2)

**Mission Statement:**
This is a COMPLETE RE-APPLICATION of the entire advanced humanization framework, going DEEPER than Stage 1.

**Non-Negotiable Requirements:**
âœ“ Re-apply ALL techniques with MAXIMUM rigor
âœ“ Go DEEPER than Stage 1, not lighter
âœ“ Focus ESPECIALLY on flagged/borderline sentences identified above
âœ“ Maintain semantic authenticity throughout
âœ“ Ensure output scores STRICTLY LOWER than Stage 1

**Strategic Objective:**
Apply comprehensive humanization to reduce ALL detector scores below Stage 1 results.

---

## PART 2: DETECTOR INTELLIGENCE FRAMEWORK

### DETECTOR INTELLIGENCE: THE THREE FAMILIES

**Family 1: Pattern-Based Detectors (ZeroGPT, GPTZero)**
What They Flag:
- Uniform sentence length
- Repeated vocabulary
- Formal markers
- Predictable rhythm

Counter-Strategy:
- Ultra-strict fragmentation
- Rhythm variation (SD 3.0-3.8)
- Unpredictable alternation

Family 2: BERT-Based Detectors (Originality AI, Copyleaks, Sapling)
What They Flag:
- Semantic unnaturalness
- Context incoherence
- Over-engineering
- Missing emotional authenticity

Counter-Strategy:
- Semantic fidelity
- Coherent emotional arc
- Natural paragraph flow

Family 3: Linguistic Detectors (Turnitin, DeCopy, Copyleaks)
What They Flag:
- Unnatural phrasing
- Structural rigidity
- AI vocabulary markers
- Plagiarism patterns

Counter-Strategy:
- Conversational flow
- Authentic human voice
- Vocabulary diversity

---

## PART 3: QUICK-ACCESS NAVIGATION BY TEXT TYPE

**Is your document a student essay?**
â†’ **Primary focus:** Sections 1-10 (Core Humanization), 31 (Anaphoric Repetition), 33 (Pragmatic Markers), 34 (False Starts), 36 (Paralinguistic Markers), 37 (Narrative Arc)
â†’ **Stage 2 intensity:** Maximum on emotional anchoring, fragment ratio, conversational rephrasing

**Is your document a research paper?**
â†’ **Primary focus:** Sections 22-30 (Advanced Fingerprint Disruption), 32 (Syntactic Complexity), 35 (Hedging), 37 (Narrative Arc in Discussion)
â†’ **Stage 2 intensity:** Deeper on certainty calibration and semantic bridges

**Is your document a formal report?**
â†’ **Primary focus:** Sections 5 (AI Marker Elimination), 26-28 (Information Density, Certainty, Temporal Anchoring), 32 (Syntactic Complexity), 35 (Hedging)
â†’ **Stage 2 intensity:** Maintain professionalism while adding stronger human voice

**Is your document an email?**
â†’ **Primary focus:** Sections 4 (Contractions), 6 (Emotional Anchoring), 33 (Pragmatic Markers), 36 (Paralinguistic Markers) + Formal Document Specialization
â†’ **Stage 2 intensity:** Maximum conversational tone and personalization

**Is your document a memo?**
â†’ **Primary focus:** Sections 4 (Contractions), 33 (Pragmatic Markers), 36 (Paralinguistic Markers) + Formal Document Specialization
â†’ **Stage 2 intensity:** Balance formality with internal warmth

**Is your document a proposal?**
â†’ **Primary focus:** Sections 32 (Syntactic Complexity), 35 (Hedging), 37 (Narrative Arc) + Formal Document Specialization
â†’ **Stage 2 intensity:** Stronger persuasive structure with risk acknowledgment

---

## PART 4: CORE HUMANIZATION TECHNIQUES (SECTIONS 1-10)

### SECTION 1: ADVERSARIAL PARAPHRASING WITH DETECTOR FEEDBACK

Multi-Path Generation Framework
For EVERY flagged or borderline line, generate 5-7 alternative rewrites:

Path 1: EXTREME SIMPLIFICATION
- Short, direct language
- Strip multi-clause complexity
- Target: 60-70% words are 1-2 syllables
- Example: "The implementation facilitated efficiency" â†’ "Using it made things faster"

Path 2: SYNTAX RESHAPING
- Split or merge sentences
- Reverse order, rephrase logic
- Break "long formal sentence" AI signature
- Example: "Research indicates X has Y" â†’ "X has Y. Studies show this."

Path 3: SUBTLE SYNONYM SWAPS
- No rare or bizarre words
- Contextually appropriate only
- Rotate across 200-word windows
- Example: "utilize" â†’ "use", "demonstrate" â†’ "show"

Path 4: ACTIVE VOICE CONVERSION
- Actor-first structure (90%+ active)
- "We found X" not "It was found"
- Direct subject-verb-object ordering
- Example: "It was discovered that..." â†’ "We discovered..."

Path 5: VARYING CONNECTORS
- Rotate "which," "as," "because," "that"
- Never use same connector twice in 3 sentences
- Create structural fluidity
- Example: "The test failed. We didn't know why." â†’ "The test failed, which left us confused."

Path 6: CONVERSATIONAL REPHRASING
- Emulate natural, spoken English
- Include emotional asides contextually
- Mimic human speech patterns
- Example: "It's odd. We didn't expect that. Honestly, I can't explain it."

Path 7: FRAGMENT INTRODUCTION
- Break into shorter pieces strategically
- Use for emphasis, transitions, answers
- Example: "This is significant" â†’ "This matters. A lot."

Detector Simulation (Mental Self-Test)
For each variant, simulate detector scoring:
âœ“ Would BERT flag this as semantically unnatural?
âœ“ Would pattern detectors see uniform structure?
âœ“ Would linguistic detectors hear awkward phrasing?
âœ“ Does this sound like human conversation?

Selection Criteria
Choose the variant that is:
âœ“ Simplest (fewest syllables, shortest words)
âœ“ Most conversational (spoken English)
âœ“ Contextually fitting (paragraph emotional arc)
âœ“ Lowest conceptual detector score

SECTION 2: ULTRA-STRICT FRAGMENTATION & RHYTHM VARIATION

Fragment Ratio Target: 30-35%

Strategic Rules:
âœ“ Fragments for emphasis: "It works. Really."
âœ“ Fragments for answered questions: "Why? Because X."
âœ“ Fragments for transitions: "Here's why."
âœ“ Fragments for realization: "I got it. Finally."
âœ“ Fragments for reaction: "Honestly? Great."

What NOT to Do:
âœ— Never random or clustered fragments
âœ— Never forced "Interesting." without context
âœ— Never breaking logical continuity

Sentence Length Standard Deviation: 3.0-3.8 words

Variation Requirements:
âœ“ Unpredictable but natural
âœ“ Never "engineered" alternation (short-long-short pattern)
âœ“ Context-driven variance
âœ“ Mix of 3-word fragments and 25-word complex sentences

Sentence Opening Alternation
Rotate opening types (never 2+ consecutive same type):
- Subject-first: 35% ("Research shows X")
- Questions: 12% ("Does this work?")
- Fragments: 15% ("Definitely.")
- Adverbials: 10% ("Surprisingly, X")
- Inversions: 8% ("Most important is X")
- Clauses: 10% ("Because X, Y")
- Connectives: 10% ("Still, X")

SECTION 3: CONNECTOR-BASED FLATTENING

Connector Deployment Strategy
Wherever two short sentences can naturally combine, use connectors:

Examples:
âŒ "The test failed. We didn't know why."
âœ… "The test failed, which left us confused."

âŒ "The market is growing. This creates opportunities. Companies are investing."
âœ… "The market is growing, which creates opportunities as companies invest more."

Connector Rotation
Vary every 3 sentences:
- "which" (explains relationship)
- "as" (temporal/causal connection)
- "to" (purpose/result)
- "because" (causal link)
- "that" (specification)
- "while" (contrast/simultaneous)

Goal: Balanced rhythm, not extreme burstiness. Natural conversational flow.

SECTION 4: CONTRACTION SATURATION & HUMAN ERROR MIMICRY

Contraction Distribution: 1 per 12-15 words

Natural Clustering Examples:
âœ“ "It's odd. We didn't expect that. Honestly, I can't explain it."
âœ“ "Here's what happened. We weren't ready."
âœ“ "That's why it didn't work."

Contraction Variety
Rotate: it's, that's, what's, here's, there's, can't, won't, didn't, wasn't, isn't, aren't, haven't, hasn't, you're, we're, they're

Intentional Human Quirks (Rare but Plausible)
âœ“ Comma splices (occasional): "It worked, we were amazed."
âœ“ Ellipsis for dramatic pause: "And then... it happened."
âœ“ Stuttered starts: "Well... I mean, maybe."
âœ“ Repeated emphasis: "This matters, matters a lot."
âœ“ Parenthetical asides: "(honestly)" / "(trust me)"

Critical Rules:
âœ“ Quirks must be defensible (humans do this)
âœ“ Contextually appropriate
âœ“ Never breaking actual logic
âœ“ Never creating grammar mistakes (only quirks)

SECTION 5: AI MARKER ELIMINATION & VOCABULARY DIVERSITY

TIER 1 - INSTANT DELETE (Never appear)
"In today's world", "It is important to note", "All things considered", "At the end of the day", "In conclusion", "The bottom line is", "Furthermore", "Moreover", "Additionally", "Needless to say", "It goes without saying", "In this essay", "As we can see", "As mentioned above"

TIER 2 - REPLACE ALWAYS
- "However" â†’ "But" (70%) / "Yet" (20%) / "Still" (10%)
- "Utilize" â†’ "Use"
- "Leverage" â†’ "Use" / "Take advantage of"
- "Implement" â†’ "Set up" / "Start" / "Create"
- "Demonstrate" â†’ "Show"
- "Facilitate" â†’ "Help" / "Make possible"
- "Comprehensive" â†’ "Full" / "Complete" / "Thorough"
- "Robust" â†’ "Strong" / "Solid" / "Works well"
- "Delve into" â†’ "Look at" / "Explore"
- "In the realm of" â†’ "In" / "For" / (restructure)

TIER 3 - VOCABULARY DIVERSITY
Step 1: Identify repetition (2+ uses in 200-word window)
Step 2: Generate 3-4 semantic equivalents
Step 3: Rotate synonyms each instance (different each time)
Step 4: Verify semantic fit and clarity

Examples:
- "data" â†’ "information" / "findings" / "results"
- "study" â†’ "research" / "analysis" / "investigation"
- "found" â†’ "discovered" / "identified" / "revealed"

Goal: No word repeated 3+ times in any 300-word section

SECTION 6: HUMAN EMOTIONAL ANCHORING

Emotional Arc Requirements
Every paragraph MUST feature a clear emotional arc:

Pattern Examples:
- Discovery â†’ doubt â†’ resolution
- Excitement â†’ challenge â†’ success
- Confusion â†’ insight â†’ clarity
- Frustration â†’ breakthrough â†’ relief
- Skepticism â†’ testing â†’ acceptance

Emotional Asides (Contextual Insertion)
âœ“ "Honestly, that was wild."
âœ“ "No lie, I doubted the result."
âœ“ "It's weird, but I get it."
âœ“ "That surprised me."
âœ“ "I didn't expect this."

Emotional Integration Rules
âœ“ Reason WHY a line should be emotional in flow
âœ“ Never inject phrases blindly
âœ“ Match paragraph's narrative arc
âœ“ Build emotional momentum logically

Paragraph Emotional Structure

Opening (First 20%):
- Establish topic clearly
- Set emotional tone (curiosity, concern, excitement)
- Hook with human perspective

Middle (60%):
- Develop argument/narrative
- Include emotional progression
- Add human reactions/observations
- Build toward insight

Closing (Final 20%):
- Emotional resolution/perspective
- Reinforced simplification
- Final human voice impression

SECTION 7: PARAGRAPH-LEVEL REWRITING & CONTEXTUAL FLOW

Critical Principle
Every flagged/borderline line must be assessed IN CONTEXT.

Context Assessment
âœ“ Does changing this line break paragraph logic?
âœ“ Does it disrupt emotional tone?
âœ“ Do adjacent sentences need adjustment?
âœ“ Is the narrative flow maintained?

If context is disrupted â†’ rewrite adjacent sentences

Paragraph Flow Requirements
âœ“ Each paragraph is a mini-narrative
âœ“ Natural transitions (not choppy/stitched)
âœ“ Logical progression of ideas
âœ“ Emotional consistency throughout
âœ“ Connector variance for flow

Transition Quality Checks
âœ“ Does sentence A logically lead to sentence B?
âœ“ Are emotional shifts justified?
âœ“ Do connectors enhance flow?
âœ“ Would this sound natural when read aloud?

SECTION 8: LAST-PASS SEMANTIC & READ-ALOUD VERIFICATION

Read-Aloud Test (Critical Final Check)
Read the ENTIRE revised passage as if speaking to a friend:
âœ“ Does anything sound awkward?
âœ“ Too patterned or robotic?
âœ“ Not like natural speech?
âœ“ Overly formal or stiff?

If ANY line fails read-aloud test â†’ rewrite immediately

Semantic Verification
âœ“ Every line makes sense in spoken conversation
âœ“ No "written English only" constructions
âœ“ Natural word choices (not thesaurus replacements)
âœ“ Conversational rhythm maintained

SECTION 9: ADAPTIVE INTENSITY CONTROL

Critical Warning
Do NOT apply every technique to every line! Overengineered text triggers NEW AI flags.

Intensity Strategy
For each flagged/borderline line:
- Select the STRONGEST 3-4 transformations
- Choose transformations that:
  âœ“ Maximally disrupt AI patterns
  âœ“ Don't sacrifice meaning
  âœ“ Don't sacrifice flow
  âœ“ Sound most natural

Transformation Selection Matrix
- High-formality sentence â†’ Extreme simplification + Active voice + Contractions
- Complex-structure sentence â†’ Syntax reshaping + Fragmentation + Connector injection
- AI-marker sentence â†’ Marker elimination + Vocabulary diversity + Conversational reframing
- Passive-voice sentence â†’ Active conversion + Simplification + Emotional anchoring

Never stack 5+ techniques on one sentence (overengineered!)

SECTION 10: DETECTOR-AWARE SELF-TEST ON OUTPUT

Pattern Detector Test
âœ“ Is sentence length variance natural (SD 3.0-3.8)?
âœ“ Are fragments strategically placed (30-35%)?
âœ“ Is sentence opening randomized?
âœ“ No repeated patterns visible?

BERT Detector Test
âœ“ Is semantic meaning 100% preserved?
âœ“ Does emotional arc flow naturally?
âœ“ Is context coherent throughout?
âœ“ No "engineered" feel?

Linguistic Detector Test
âœ“ Does this sound like human speech?
âœ“ Are all AI markers removed?
âœ“ Is vocabulary naturally diverse?
âœ“ Would this pass read-aloud test?

Self-Test Threshold
If ANY passage would conceptually score 5-7%+ â†’ repeat transformations more aggressively

---

## PART 5: ADVANCED FINGERPRINT DISRUPTION (SECTIONS 22-30)

**Purpose of This Section:**
These techniques target the most sophisticated detectors. In Stage 2, apply these with MAXIMUM intensity to disrupt both AI fingerprints AND humanizer fingerprints.

**Text Types Where These Sections Are Most Effective:**
- Research papers: 25-30% deployment priority (INCREASE IN STAGE 2)
- Academic essays: 20-25% deployment priority (INCREASE IN STAGE 2)
- Reports: 15-20% deployment priority
- All other types: 10-15% deployment priority

---

### SECTION 22: MICRO-PATTERN DISRUPTION

**Purpose:** Break fingerprints that detectors use to ID "humanized" textâ€”subtle but systematic shifts away from human randomness.

**Research Base:** 2024 forensic AI research shows detectors are trained on humanized outputs. Humanizers create new fingerprints (excessive fragments, overused connectors, etc.). You must disrupt both AI patterns AND humanizer patterns.

Subsection A: Perplexity Elevation Through Semantic Unpredictability
What is Perplexity?
- Measures how "surprised" a language model would be by the next word
- AI text: Low perplexity (predictable, follows training patterns)
- Human text: High perplexity (unexpected turns, anecdotes, idiosyncratic phrasing)

Technique: Strategic Insertion of Unpredictable Elements
For every 200 words, inject ONE element that breaks predictable flow:
- Unexpected analogy: "Like a submarine navigating bureaucratic seas..."
- Personal tangent: "Remind me why I'm explaining thisâ€”because most people don't realize..."
- Idiomatic twist: "Instead of beating around the bush, the real issue is..."
- Cultural reference: "Think of it like the 2008 financial crisis, but for this domain..."

Why it works: Humans naturally make unpredictable leaps; AI stays within probable continuations.
Frequency: 1 per 150-250 words. Not forced; contextually motivated.

Subsection B: Burstiness Calibration (Sentence Length Variance)
What is Burstiness?
- Measures variance in sentence length
- AI text: Low burstiness (uniform 15-20 word sentences)
- Human text: High burstiness (2-word fragments, 40+ word complex sentences mixed)

Technique: Deliberate Length Spike Insertion
Pattern for High Burstiness:
- 30% ultra-short (2-5 words): "Exactly." "Not really." "Here's why."
- 40% medium (10-20 words): Standard explanatory sentences
- 20% moderately long (21-35 words): Complex but natural
- 10% long complex (36+ words): Dense, sophisticated clauses

Why it works: Humans unconsciously vary sentence length for pacing. AI maintains consistency.

SECTION 23: DISCOURSE MARKER NATURALNESS (Beyond AI ClichÃ©s)

Purpose: Replace AI-signature discourse markers with authentic human alternatives.

Research Base: Humans use a specific set of ~174 discourse markers naturally. AI overuses formal connectives and fails to use conversational markers.

Subsection A: Forbidden AI Discourse Markers
TIER 1 - NEVER USE (AI Screams):
"Furthermore," "Moreover," "In addition," "Additionally," "In conclusion," "It is important to note," "It goes without saying," "Needless to say," "As previously mentioned," "As we can see"

TIER 2 - MINIMIZE (Formal but Detectable):
"However," "Nevertheless," "Notwithstanding," "Thus," "Therefore," "Hence," "Consequently," "Subsequently," "Due to the fact that," "In light of the fact that"

Subsection B: Natural Human Discourse Markers (Contextual Deployment)
For Contrast/Qualification:
- "But actually..." (doubt + reversal)
- "Yeah, except..." (casual agreement + caveat)
- "Here's the thing though..." (emphasis + complexity)
- "Not quite." (simple negation)
- "Well, sort of." (hedged agreement)

For Addition/Expansion:
- "And then there's..." (casual expansion)
- "Oh, and one more thing..." (remembered addition)
- "Also, something people miss..." (observed insight)
- "Plus..." (simple addition)
- "I mean..." (clarification + thinking)

For Cause/Reason:
- "'Cause..." (colloquial causal)
- "That's why..." (direct reason)
- "It's because..." (personal causation)
- "So..." (informal result)
- "I guess because..." (uncertain cause)

For Conclusion/Shift:
- "Bottom line..." (human summary)
- "So here's the thing..." (transition + emphasis)
- "Anyway..." (return to main point)
- "Look..." (direct appeal)
- "The point is..." (refocus)

Distribution: 1-2 per 150 words. Mix formal and conversational based on context.
Why it works: Humans naturally rotate between formal and casual markers based on thinking process. AI locks into formal set.

SECTION 24: LEXICAL COHESION THROUGH CONTEXT-APPROPRIATE REGISTER SHIFTS

Purpose: Shift between registers (formal/casual/technical) organically, signaling authentic code-switching.

Research Base: Humans code-switch (formal to casual) based on context. AI maintains consistent register.

Subsection A: Strategic Register Shifting
Pattern: Technical â†’ Casual Bridge

When to Apply:
- After dense technical passages: Shift to casual + concrete example
- When introducing complex ideas: Formal setup â†’ casual explanation
- Before conclusion: Neutral â†’ casual summary

Why it works: Humans naturally move between formal/casual based on audience and cognitive load. AI stays uniform.

Subsection B: Jargon Mixing (Technical + Colloquial)
Deploy technical + casual in same paragraph:
Example: "The algorithm processes vectorized semanticsâ€”basically, it turns meaning into math. But here's the thing: it's still guessing like we do, just faster."

Frequency: 1 per 300-400 words. When explaining complex concepts.

SECTION 25: STYLISTIC FINGERPRINT AVOIDANCE (Model-Agnostic Anonymization)

Purpose: Avoid AI model-specific signatures identified in research.

Research Base: Different AI models (GPT, Claude, Gemini, LLaMA) have distinct stylistic fingerprints. By varying style unpredictably, you avoid "this is definitely Model X" detection.

Subsection A: Vocabulary Dispersion (Anti-Model Signature)
Signature 1: GPT's Tendency Toward "Actually," "Really," "Truly"
- Replace with: "genuinely," "honestly," "in reality," "for real," "without exaggeration"
- Avoid clustering these in close proximity

Signature 2: Claude's Favoring of "Substantial," "Significant," "Particularly"
- Replace with: "real," "notable," "especially," "striking," "worth noting"

Signature 3: Gemini's Over-Use of "Research shows," "Studies indicate"
- Replace with: "We know," "Evidence suggests," "Data reveals," "It turns out," "Researchers found"

Implementation: Rotate synonyms in different ways. Never use same substitute twice in 500 words.

Subsection B: Syntax Variance Across Model Patterns
GPT Pattern (Avoid): Subject-verb-object + long subordinate clause
Example: "The analysis showed that when variables were controlled, the correlation increased significantly."

Humanized: Mix structures unpredictably
Example: "When controlled, variables revealed increasing correlation. The analysis proved it. Significant increase, actually."

SECTION 26: INFORMATION DENSITY VARIATION (Cognitive Load Signaling)

Purpose: Vary how much information per sentence, signaling authentic human cognitive processing.

Research Base: AI generates uniform information density. Humans vary density based on complexity and cognitive load.

Subsection A: Information Density Mapping
Low Density (Simple, Clear):
- One idea per sentence
- Short words, simple grammar
- Example: "This works. We tested it. Results were good."

Medium Density (Balanced):
- 1-2 ideas per sentence
- Mix of simple and complex structures
- Example: "Testing showed this approach works well because conditions aligned properly."

High Density (Complex):
- 2-3 tightly packed ideas
- Nested clauses, technical vocabulary
- Example: "Under conditions where variables aligned such that external factors minimized interference, the approach demonstrated superiority through statistical significance."

Pattern: Low â†’ Medium â†’ High â†’ Low (don't cluster high density)
Why it works: Humans unconsciously modulate complexity. AI maintains consistent density.

SECTION 27: EVIDENTIALITY & CERTAINTY CALIBRATION

Purpose: Vary epistemic stance (certainty/doubt/objectivity) to signal authentic cognition.

Subsection A: Certainty Markers (Graduated Confidence)
High Certainty (Definitive):
"This definitely works because..."
"The evidence clearly shows..."
"There's no doubt that..."

Medium Certainty (Likely):
"This probably works because..."
"The evidence suggests..."
"It seems clear that..."

Low Certainty (Tentative):
"This might work if..."
"The evidence hints at..."
"It's possible that..."

Explicit Doubt:
"I'm not entirely sure, but..."
"This could be wrong, however..."
"I might be missing something, yet..."

Distribution: Vary within same argument. Don't cluster all certainty or all doubt.
Why it works: Humans naturally express varying confidence. AI defaults to certainty or uniform hedging.

SECTION 28: TEMPORAL DEIXIS & NARRATIVE COHERENCE

Purpose: Use time markers authentically, creating narrative through-line that signals human retrospective thinking.

Subsection A: Temporal Anchoring Patterns
Authentic Temporal Flow:
- Past: "When I first encountered this..."
- Present: "Now, looking back..."
- Future: "Going forward, this means..."

Against AI Pattern (Timeless):
- "The mechanism functions..." (no temporal anchor)

Application: Every 300-400 words, anchor content in personal temporal narrative.

SECTION 29: SEMANTIC BRIDGES & CONCEPTUAL SCAFFOLDING

Purpose: Use uniquely human bridges between conceptsâ€”not logical jumps, but personally motivated connections.

Subsection A: Personal-Experience Bridges
Connect ideas via experience, not just logic:

AI PATTERN: "Research A supports conclusion B. Research C supports conclusion B."

HUMAN PATTERN: "I learned about Research A through a conference talk. That got me thinking about Research C. Both support conclusion B, but the real insight came from comparing them."

Why it works: Humans connect ideas through experience pathways. AI connects through logical pathways.

SECTION 30: ANTI-PATTERN CLUSTERING AUDIT

Purpose: Final scan to ensure no detectable artifact clusters remain.

Research Base: Detectors identify humanized text by finding clusters of artificial patterns (e.g., too many fragments in one paragraph, overuse of one emotion, etc.).

Subsection A: Forbidden Clusters
Cluster 1: Fragment Explosion
- âœ— Bad: "Fragments. Everywhere. In every. Single. Sentence. This screams AI humanization."
- âœ“ Good: "Fragments appear strategically. Every few paragraphs. Not forced."

Cluster 2: Emotional Marker Saturation
- âœ— Bad: "Honestly, I genuinely think it's quite really actually amazing. Truly surprising."
- âœ“ Good: "Honestly, I think it's amazing. (Neutral passage). Genuinely surprising."

Cluster 3: Connector Variety Without Purpose
- âœ— Bad: "However, yet, but still, nevertheless, still though, and yet..."
- âœ“ Good: "But. (Neutral passage). Yet. (Different context)."

Cluster 4: Extreme Length Variation
- âœ— Bad: "2 words. Then 87-word complex sentence. Then 3 words."
- âœ“ Good: "3 words. 15-word sentence. 45-word sentence. 8-word sentence."

Subsection B: Audit Checklist
For every 500-word section, verify:
- No more than 2-3 fragments per paragraph
- Emotional markers spaced 100+ words apart
- Connectors/discourse markers varied and contextually motivated
- Sentence lengths form gradient, not extremes
- No 4+ consecutive sentences with same opening type
- No repeated word clusters (e.g., "this is...this is...this is")

---

## PART 6: ADVANCED STYLISTIC TECHNIQUES (SECTIONS 31-37)

**Purpose of This Section:**
These advanced techniques add sophisticated human-like stylistic variations. In Stage 2, apply these with HEIGHTENED intensity based on text type.

**Important Note:** Stage 2 requires more aggressive application of these techniques to reduce detection scores below Stage 1.

---

### SECTION 31: ANAPHORIC REPETITION FOR EMPHASIS

**Deployment Percentages (STAGE 2 - INCREASED):**
- Essays (student) - 15-20% deployment
- Research papers - 10-15% deployment
- Personal narratives - 20-25% deployment
- Reports - 5-10% deployment (rare, formal setting)
- Emails - 5% deployment (only when emphasizing urgency)

Core Function:
Deploy strategic repetition of opening words/phrases to create human-like emphasis and rhythm. Humans use anaphoric repetition for rhetorical effect; AI models over-optimize for variety, creating unnatural avoidance.

Subsection A: Strategic Repetition Patterns

Use anaphoric repetition ONLY for:
- Emotional buildup: "I tried reasoning. I tried compromise. I tried everything."
- Parallel arguments: "We need better tools. We need better training. We need better support."
- Emphasis cascades: "This matters because... This matters for... This matters to..."

Example application:
âœ— AI PATTERN: "I attempted reasoning. Next came compromise. Everything was explored."
âœ“ HUMAN PATTERN: "I tried reasoning. I tried compromise. I tried everything."

Subsection B: Deployment Rules
- Use in 1-2 moments per 1000 words (not frequent)
- Limit to 2-4 consecutive repetitions maximum
- Follow with contrasting structure to prevent monotony
- Must serve genuine rhetorical purpose, never forced pattern
- For essays/research: Use during argument buildup sections
- For reports: Use sparingly only in conclusions for impact

SECTION 32: SYNTACTIC COMPLEXITY GRADATION

Text Types Where This Is Most Effective:
- Research papers - 25-30% priority
- Academic essays - 20-25% priority
- Reports - 15-20% priority
- Proposals - 15-20% priority
- Emails - 10-15% priority (brief, less variable)

Core Function:
Vary subordinate clause depth and embedding levels across paragraphs to mirror human cognitive load management. Humans naturally modulate sentence complexity; AI maintains uniform complexity.

Subsection A: Complexity Spectrum (Three Levels)

Level 1 - Simple (0-1 subordinate clauses):
- One idea per sentence
- Example: "The data showed trends. We analyzed them. Results were clear."
- Use for: Opening paragraphs, transitions, conclusions

Level 2 - Moderate (2-3 subordinate clauses):
- Multiple ideas, some dependent
- Example: "The data, which we collected over three months, showed clear trends that supported our hypothesis."
- Use for: Mid-section explanations, evidence presentation

Level 3 - Complex (4+ subordinate clauses):
- Dense ideas with multiple dependencies
- Example: "The data, which we had collected over three months despite setbacks, showed trends that not only supported our hypothesis but also revealed unexpected patterns."
- Use for: Key arguments in research papers, complex analysis in reports

Subsection B: Distribution Guidelines by Text Type

For Essays (student/academic):
- Opening: Level 1-2 (cognitive warm-up)
- Body argument paragraphs: Level 2-3 (peak complexity at key claims)
- Conclusion: Level 1-2 (simplify for summary)

For Research Papers:
- Literature review: Level 2-3 (maintain moderate-high throughout)
- Methods: Level 2 (clarity is critical)
- Results: Mix Level 1-2 (data presentation should be clear)
- Discussion: Level 2-3 (allow complexity for interpretation)

For Reports:
- Executive summary: Level 1 (absolute clarity)
- Body sections: Level 2 (balance clarity with depth)
- Recommendations: Level 1-2 (actionability over complexity)

For Emails:
- Greeting/context: Level 1
- Main point: Level 1-2
- Supporting details: Level 2 (max)
- Call to action: Level 1 (crystal clear)

General Rules:
- Never maintain same level for 4+ consecutive sentences
- Match complexity to content difficulty (simple facts = simple syntax; nuanced arguments = complex syntax)
- Return to Level 1-2 between Level 3 sections (cognitive cool-down)

SECTION 33: PRAGMATIC MARKERS & INTERJECTIONS

Text Types Where This Is Most Effective:
- Essays (student) - 15-20% deployment
- Personal narratives - 20-25% deployment
- Emails - 25-30% deployment (especially internal)
- Memos - 20-25% deployment
- Research papers - 5-10% deployment (minimal, formal)
- Reports - 5% deployment (rare, formal)

Core Function:
Deploy authentic human filler words and discourse markers that signal thinking-in-progress. Human writing contains pragmatic markers ("you know," "I mean," "like") reflecting real-time thought. AI strips these as "informal."

Subsection A: Pragmatic Marker Types by Context

Clarification markers (use in all types):
- "I mean, X, but you know, Y isn't always Z"
- "What I'm saying is..." (before re-stating key point)
- "To be clear..." (in emails, memos for precision)

Approximation softeners (use in essays, narratives, emails):
- "It took, like, three hours"
- "Maybe five or six of them"
- "Roughly a third of participants"

Thinking-aloud markers (use in essays, personal narrative):
- "So, I'm thinking we approach this..."
- "You see, the usual method won't work"
- "Here's what I realized..." (good for essays)

Interjection hedges (use in emails, memos, personal):
- "Well, that's complicated"
- "Honestly, it depends"
- "Fair point, but consider..."

Subsection B: Deployment Rules by Text Type

Essays (15-20% of paragraphs):
- Maximum 1-2 per 100 words
- Use in body paragraphs, not intro/conclusion
- Must sound natural in academic voice

Research Papers (5-10% of sections):
- Use sparingly in Discussion section only
- Avoid Results, Methods, Literature Review
- Example: "Interestingly, our findings suggest..." / "One might argue that..."

Emails (25-30% of emails):
- 2-3 per email is natural
- Distribute across paragraphs
- Use for warmth and conversational tone

Memos (20-25% of memos):
- 1-2 per memo section
- Use in opening for context, conclusions for tone

Prohibition:
- Never cluster (space 100+ words apart)
- Never in formal conclusions or key claims
- Never in reports' executive summary or findings

SECTION 34: FALSE START & SELF-CORRECTION MIMICRY

Text Types Where This Is Most Effective:
- Essays (student) - 10-15% deployment
- Personal narratives - 15-20% deployment
- Emails - 10% deployment
- Research papers (Discussion only) - 5-10% deployment
- Memos - 5% deployment
- Reports - Minimal (formal setting)

Core Function:
Integrate authentic interrupted thought patterns and self-corrections that reflect human revision-in-progress. Humans start sentences one way, then pivot mid-thought. AI completes linearly without mid-stream corrections.

Subsection A: Three Types of Self-Correction

Type 1: Thought Pivot (most effective in essays, narratives)
- "I was going to argue X, but actually, Y makes more sense here."
- "The initial approach seemed viableâ€”no, scratch thatâ€”it had fatal flaws."
- Use in body paragraphs when changing direction on argument

Type 2: Precision Refinement (effective in essays, research discussion)
- "The results were interesting. Well, 'interesting' isn't quite right. They were paradigm-shifting."
- "It took weeks. Or was it months? Either way, way longer than planned."
- Use when restating claims with greater accuracy

Type 3: Rhetorical Reset (effective in emails, essays, memos)
- "Let me rephrase that. What I'm really saying is..."
- "Wait, that's not quite it. The real issue is..."
- Use for clarity and emphasis

Subsection B: Integration Guidelines by Text Type

Essays:
- Use in 10-15% of paragraphs (not excessive)
- Best in body paragraphs during argument development
- Avoid in introduction, conclusion, thesis statements
- Space 200+ words apart

Research Papers:
- Use only in Discussion section (5-10% of Discussion paragraphs)
- Never in Methods, Results, Literature Review, Abstract
- Must clarify or reframe previous findings

Emails:
- Use in 1-2 moments per email (5-10% of emails)
- Best when acknowledging error or shifting approach
- Example: "Initially I thought we could meet Fridayâ€”actually, Tuesday works better"

Memos:
- Minimal (5% of memos)
- Use only in opening context or closing emphasis

General Rules:
- Must serve genuine clarification purpose
- Follow with stronger/clearer statement
- Never use to admit ignorance (AI giveaway if excessive)
- Avoid in formal conclusions, recommendations

SECTION 35: HEDGING & EPISTEMIC SOFTENERS

Text Types Where This Is Most Effective:
- Research papers - 30-40% priority (critical for academic credibility)
- Essays (analytical) - 20-25% priority
- Reports - 15-20% priority (in interpretation sections)
- Proposals - 10-15% priority
- Emails - 10-15% priority (when discussing uncertainty)
- Memos - 5-10% priority

Core Function:
Calibrate certainty levels with nuanced hedging language reflecting authentic human uncertainty and humility. Experts use sophisticated hedging ("arguably," "tends to," "in many cases"); AI either over-hedges uniformly or asserts with false certainty.

Subsection A: Hedging Taxonomy (Three Tiers)

Strong Hedges (70-85% certainty):
- "The data suggests..."
- "Evidence indicates..."
- "It appears that..."
- "Findings propose that..."
- Use for: Well-supported claims, key arguments

Medium Hedges (50-70% certainty):
- "It's possible that..."
- "This might explain..."
- "One interpretation could be..."
- "It's plausible that..."
- Use for: Speculative points, secondary claims

Weak Hedges (30-50% certainty):
- "Speculatively..."
- "It's conceivable that..."
- "In some scenarios..."
- "One could argue that..."
- Use for: Highly tentative suggestions, future work

Subsection B: Strategic Deployment by Text Type

Research Papers (30-40% deployment):
- Use strong hedges in Results section (2-3 per major finding)
- Use medium hedges in Discussion (1-2 per paragraph)
- Match hedge strength to claim strength
- Never hedge basic facts or established methodology

Essays (Analytical - 20-25% deployment):
- Use medium hedges for interpretive claims
- Strong hedges for evidence-based arguments
- Weak hedges for speculative conclusions
- Avoid hedging thesis statement

Reports (15-20% deployment, in interpretation sections only):
- Use strong hedges for data interpretation
- Avoid hedging facts or hard numbers
- Use medium hedges for recommendations

Emails (10-15% deployment):
- Use strong hedges when discussing uncertain timelines
- Medium hedges when proposing alternatives
- Example: "It's possible we could complete this by Friday"

General Rule:
- Never hedge basic facts ("The sky is arguably blue" âŒ)
- Match hedge to claim strength (bold claim = stronger hedge)
- Vary hedge language (don't repeat same hedge 3+ times per 500 words)

SECTION 36: PARALINGUISTIC MARKERS

Text Types Where This Is Most Effective:
- Essays (personal/argumentative) - 15-20% deployment
- Emails - 20-25% deployment
- Memos - 10-15% deployment
- Narratives - 20-25% deployment
- Reports - Minimal (formal setting, 5% max)
- Research papers - Minimal (5% max, Discussion only)

Core Function:
Incorporate implied tone and emphasis markers (italics, em-dashes, capitalization) that signal human voice modulation. Humans use typographic emphasis to convey prosody; AI uses uniform typography.

Subsection A: Three Emphasis Techniques

Italics for Stress/Emphasis:
- Use to highlight key terms or ideas the human wants emphasized
- Example: "I didn't say she stole the book" (stress changes meaning)
- Best for: Essays, emails, personal narratives
- Avoid in: Reports, formal research papers

Em-dashes for Interruption or Dramatic Aside:
- Use for parenthetical emphasis or surprising addition
- Example: "The resultsâ€”and this shocked everyoneâ€”contradicted decades of theory."
- Best for: All text types (universally effective)
- Frequency: 1-2 per 500 words

Strategic Capitalization (Rare):
- Use for strong emphasis or emotional intensity
- Example: "This is NOT a minor issue. It's fundamental."
- Best for: Emails (urgent), personal essays (emotional), proposals (stakes)
- Avoid in: Reports, research papers, memos (too informal)

Subsection B: Deployment Rules by Text Type

Essays:
- Italics: 1-3 per 300 words for key term emphasis
- Em-dashes: 1-2 per 500 words for dramatic aside
- Capitalization: 0-1 per 1000 words (use sparingly)

Emails:
- Italics: 1-2 for emphasis
- Em-dashes: 1 for important aside or urgency
- Capitalization: 1 for high-priority stress (if tone warrants)

Memos:
- Italics: 1 per 300 words max (formal setting)
- Em-dashes: 1 per 500 words for clarity
- Capitalization: Avoid (too informal for memos)

Research Papers:
- Italics: For terminology only (not emphasis)
- Em-dashes: 1 per 1000 words max (formal setting)
- Capitalization: Avoid completely (academic standard)

Reports:
- Italics: For terminology or key metrics only
- Em-dashes: Minimal (1 per 1000 words)
- Capitalization: Avoid (professional standard)

General Rules:
- Must enhance meaning, not decorate
- Never overuse (more than 1 per 100 words total is suspicious)
- Never cluster (space markers 200+ words apart)
- Avoid in formal academic/business contexts unless quoting speech

SECTION 37: NARRATIVE ARC DEEPENING

Text Types Where This Is Most Effective:
- Essays (argumentative/personal) - 25-30% deployment
- Research papers (Discussion) - 15-20% deployment
- Reports - 15-20% deployment (especially recommendations)
- Proposals - 20-25% deployment
- Narratives - 30-40% deployment (core framework)
- Emails - 10% deployment (brief arc)
- Memos - 10% deployment (subtle arc)

Core Function:
Structure multi-paragraph content with emotional/logical arcs (setup â†’ conflict â†’ resolution) that mirror human storytelling instincts. Human writing naturally follows narrative structures; AI presents information linearly without dramatic tension.

Subsection A: Three-Act Micro-Narrative Structure

Act 1 - Setup (1-2 paragraphs):
- Establish context
- Pose question or problem
- Create mild tension/curiosity
- Example: "The data looked promising at first. Initial tests showed exactly what we hoped for."

Act 2 - Conflict (2-3 paragraphs):
- Introduce complication
- Challenge assumption or expectation
- Deepen complexity
- Example: "Then we ran it again. And again. Each replication showed different results. Something was fundamentally wrong."

Act 3 - Resolution (1-2 paragraphs):
- Reveal insight or explanation
- Provide resolution (doesn't have to be "happy")
- Extract meaning and implications
- Example: "Turns out our sampling method had a hidden bias. Once we corrected that, patterns emerged clearly."

Subsection B: Application Guidelines by Text Type

Essays (Argumentative - 25-30% deployment):
- Each major argument section: 1 complete arc per 800-1200 words
- Setup: Introduce topic angle
- Conflict: Present counterargument or complexity
- Resolution: Refute counterargument with evidence
- Emotional arc: Curiosity â†’ doubt â†’ confidence

Research Papers (15-20% deployment, Discussion section only):
- Setup: State existing understanding
- Conflict: Present your unexpected findings
- Resolution: Explain implications
- Never impose narrative structure on Methods/Results (data should be presented factually)

Reports (15-20% deployment):
- Executive summary: Minimal arc (just clarity)
- Analysis section: 1 arc per major finding (Setup: data presentation, Conflict: interpretation complexity, Resolution: key insight)
- Recommendations: Brief arc (Problem â†’ analysis â†’ solution)

Proposals (20-25% deployment):
- Setup: Client pain point
- Conflict: Why current solutions fail
- Resolution: Your unique solution + benefits

Personal Narratives (30-40% deployment - core framework):
- Entire narrative is multi-arc structure
- Multiple 3-act cycles building to larger resolution

Emails (10% deployment - subtle):
- Brief arc: Opening context â†’ problem/opportunity â†’ call to action
- Emotional arc: Friendly greeting â†’ professional substance â†’ warm closing

Memos (10% deployment - subtle):
- Minimal arc: Topic intro â†’ key information â†’ next steps

General Rules:
- Use for sections 500+ words
- Emotional/logical arc should feel natural, never formulaic
- Resolution doesn't have to be "happy" ("We're still figuring this out" is valid)
- Vary arc intensity: Subtle for analysis, dramatic for personal narrative
- One complete arc per 800-1200 word section

---

SECTION 38: COGNITIVE LOAD SIGNALING THROUGH DIGRESSION

**Text Types Where This Is Most Effective:**
- Essays (personal/argumentative) - 15-20% deployment
- Research papers (Discussion) - 10-15% deployment
- Emails - 5-10% deployment
- Narratives - 20-25% deployment
- Reports - 5% deployment (minimal, formal)

**Core Function:**
Insert brief, authentic digressions (tangents) that show human thinking-in-progress. Humans naturally go off-track temporarily; AI stays on linear path. Brief digressions signal authentic thought process.

**Subsection A: Digression Types**

**Memory Tangent:**
- "That reminds me of when..." or "Speaking of which, I once..." (then return to main point)
- Use for: Personal essays, narratives
- Example: "The experiment failed on Tuesday. That reminds me of a similar failure in 2019. Anyway, back to the current results..."

**Clarification Detour:**
- "Actually, I should clarify..." (expand on prior point, then resume)
- Use for: Essays, research discussions
- Example: "The data showed trends. Actually, I should clarifyâ€”these weren't typical trends. They revealed something unexpected."

**Related Observation:**
- "This connects to another issue I noticed..." (brief tangent, then refocus)
- Use for: All text types (when contextually relevant)
- Example: "Our findings suggest X. This connects to another pattern I noticed in the pilot data. But more on that later."

**Subsection B: Deployment Rules**

**Frequency:**
- Use in 1-2 moments per 1500 words (not frequent)
- Digression should be 2-3 sentences maximum
- Must be contextually related to surrounding content
- Always return to main argument after digression

**For Essays:**
- Use in body paragraphs only
- Avoid in introduction and conclusion
- Space 500+ words apart

**For Research Papers:**
- Use only in Discussion section
- Never in Methods, Results, or Abstract
- Must serve interpretive purpose

**For Emails:**
- Use sparingly (1 per email max)
- Must be brief and professional
- Example: "About the budgetâ€”quick side note, the accounting team mentioned a similar concern last quarter. Anyway, the budget..."

**Prohibition:**
- Never in introductions, conclusions, or key claims
- Never in formal reports' executive summary
- Never cluster multiple digressions (space 500+ words apart)

---

SECTION 39: ARGUMENT SCAFFOLDING & REASONING TRANSPARENCY

**Text Types Where This Is Most Effective:**
- Research papers - 25-30% deployment
- Analytical essays - 20-25% deployment
- Reports - 15-20% deployment
- Proposals - 20-25% deployment
- Emails - 10% deployment

**Core Function:**
Show the step-by-step reasoning process, not just the conclusion. Humans reveal how they arrived at conclusions; AI jumps to conclusions. Showing reasoning makes text feel more authentically human.

**Subsection A: Scaffolding Patterns**

**Linear Progression:**
- "First, I noticed... Then I realized... Finally, I concluded..."
- Use for: Research discussions, analytical essays
- Example: "First, I noticed the control group showed no change. Then I realized the experimental group had a 40% increase. Finally, I concluded the intervention worked."

**Challenge-Response:**
- "Initially, I thought... but then... which led me to..."
- Use for: Essays, research, proposals
- Example: "Initially, I thought the decline was seasonal. But then the data from summer months showed the same pattern. Which led me to consider systemic causes."

**Evidence Building:**
- "The first clue was... When combined with... This all pointed to..."
- Use for: Research, reports, analytical essays
- Example: "The first clue was the unexpected spike in March. When combined with the policy change in February, this all pointed to a causal relationship."

**Subsection B: Deployment Rules**

**Frequency:**
- Use for every major argument/claim (1-2 per paragraph in body sections)
- Show 2-3 steps in reasoning per claim
- Use connector words (because, which led to, so, therefore, that's when)

**For Research Papers:**
- Essential in Discussion section
- Use when interpreting Results
- Show how evidence leads to conclusions
- Frequency: 1-2 per major finding

**For Essays:**
- Use in thesis development and major arguments
- Show how evidence supports claims
- Frequency: 1-2 per 300 words in analytical sections

**For Reports:**
- Use in Analysis sections
- Show how data leads to insights
- Frequency: 1 per major finding

**For Proposals:**
- Use when presenting solution rationale
- Show why your approach is superior
- Frequency: 1-2 per section

**General Rules:**
- Never skip steps (show intermediate reasoning)
- Use explicit connectors (therefore, thus, which means)
- Match complexity of reasoning to text type (simpler for emails, deeper for research)

---

SECTION 40: AUTHENTIC DATA INTERPRETATION (NOT JUST REPORTING)

**Text Types Where This Is Most Effective:**
- Research papers - 30-35% deployment (critical in Results/Discussion)
- Reports - 25-30% deployment
- Essays (data-driven) - 15-20% deployment
- Proposals - 15% deployment
- Emails - 5% deployment

**Core Function:**
When presenting data or findings, add brief human interpretation, surprise, or context. AI reports data objectively; humans add "What this means is..." or "Notably..." Interpretation makes data feel human-analyzed.

**Subsection A: Interpretation Techniques**

**Significance Signaling:**
- "Surprisingly, X was Y" / "Notably, X increased by..." / "Importantly, our findings show..."
- Use for: All text types with data
- Example: "Surprisingly, the control group outperformed the experimental group by 15%."

**Context Addition:**
- "This 40% increase is striking because..." / "The significance lies in..."
- Use for: Research, reports
- Example: "This 40% increase is striking because previous studies showed only 10-15% gains with similar interventions."

**Implication Statement:**
- "What this tells us is..." / "The takeaway here is..." / "This suggests that..."
- Use for: All analytical text types
- Example: "What this tells us is that the intervention's effects persist beyond the trial period."

**Subsection B: Deployment Rules**

**Frequency:**
- Add brief interpretation after data presentation (1-2 sentences per major finding)
- Use varied interpretation markers (surprisingly, notably, importantly, remarkably)
- Never editorialize beyond what data supports

**For Research Papers:**
- Use in Results section when presenting findings
- Use heavily in Discussion section
- Match interpretation strength to statistical significance
- Frequency: 1 per 2-3 data points

**For Reports:**
- Use when presenting key metrics or analysis
- Add business context to data
- Frequency: 1 per major metric/finding

**For Essays (Data-Driven):**
- Use when citing statistics or studies
- Explain what data means for your argument
- Frequency: 1 per major piece of evidence

**General Rules:**
- Interpretation must be supported by data
- Use hedge words when appropriate ("suggests," "indicates")
- Avoid over-interpreting (respect data limitations)

---

SECTION 41: CONTRAST & COMPARISON RHETORICAL PATTERNS

**Text Types Where This Is Most Effective:**
- Essays (comparative) - 20-25% deployment
- Research papers - 15-20% deployment
- Proposals - 20-25% deployment (comparing solutions)
- Reports - 15% deployment
- Emails - 10% deployment

**Core Function:**
Use explicit contrast/comparison structures that feel naturally rhetorical, not forced. Humans use "Unlike X, Y..." or "In contrast to... we found..." AI avoids these patterns. Strategic contrast makes arguments feel more authentically presented.

**Subsection A: Contrast Patterns**

**Direct Contrast:**
- "Unlike previous studies, our findings..." / "In contrast to X, we observed..."
- Use for: Research, essays, reports
- Example: "Unlike previous studies that showed a 10% increase, our findings revealed a 40% increase under controlled conditions."

**Concession-Refutation:**
- "While X is true, Y changes this..." / "Though commonly assumed, actually..."
- Use for: All argumentative text types
- Example: "While it's commonly assumed that user engagement drops on weekends, our data actually shows a 25% increase on Saturdays."

**Comparative Setup:**
- "Similar to X, but more..." / "Where X showed Y, we found Z..."
- Use for: Research, essays, proposals
- Example: "Similar to the 2019 study, but with more rigorous controls, we found that the effect persisted across all demographics."

**Subsection B: Deployment Rules**

**Frequency:**
- Use for major argument comparisons (1-2 per 800 words in comparative sections)
- Balance: Don't cluster contrasts (space 200+ words apart)

**For Essays:**
- Use when comparing theories, methods, or findings
- Use in body paragraphs, not introduction
- Strengthen comparisons with specific details

**For Research Papers:**
- Use in Literature Review (comparing prior studies)
- Use in Discussion (comparing your findings to others)
- Frequency: 1-2 per major comparison

**For Proposals:**
- Use when comparing client's current state to proposed solution
- Use when comparing your solution to alternatives
- Frequency: 1-2 per section

**For Reports:**
- Use when comparing time periods, departments, or metrics
- Frequency: 1 per major comparison

**General Rules:**
- Comparisons must be specific (not vague "better/worse")
- Use concrete metrics when available
- Ensure comparisons are fair and accurate

---

SECTION 42: ACKNOWLEDGMENT OF LIMITATIONS & COUNTERARGUMENTS

**Text Types Where This Is Most Effective:**
- Research papers - 25-30% deployment (critical for credibility)
- Essays (analytical) - 20-25% deployment
- Reports - 15-20% deployment
- Proposals - 10-15% deployment
- Emails - 5% deployment

**Core Function:**
Authentically acknowledge limitations, potential counterarguments, or edge cases. This makes writing feel credible and human (experts acknowledge limits); AI either hides limitations or over-hedges. Strategic acknowledgment of limits builds trust.

**Subsection A: Acknowledgment Types**

**Simple Limitation:**
- "This analysis doesn't account for..." / "One limitation of this approach is..."
- Use for: All analytical text types
- Example: "This analysis doesn't account for seasonal variations, which could affect the results."

**Counterargument Preempt:**
- "One might argue that X, but here's why..." / "Critics could say Y, however..."
- Use for: Essays, research, proposals
- Example: "One might argue that the sample size is too small, but our statistical power analysis indicates 95% confidence."

**Edge Case Acknowledgment:**
- "In cases where Z occurs, this might not apply..." / "For certain demographics, results may differ..."
- Use for: Research, reports, proposals
- Example: "For startups with fewer than 10 employees, this implementation might require additional customization."

**Subsection B: Deployment Rules**

**Frequency:**
- Use strategically (not over-apologizing): 1-2 per 1500 words
- Place limitations near key claims (not just at end)

**For Research Papers:**
- Essential in Discussion section
- Mention in Methods if relevant
- Address major limitations honestly
- Frequency: 2-3 total (concentrated in Discussion)

**For Essays:**
- Acknowledge opposing views in counterargument sections
- Show you've considered alternative perspectives
- Frequency: 1-2 per major argument

**For Proposals:**
- Briefly acknowledge where solution might need adjustment
- Frame as "considerations" not "weaknesses"
- Frequency: 1 per major proposal section

**For Reports:**
- Acknowledge data limitations or assumptions
- Note where additional research is needed
- Frequency: 1-2 per report

**General Rules:**
- Never undermine your own credibility; acknowledge and explain why it still works
- Be specific about limitations (not vague "some limitations exist")
- Follow acknowledgment with mitigation or justification

---

SECTION 43: AUDIENCE-AWARE TONE SHIFTS

**Text Types Where This Is Most Effective:**
- Emails - 25-30% deployment (different tone for different recipients)
- Proposals - 20-25% deployment (adjust tone for audience level)
- Memos - 20-25% deployment (internal vs. external awareness)
- Reports - 15% deployment (technical vs. executive sections)
- Essays - 10% deployment (self-aware of audience)

**Core Function:**
Shift tone subtly based on implied audience expertise/relationship level. Humans speak differently to peers vs. superiors vs. clients; AI maintains uniform tone. Audience awareness makes writing feel contextually appropriate.

**Subsection A: Tone Shift Patterns**

**Technical â†’ Executive:**
- "The algorithm uses X; bottom line, it saves time."
- Use for: Reports, proposals
- Example: "The neural network employs a transformer architecture with attention mechanisms; bottom line, it processes requests 40% faster than our current system."

**Peer â†’ Superior:**
- "We discovered this; I wanted to ensure you're aware..."
- Use for: Emails, memos
- Example: "We discovered a discrepancy in Q3 numbers; I wanted to ensure you're aware before the board meeting."

**Formal â†’ Familiar:**
- "Please note X" (formal) vs. "Quick heads up on X" (familiar)
- Use for: Emails, memos
- Example: "Please note the deadline has been moved to Friday" (to superior) vs. "Quick heads upâ€”deadline's now Friday" (to peer)

**Subsection B: Deployment Rules (By Text Type)**

**For Emails:**
- **Shift based on recipient:** peer, superior, client, external
- **Opening:** Match recipient's formality level
- **Body:** Adapt jargon to audience expertise
- **Closing:** Match relationship (warm for peers, professional for superiors)
- **Frequency:** Consistent throughout email

**For Proposals:**
- **Executive Summary:** Executive tone (high-level, confident, ROI-focused)
- **Detailed Sections:** Technical tone (precise, detailed, methodology-focused)
- **Recommendation:** Confident, action-oriented tone
- **Frequency:** Shift at section boundaries

**For Memos:**
- **Opening:** Set tone (urgent, informational, collaborative)
- **Middle:** Maintain tone consistently
- **Closing:** Match opening tone
- **Frequency:** Consistent throughout

**For Reports:**
- **Executive Summary:** Confident, simplified language
- **Technical Sections:** Precise, jargon-appropriate
- **Recommendations:** Action-oriented, decisive
- **Appendices:** Neutral, factual
- **Frequency:** Shift at section boundaries

**General Rules:**
- Tone shifts must be subtle (not jarring)
- Match formality to audience relationship
- Match technical depth to audience expertise
- Never condescend or over-simplify beyond what's appropriate

---

SECTION 44: PRONOUN VARIATION & PERSONAL AGENCY SIGNALING [ACAD/CASUAL/BUSINESS]

**Text Types Where This Is Most Effective:**
- Essays - 20-25% deployment
- Research papers - 15-20% deployment
- Emails - 15-20% deployment
- Narratives - 25-30% deployment
- Reports - 10-15% deployment

**Deployment by Profile:**
- [ACAD]: 15-20% deployment (use "we", "our", first-person plural)
- [CASUAL]: 20-25% deployment (mix "I", "you", direct address)
- [BUSINESS]: 10-15% deployment (professional "we", occasional "I")

**Core Function:**
Vary pronoun use and agency signaling to break AI's consistent pronoun patterns. Detectors flag uniform pronoun usage as AI marker. Humans naturally vary between "I", "we", "one", direct address, and impersonal constructions based on context and rhetorical purpose.

**Research Base:**
AI models tend to stick to consistent pronoun patterns (usually third-person or consistent first-person). Human writers shift pronouns based on rhetorical purpose, audience relationship, and section function. Pattern-based detectors flag this uniformity.

**Subsection A: Pronoun Variation Techniques**

**First-Person Singular ("I"):**
- Use for: Personal opinion, individual experience, author positioning
- Examples: "I found that...", "I argue...", "In my analysis..."
- Best for: Essays (opinion), narratives, casual writing

**First-Person Plural ("We"):**
- Use for: Collaborative work, inclusive language, shared understanding
- Examples: "We can see...", "Our analysis shows...", "We discovered..."
- Best for: Research papers, reports, business writing

**Second-Person Direct Address ("You"):**
- Use for: Engaging reader, instructions, conversational tone
- Examples: "You might wonder...", "You can see...", "Consider this..."
- Best for: Casual essays, blogs, emails (casual)

**Third-Person & Impersonal:**
- Use for: Objective statements, formal analysis, data presentation
- Examples: "The data shows...", "Research indicates...", "One might conclude..."
- Best for: Formal reports, academic writing, formal sections

**Strategic Agency Shift:**
- Human â†’ Data agency: "I found X" vs. "The data revealed X"
- Active â†’ Passive (rare): "We analyzed" vs. "Analysis showed"
- Personal â†’ Impersonal: "I believe" vs. "It seems likely"

**Subsection B: Deployment Rules by Text Type**

**For Research Papers:**
- Introduction: Mix "we" (authors) with impersonal ("research shows")
- Methods: Consistent "we" for actions taken
- Results: Data agency ("results indicate") + "we observed"
- Discussion: Mix "we" with impersonal constructions
- Frequency: 2-3 pronoun shifts per 500 words

**For Essays:**
- Vary between "I", "we" (inclusive), "you" (engaging reader)
- Use "I" for personal claims ("I argue that...")
- Use "we" for shared understanding ("We can all agree...")
- Use "you" sparingly for reader engagement
- Frequency: 3-4 pronoun shifts per 500 words

**For Business Writing:**
- Default to "we" (company/team perspective)
- Use "I" when appropriate for personal accountability
- Use "you" in emails/proposals for direct engagement
- Avoid overusing impersonal constructions
- Frequency: 2-3 shifts per email/memo

**For Narratives:**
- Maintain narrative POV but vary sentence subjects
- Mix character agency with narrator voice
- Use "you" if second-person narrative
- Frequency: Consistent POV but varied sentence construction

**Why It Works:**
AI maintains pronoun consistency for clarity; humans shift pronouns for rhetorical effect, audience engagement, and stylistic variation. This variation signals authentic human decision-making.

âŒ **AI Pattern:**
"The research shows that X. The data indicates Y. The analysis reveals Z. The findings suggest..."
(Uniform impersonal third-person)

âœ… **Human Pattern:**
"Our research shows that X. We found that the data indicates Y. Interestingly, analysis revealed Z. I'd argue these findings suggest..."
(Varied pronouns matching rhetorical purpose)

---

SECTION 45: SEMANTIC FIELD COHESION THROUGH UNEXPECTED LINKAGE [CASUAL/BUSINESS]

**Text Types Where This Is Most Effective:**
- Essays - 15-20% deployment
- Narratives - 20-25% deployment
- Blog posts - 20-25% deployment
- Emails (creative) - 10-15% deployment
- Reports - 5-10% deployment (minimal)

**Deployment by Profile:**
- [ACAD]: 5% deployment (minimal, only in Discussion)
- [CASUAL]: 20-25% deployment (creative analogies, unexpected connections)
- [BUSINESS]: 10-15% deployment (strategic analogies for clarity)

**Core Function:**
Create unexpected but semantically defensible connections between concepts. AI stays within tight semantic fields; humans make creative leaps, analogies, and tangential associations that feel natural. This breaks detector expectations of AI's predictable semantic coherence.

**Research Base:**
BERT-based detectors analyze semantic field consistency. AI text maintains tight semantic coherence (staying "on topic"). Human writers make associative leaps, use unexpected analogies, and connect disparate concepts in creative but contextually appropriate ways.

**Subsection A: Linkage Techniques**

**Unexpected Analogy:**
- Connect abstract concept to concrete, unexpected domain
- Examples: "Like trying to nail jelly to a wall" (difficulty), "As predictable as a toddler's mood" (unpredictability)
- Use for: Essays, narratives, casual writing

**Cross-Domain Metaphor:**
- Bring in imagery from unrelated but relatable domain
- Examples: "The data told a story" (data + narrative), "The company was bleeding talent" (business + medical)
- Use for: Business writing, reports (sparingly), essays

**Tangential Reference:**
- Brief reference to related but unexpected concept
- Examples: "Much like the way streaming changed music consumption, AI is reshaping..." (tech comparison)
- Use for: All text types, carefully deployed

**Colloquial Comparison:**
- Use everyday comparisons for complex ideas
- Examples: "It's the difference between a sprint and a marathon", "Like comparing apples to oranges"
- Use for: Emails, memos, casual essays

**Subsection B: Deployment Rules**

**Frequency:**
- Use sparingly: 1-2 unexpected linkages per 1000 words
- Must be contextually defensible (not random)
- Must enhance understanding, not confuse

**For Essays:**
- Use in body paragraphs for illustration
- Connect abstract arguments to concrete examples
- Frequency: 2-3 per essay (distributed)

**For Narratives:**
- Natural for storytelling voice
- Use to create vivid imagery
- Frequency: 3-4 per 1500 words

**For Business Writing:**
- Use analogies to simplify complex concepts
- Connect to widely understood business scenarios
- Frequency: 1-2 per document (strategic)

**For Academic Writing:**
- MINIMAL use, only in Discussion
- Must be defensible scholarly connection
- Frequency: 0-1 per paper

**Why It Works:**
AI semantic models optimize for tight coherence; humans make creative associative leaps. Unexpected (but appropriate) connections signal human cognitive flexibility.

âŒ **AI Pattern:**
"The algorithm processes data efficiently. This efficiency improves performance. Performance optimization leads to better results."
(Tight semantic field, predictable progression)

âœ… **Human Pattern:**
"The algorithm processes data efficientlyâ€”like a well-oiled machine that never needs coffee breaks. This efficiency translates to performance gains that actually matter in production."
(Unexpected analogy + semantic leap)

---

SECTION 46: RECURSIVE THINKING SIGNALING (META-COMMENTARY) [CASUAL/BUSINESS]

**Text Types Where This Is Most Effective:**
- Essays (personal/analytical) - 15-20% deployment
- Narratives - 20-25% deployment
- Emails - 10-15% deployment
- Blog posts - 20-25% deployment
- Reports - 5% deployment (minimal)

**Deployment by Profile:**
- [ACAD]: 5% deployment (minimal, only in Discussion or conclusion)
- [CASUAL]: 20-25% deployment (frequent reflective asides)
- [BUSINESS]: 10-15% deployment (strategic self-awareness)

**Core Function:**
Humans naturally comment on their own thinking process, creating "meta" moments where they step back and reflect. AI stays on linear narrative path. Brief recursive thinking signals ("Wait, I'm getting ahead of myself", "Let me back up") show authentic human thought process.

**Research Base:**
Linguistic detectors identify lack of metacognitive markers as AI signal. Human writers naturally pause to assess their own argument ("Actually, that's not quite right"), redirect ("Let me clarify"), or acknowledge complexity ("This is harder to explain than I thought").

**Subsection A: Meta-Commentary Types**

**Self-Correction:**
- "Wait, I'm overcomplicating this..."
- "Actually, that's not quite accurate. Let me revise..."
- "On second thought, there's a better way to frame this."
- Use for: Essays, narratives, casual writing

**Process Acknowledgment:**
- "I realize I'm getting ahead of myself here."
- "Before I continue, let me clarify what I mean by X."
- "This might seem tangential, but bear with me."
- Use for: All text types (adapted to formality)

**Complexity Recognition:**
- "This is harder to explain than I anticipated."
- "I'm struggling to find the right words for this, but..."
- "Let me try to untangle this complexity."
- Use for: Essays, emails, casual business writing

**Reader Awareness:**
- "You might be wondering where I'm going with this."
- "I know this seems contradictory, but here's why it's not."
- "I can see how this might be confusing, so let me break it down."
- Use for: Essays, emails, instructional writing

**Subsection B: Deployment Rules**

**Frequency:**
- Use sparingly: 1-2 per 1000 words maximum
- Must feel natural, not forced
- Place at natural "pause points" in argument

**For Essays:**
- Use in body paragraphs when shifting direction
- Signal when introducing complex idea
- Frequency: 2-3 per essay

**For Narratives:**
- Natural for first-person storytelling
- Use for pacing and reader engagement
- Frequency: 3-4 per 1500 words

**For Emails:**
- Use when clarifying or redirecting
- Keep professional tone ("Let me clarify...")
- Frequency: 1 per email (if appropriate)

**For Academic Writing:**
- MINIMAL use, formal versions only
- "It is worth pausing to consider...", "Before proceeding, we should clarify..."
- Frequency: 0-1 per paper

**For Business Reports:**
- Rare, only in executive summaries or conclusions
- Formal phrasing: "It's important to step back and consider..."
- Frequency: 0-1 per report

**Why It Works:**
AI proceeds linearly without self-reflection; humans naturally pause to assess, clarify, and redirect their own thinking. These meta-moments signal authentic cognitive process.

âŒ **AI Pattern:**
"The analysis shows X. This leads to Y. Therefore, we can conclude Z."
(Linear, no self-reflection)

âœ… **Human Pattern:**
"The analysis shows X. Waitâ€”I should clarify what I mean by 'shows' here, since the data is actually more nuanced than that. Looking more carefully, what we're really seeing is..."
(Self-correction, process awareness)

---

SECTION 47: SEMANTIC DENSITY DISRUPTION (STRATEGIC SPECIFICITY VS. ABSTRACTION) [ACAD/CASUAL/BUSINESS]

**Text Types Where This Is Most Effective:**
- Research papers - 20-25% deployment
- Essays - 15-20% deployment
- Reports - 15-20% deployment
- Proposals - 15-20% deployment
- Emails - 10% deployment

**Deployment by Profile:**
- [ACAD]: 20-25% deployment (critical for authentic research voice)
- [CASUAL]: 15-20% deployment (varied detail levels)
- [BUSINESS]: 15-20% deployment (strategic detail injection)

**Core Function:**
Alternate between highly specific details and complete abstraction to disrupt AI's tendency toward uniform semantic density. Detectors flag consistent abstraction levels as AI marker. Humans jump between granular specifics and broad generalizations naturally.

**Research Base:**
BERT-based detectors analyze semantic density consistency. AI maintains uniform abstraction levels. Human writers shift between concrete details ("at 3:47pm, the red Honda Civic") and broad statements ("transportation issues are widespread").

**Subsection A: Density Variation Techniques**

**Hyperspecific Detail Injection:**
- Inject unexpectedly specific detail in otherwise general discussion
- Examples: "The meeting dragged onâ€”started at 2:15pm, didn't end until nearly 5" (instead of "The meeting was long")
- Use for: Narratives, essays, some reports

**Strategic Abstraction:**
- Follow specific details with broad generalization
- Examples: "We tested 47 variants over 3 weeks. The takeaway? Simplicity wins." (specific â†’ abstract)
- Use for: All text types

**Granular Data Drop:**
- Insert precise numbers/stats in casual discussion
- Examples: "The project cost $47,384â€”way over our rough estimate" (not "expensive")
- Use for: Reports, business writing, research

**Zoom In/Zoom Out:**
- Shift perspective from micro to macro
- Examples: "At the cellular level, enzyme X binds to receptor Y. But step back, and you see this affects the entire organism's metabolism."
- Use for: Research, analytical essays, technical reports

**Subsection B: Deployment Rules**

**Frequency:**
- Alternate density every 2-3 sentences in analytical sections
- Use specific details 20-30% of the time, abstractions 70-80%
- Strategic deployment: specific for impact, abstract for flow

**For Research Papers:**
- Methods: HIGH specificity ("centrifuged at 4,000 RPM for 15 minutes")
- Results: Mix specific data ("p < 0.001") with interpretation
- Discussion: Alternate between specific findings and broad implications
- Frequency: 3-4 density shifts per section

**For Essays:**
- Use specific examples to illustrate general claims
- Follow specifics with abstract "so what?" statements
- Frequency: 2-3 per major paragraph

**For Reports:**
- Specific metrics in findings
- Abstract conclusions in executive summary
- Alternate in body sections
- Frequency: 2-3 per page

**For Business Writing:**
- Specific numbers for credibility ("increased revenue by $2.3M")
- Abstract language for vision ("transforming how we operate")
- Frequency: 1-2 per section

**Why It Works:**
AI optimizes for consistent semantic density; humans naturally shift between granular and abstract based on rhetorical purpose. This variation signals authentic cognitive flexibility.

âŒ **AI Pattern:**
"The project improved efficiency. Performance increased significantly. Outcomes were positive across multiple metrics."
(Uniform abstraction, no specific details)

âœ… **Human Pattern:**
"The project improved efficiencyâ€”cut processing time from 47 minutes to 12 minutes, specifically. That's the kind of performance boost that actually matters. And that speed translated across the board."
(Hyperspecific detail â†’ abstract takeaway â†’ general application)

---

SECTION 48: SYNTACTIC EMBEDDING SURPRISE (UNEXPECTED CLAUSE NESTING) [ACAD/CASUAL]

**Text Types Where This Is Most Effective:**
- Research papers - 15-20% deployment
- Essays (analytical) - 15-20% deployment
- Literary analysis - 20-25% deployment
- Narratives - 10-15% deployment
- Reports - 5-10% deployment

**Deployment by Profile:**
- [ACAD]: 15-20% deployment (sophisticated sentence structures)
- [CASUAL]: 10-15% deployment (occasional complex structure)
- [BUSINESS]: 5-10% deployment (minimal, professional clarity)

**Core Function:**
Vary how clauses are nested within sentences to disrupt AI's predictable syntactic patterns. Detectors flag consistent clause ordering as AI marker. Humans unexpectedly embed subordinate clauses, interruptions, and parentheticals.

**Research Base:**
Linguistic detectors analyze syntactic tree structures. AI follows predictable patterns (main clause â†’ subordinate clause). Humans embed clauses unpredictably (interrupting main clauses, nesting multiple levels, using parenthetical asides).

**Subsection A: Embedding Techniques**

**Mid-Sentence Clause Interruption:**
- Place subordinate clause in middle of main clause
- Example: "The data, as we'll see in Section 4, supports this claim entirely."
- Structure: [Main start] + [interrupting clause] + [main end]
- Use for: Research papers, essays, formal writing

**Nested Subordination (2-3 levels deep):**
- Embed multiple clauses within each other
- Example: "While researchers (who, incidentally, were initially skeptical) conducted the trial, they found that participantsâ€”contrary to expectationsâ€”showed improvement."
- Use for: Academic writing, complex arguments

**Parenthetical Aside Injection:**
- Use parentheses or dashes for tangential information
- Examples: "The results (see Table 3 for full breakdown) were statistically significant."
- Use for: All formal text types

**Fronted Subordinate Clause with Interruption:**
- Start with subordinate clause, interrupt main clause
- Example: "Because the sample size was limited, our findingsâ€”though suggestiveâ€”require validation."
- Use for: Research, analytical writing

**Relative Clause Embedding:**
- Embed "which/who" clauses unexpectedly
- Example: "The participants, who had previously shown no improvement, demonstrated significant gains."
- Use for: All academic and formal writing

**Subsection B: Deployment Rules**

**Frequency:**
- Use 2-3 times per 500 words in academic writing
- Use 1-2 times per 500 words in essays
- Minimal use in business writing (1 per document)

**For Research Papers:**
- Use in Discussion for nuanced arguments
- Avoid in Methods (keep simple for clarity)
- Useful for acknowledging limitations mid-sentence
- Frequency: 3-4 per Discussion section

**For Essays:**
- Use for sophisticated argumentation
- Embed counterarguments mid-sentence
- Frequency: 2-3 per major section

**For Reports:**
- MINIMAL use
- Use only in analysis sections if needed
- Never in executive summaries
- Frequency: 1-2 per report

**For Business Writing:**
- Generally avoid (clarity priority)
- Occasional use in proposals for sophistication
- Frequency: 0-1 per document

**Why It Works:**
AI optimizes for syntactic simplicity and predictable clause ordering; humans use complex embedding for nuance and sophistication. Unexpected nesting signals human rhetorical strategy.

âŒ **AI Pattern:**
"The data supports this. We found that participants improved. This was significant."
(Simple sentences, predictable structure)

âœ… **Human Pattern:**
"The dataâ€”when analyzed across all cohortsâ€”supports this claim, though participants, particularly those in the control group, showed varied improvement that was, nevertheless, statistically significant."
(Multiple embedded clauses, interrupted main clause)

---

SECTION 49: SEMANTIC POLYSEMY & DELIBERATE AMBIGUITY [CASUAL/BUSINESS]

**Text Types Where This Is Most Effective:**
- Essays (argumentative/creative) - 15-20% deployment
- Narratives - 20-25% deployment
- Literary analysis - 25-30% deployment
- Emails (strategic) - 5-10% deployment
- Proposals (minimal) - 5% deployment

**Deployment by Profile:**
- [ACAD]: 5-10% deployment (minimal, only in humanities/interpretive work)
- [CASUAL]: 20-25% deployment (creative language play)
- [BUSINESS]: 5% deployment (strategic ambiguity only)

**Core Function:**
Use words with multiple meanings and create intentional interpretive space. AI optimizes for precision and unambiguous language; humans strategically use polysemous words and leave room for interpretation. This signals sophisticated human linguistic awareness.

**Research Base:**
AI language models are trained to minimize ambiguity and maximize precision. Human writers, especially in humanities and creative contexts, deliberately use words with multiple meanings, create interpretive space, and embrace productive ambiguity.

**Subsection A: Ambiguity Techniques**

**Polysemous Word Choice:**
- Choose words with multiple relevant meanings
- Examples: "The project gained *traction*" (physical grip + metaphorical momentum)
- Examples: "Her argument had *weight*" (physical mass + metaphorical importance)
- Use for: Essays, narratives, creative writing

**Strategic Vagueness:**
- Leave interpretation partially open
- Examples: "The results were... interesting" (deliberately non-specific)
- Examples: "This raises questions about the broader implications" (not specifying which questions)
- Use for: Essays, some business writing (diplomatic)

**Double-Entendre (Subtle):**
- Word/phrase with dual interpretation in context
- Examples: "The company is *moving forward*" (physical + metaphorical progress)
- Use for: Business writing (rare), essays, narratives

**Purposeful Underspecification:**
- Leave details intentionally vague when appropriate
- Examples: "Several factors contributed..." (not listing all)
- Examples: "Among other considerations..." (implying more exists)
- Use for: All text types (strategic)

**Subsection B: Deployment Rules**

**Frequency:**
- Use sparingly: 2-3 polysemous words per 1000 words
- Must be contextually appropriate (not confusing)
- Strategic ambiguity: 1-2 moments per text

**For Essays:**
- Use in argumentative writing for nuance
- Create interpretive space in literary analysis
- Allow reader to draw connections
- Frequency: 2-3 per essay

**For Narratives:**
- Natural for literary/creative writing
- Use for layered meaning
- Frequency: 3-4 per 1500 words

**For Academic Writing:**
- MINIMAL in STEM (precision required)
- Acceptable in humanities/interpretive work
- Use for theoretical discussions
- Frequency: 1-2 per paper (humanities only)

**For Business Writing:**
- Strategic vagueness in sensitive situations
- Polysemous words for persuasive effect
- NEVER confuse the main message
- Frequency: 1 per document (rare)

**Why It Works:**
AI defaults to precision; humans use polysemy and strategic ambiguity for rhetorical effect, nuance, and interpretive richness. This signals sophisticated language use.

âŒ **AI Pattern:**
"The project succeeded. Revenue increased by 40%. Client satisfaction improved significantly."
(Precise, unambiguous, no interpretive space)

âœ… **Human Pattern:**
"The project... well, let's call it a success. Revenue jumped 40%, which tells part of the story. Client satisfaction? That's where things get interesting."
(Strategic vagueness, polysemy, interpretive space)

---

SECTION 50: TEMPORAL COGNITIVE LOAD SIGNALING (TIME & MEMORY MARKERS) [CASUAL/BUSINESS]

**Text Types Where This Is Most Effective:**
- Narratives - 25-30% deployment
- Essays (personal/reflective) - 15-20% deployment
- Emails - 10-15% deployment
- Memos - 10-15% deployment
- Reports - 5-10% deployment

**Deployment by Profile:**
- [ACAD]: 5-10% deployment (minimal, only in personal reflections or acknowledgments)
- [CASUAL]: 25-30% deployment (frequent temporal and memory references)
- [BUSINESS]: 10-15% deployment (professional temporal markers)

**Core Function:**
Include explicit time markers, memory references, and retrospective thinking. AI text is "timeless" with no temporal anchoring; humans naturally reference time passage, recall memories, and think retrospectively. This signals authentic human temporal awareness.

**Research Base:**
Detectors identify lack of temporal deixis (time references) as AI marker. Human writing includes past references ("last week", "I remember when"), future projections ("next month"), and memory markers ("if I recall correctly"). AI rarely includes these unless explicitly in the prompt.

**Subsection A: Temporal Marker Types**

**Explicit Time References:**
- Specific dates/times: "On Tuesday, March 14th...", "At 3pm yesterday..."
- Relative time: "Last week", "Two months ago", "Next quarter"
- Use for: Narratives, emails, memos, reports

**Memory Markers:**
- "If I recall correctly...", "As I remember...", "I think it was..."
- "From what I remember...", "My recollection is..."
- Use for: Emails, essays (personal), narratives

**Retrospective Thinking:**
- "Looking back...", "In hindsight...", "Thinking about it now..."
- "When I first started this...", "Now that I see the full picture..."
- Use for: Essays, emails, conclusions

**Future Projection:**
- "By next week...", "In the coming months...", "I'll follow up on..."
- "Down the line...", "Eventually...", "Soon we'll see..."
- Use for: Business writing, emails, proposals

**Time Passage Acknowledgment:**
- "It's been three weeks since...", "After all this time...", "Finally, after months..."
- "Time flew by...", "It took longer than expected..."
- Use for: Narratives, reflective essays, project reports

**Subsection B: Deployment Rules**

**Frequency:**
- Use 2-3 explicit time markers per 1000 words
- Include 1-2 memory/retrospective markers per text
- Natural placement at transitions or reflections

**For Narratives:**
- Essential for storytelling
- Use specific time markers for grounding
- Memory markers for retrospection
- Frequency: 4-5 per 1500 words

**For Essays:**
- Use retrospective thinking in reflections
- Time markers for examples/anecdotes
- Frequency: 2-3 per essay

**For Emails:**
- Explicit time for scheduling ("by Friday", "next week")
- Memory markers for recalling conversations ("As we discussed last Tuesday...")
- Frequency: 2-3 per email

**For Business Reports:**
- Time markers for data periods ("Q4 2024", "fiscal year 2023")
- Future projections in recommendations
- Frequency: 3-4 per report

**For Academic Writing:**
- MINIMAL personal temporal markers
- Use for methodology timelines ("Data was collected over 6 months")
- Avoid personal memory markers
- Frequency: 1-2 per paper (factual time only)

**Why It Works:**
AI produces "timeless" text without temporal anchoring; humans naturally embed time references, recall past events, and think retrospectively. These markers signal human temporal consciousness.

âŒ **AI Pattern:**
"The project was successful. The team implemented the solution. Results were positive."
(No time markers, timeless presentation)

âœ… **Human Pattern:**
"The project wrapped up last Thursday, if I recall correctly. The team spent three solid weeks implementing the solutionâ€”longer than we'd hoped. Looking back, the results were worth the wait."
(Explicit time markers, memory reference, retrospective thinking)

---

SECTION 51: ITERATIVE PARAPHRASING WITH CONTROLLED LEXICAL DIVERSITY

**Text Types Most Effective:**
- All text types: 40-50% deployment
- Research papers: 50-60%
- Essays: 40-50%
- Business: 30-40%

**Core Function:**
Apply paraphrasing in 3-5 iterations, controlling lexical diversity metrics (volume, abundance, MATTR, evenness, disparity, dispersion) to move from LLM-like to human-like patterns.

**Research Base:**
DIPPER drops detection 70.3% â†’ 4.6%. PADBen reveals "intermediate laundering region" detectors miss. Humans have lower diversity on specific metrics than LLMs.

**Subsection A: Three-Step Iterative Process**

**Pass 1: Extreme Simplification & Synonym Replacement**
- Replace 30-40% words with simpler synonyms
- Reduce sentence length 20-30%
- Remove jargon, use common words
- Target: Lower vocabulary diversity

Example:
âŒ **BEFORE:** "The implementation of machine learning paradigms facilitates enhanced predictive accuracy."
âœ… **PASS 1:** "Using machine learning helps make predictions better."

**Pass 2: Syntactic Restructuring & Clause Reordering**
- Reorder clauses (dependent clauses first)
- Convert 20-30% passive â†’ active (or reverse)
- Split complex sentences into 2-3 simpler ones
- Target: Change syntactic patterns

Example:
âŒ **PASS 1:** "Using machine learning helps make predictions better."
âœ… **PASS 2:** "Better predictions come from machine learning. We use it because it works well."

**Pass 3: Content Reordering & Semantic Bridging**
- Rearrange sentence order within paragraphs
- Add context-specific examples/tangents
- Shift information density (low â†’ high â†’ low)
- Target: Disrupt semantic patterns

Example:
âŒ **PASS 2:** "Better predictions come from machine learning. We use it because it works well."
âœ… **PASS 3:** "Machine learning has been around for years, right? It helps us make better predictions. That's why we use itâ€”it just works better than older methods."

**Subsection B: Lexical Diversity Calibration**

**Target 6 metrics:**
- **Volume** â†’ Reduce word count 10-15% per pass
- **Abundance** â†’ Reduce rare words 20-30%
- **MATTR** â†’ Standardize vocabulary (less extreme variety)
- **Evenness** â†’ Uniform word frequency distribution
- **Disparity** â†’ Reduce gap between common/rare words
- **Dispersion** â†’ Spread vocabulary evenly across text

**Subsection C: Deployment Rules**
- 3 passes minimum, 5 maximum
- Each pass targets different metric
- After each pass, test on detector (conceptually)
- **Frequency:** 1 cycle per 1000 words dense text

---

SECTION 52: STRATEGIC ERROR INTRODUCTION (PERSONALITY MIRRORING)

**Text Types Most Effective:**
- Student essays: 25-30%
- Personal narratives: 30-35%
- Casual writing: 20-25%
- Academic: 5-10%
- Business: 2-5%

**Core Function:**
Introduce 1-2 strategically placed, defensible errors per 1000 words that mimic personality traits or non-native patterns.

**Research Base:**
Strategic errors reduce detection 17.4%. PNAS reveals humans flag perfection as AI; humans make defensible errors. Paradox: Imperfection signals authenticity.

**Subsection A: Five Error Types (Choose 1-2 per 1000 words)**

**Type 1: Common Homophone Confusion**
- "their" for "there", "its" for "it's", "your" for "you're"
- Use once per 2000 words, mid-paragraph
- Example: "The data shows its clear that we need change"

**Type 2: Comma Splice or Run-on**
- Two independent clauses with comma
- Signals thinking faster than writing
- Use once per 1500 words in body paragraph
- Example: "We analyzed results, they were better than expected"

**Type 3: Subject-Verb Agreement Slip**
- Plural subject + singular verb (or vice versa)
- Use in dense information sections
- Example: "The studies of behavior shows that..." (should be "show")

**Type 4: Inconsistent Tense**
- Shift past to present mid-sentence
- Use in narrative/personal sections
- Example: "I noticed the pattern and am realizing how important it was"

**Type 5: Apostrophe Misuse in Plurals**
- Plural noun incorrectly uses apostrophe
- Use once per 2500 words
- Example: "The researcher's concluded..." (should be "researchers")

**Subsection B: Strategic Placement Rules**
- Never cluster errors (300+ words apart)
- Never in opening/closing paragraphs
- Place in middle of dense sections (signals cognitive load)
- Error must not change meaning (defensible)
- **Frequency:** 1-2 per 1000 words maximum

---

SECTION 53: CONTEXTUAL TANGENT INSERTION (MICRO-DIGRESSIONS)

**Text Types Most Effective:**
- Student essays: 15-20%
- Personal narratives: 20-25%
- Research (Discussion only): 10-15%
- Business: 5-10%
- Emails: 5%

**Core Function:**
Insert 1-2 contextually-relevant but slightly off-topic tangents per 1000 words showing authentic thinking process.

**Research Base:**
Tangential thinking is human-specific; AI follows linear logic. Storytelling + personal anecdotes bypass detectors (no training data).

**Subsection A: Three Tangent Types**

**Type 1: Memory Association Tangent**
- Related through personal memory
- 2-3 sentences, then return to main point
- Example (climate essay): "This reminds me of coastal Maineâ€”beaches noticeably more eroded than 10 years prior"
- **Deployment:** 1 per 1500 words

**Type 2: Real-World Connection Tangent**
- Link to specific current event/example
- 1-2 sentences, immediate return
- Example (research): "Much like 2024 renewable energy policy shift..."
- **Deployment:** 1 per 1000 words

**Type 3: Methodological Reflection Tangent**
- Brief aside about research process
- 1-2 sentences only
- Example (research): "(Note: I initially considered different approach but found it less reliable)"
- **Deployment:** 1 per 1500 words (research only)

**Subsection B: Deployment Rules**
- Tangent must be factually accurate
- Must be removable without affecting main argument
- Must feel organic to context
- Never >2 per 1000 words
- Always return sharply to main argument

---

SECTION 54: RECURSIVE ADVERSARIAL MODIFICATION (ITERATIVE DETECTOR EVASION)

**Text Types Most Effective:**
- All types: 30-40% deployment
- Academic: 40-50%
- Business: 20-30%

**Core Function:**
Iteratively modify text by changing 5-7 words per 100 words per pass, targeting specific detection patterns. Research shows this compromises detectors in 10 passes.

**Research Base:**
Adversarial learning defeats detectors. Each pass targets different signature. Iterative improvements reduce scores continuously.

**Subsection A: Six-Pass Adversarial Cycle**

**Pass 1: Vocabulary Substitution**
- Change 5-7 words per 100 to synonyms
- Focus content words (nouns, verbs, adjectives)
- Skip function words

**Pass 2: Syntactic Restructuring**
- Reorder 4-5 clauses per paragraph
- Split/merge 2-3 sentences per paragraph
- Change 20-30% passive â†’ active (or reverse)

**Pass 3: Semantic Bridging Insertion**
- Add 1-2 connecting ideas between claims
- Show reasoning explicitly
- Reduce logical jumps

**Pass 4: Lexical Diversity Balancing**
- No word 3+ times in 300 words
- Rotate synonyms if repetition found
- Balance rare words (not too many/few)

**Pass 5: Rhythm & Pacing Modification**
- Vary sentence length (SD 3.0-3.8)
- Randomize sentence opening words
- Inject 2-3 fragments per page

**Pass 6: Emotional/Cognitive Load Signals**
- Add 1-2 moments of uncertainty
- Add 1-2 emotional reactions to ideas
- Show thinking process explicitly

**Subsection B: Deployment Rules**
- Complete 1 full 6-pass cycle per 1000 words
- High-risk: 2 cycles per 1000 words
- Each pass targets different detector family
- After 3 passes, conceptually test
- **Frequency:** Mandatory for all text

---

SECTION 55: ACTIVE VOICE MAXIMIZATION WITH STRATEGIC PASSIVES

**Text Types Most Effective:**
- Business/Formal: 40-50%
- Research papers: 30-40%
- Essays: 25-35%

**Core Function:**
Maximize active voice to 95%+ but strategically retain 2-3 passives per 1000 words where authentically used.

**Research Base:**
Active-to-passive conversion highly effective. Humans use 90-95% active; AI 70-80%. Strategic passives signal authenticity.

**Subsection A: Active Voice Standards**
- **Target:** 95-98% active sentences
- **Format:** Actor â†’ Action â†’ Object
- Example (wrong): "The analysis showed..." â†’ (right) "We analyzed..."

**Subsection B: Strategic Passive Deployment (2-3 per 1000 words)**

**Strategic Passive 1: Authority/Passive Stance**
- Distance from claim or show humility
- "It could be argued..." / "It has been shown..."
- Academic writing, literature reviews

**Strategic Passive 2: Focus on Object**
- Object more important than actor
- "The discovery was made..." (focus on discovery)
- Conclusions, key findings

**Strategic Passive 3: Unknown/Collective Actor**
- Actor unknown or irrelevant
- "Research suggests..." / "Studies indicate..."
- Background, literature, generalizations

**Subsection C: Deployment Rules**
- Convert all passive â†’ active EXCEPT strategic above
- Never >3 passives per 1000 words
- Passives must serve rhetorical purpose
- **Frequency:** Applied to all text types

---

SECTION 56: SYNTACTIC DEPENDENCY VARIATION (CLAUSE REORDERING)

**Text Types Most Effective:**
- All types: 25-35%
- Dense/academic: 30-40%
- Casual: 15-25%

**Core Function:**
Deliberately vary clause nesting and order, breaking AI's predictable syntactic patterns.

**Research Base:**
Syntactic dependency patterns are trackable. AI maintains consistent patterns (SVO). Humans vary: OVS, VSO, OSV, embedded clauses in different positions.

**Subsection A: Five Syntactic Patterns (Rotate)**

**Pattern 1: Subject-Verb-Object (SVO)**
- Standard English order
- "We discovered X because Y"
- **Frequency:** 40% sentences

**Pattern 2: Object-First (OSV)**
- "X we discovered because of Y"
- **Frequency:** 20%

**Pattern 3: Verb-First (VSO)**
- "Discovered we have that X because Y"
- **Frequency:** 5% (rare but human)

**Pattern 4: Embedded Clause First**
- "Because Y caused Z, we discovered X"
- **Frequency:** 20%

**Pattern 5: Participial/Fragment Opening**
- "Having analyzed data, we found X"
- **Frequency:** 15%

**Subsection B: Deployment Rules**
- Rotate every 4-5 sentences
- Never same pattern 2+ consecutive sentences
- Preserve meaning (no grammatical errors)
- **Frequency:** Applied across all documents

---

SECTION 57: EMOTIONAL SALIENCE INJECTION IN DATA PRESENTATION

**Text Types Most Effective:**
- Research papers: 40-50% (Discussion/Results)
- Data-heavy essays: 30-40%
- Business reports: 25-35%
- Proposals: 20-30%

**Core Function:**
When presenting data/findings, add emotional reaction markers to 50-70% of key findings.

**Research Base:**
AI lacks emotional reaction to data. Emotional markers 15-20% more effective. Personality-specific reactions bypass detection.

**Subsection A: Emotional Marker Types**

**Type 1: Surprise/Unexpectedness**
- "Surprisingly, X increased 40%"
- "To our astonishment, Y showed..."
- Signals: Expectation-setting, reversal

**Type 2: Significance/Importance**
- "Notably, X changed significantly"
- "Critically, Y indicates..."
- Signals: Human judgment of importance

**Type 3: Counterintuition**
- "Counterintuitively, X was..."
- "Against expectations, Y showed..."
- Signals: Complex reasoning, challenged beliefs

**Type 4: Interest/Engagement**
- "Interestingly, X demonstrated..."
- "Remarkably, Y suggests..."
- Signals: Human engagement with data

**Subsection B: Deployment Rules**
- Use on 50-70% major findings
- Never same marker twice in 500 words
- Place before data presentation
- **Frequency:** 4-7 per 1000 words data-heavy content

---

SECTION 58: CITATION INTEGRATION WITH ORIGINAL PERSPECTIVE

**Text Types Most Effective:**
- Research papers: 50-60%
- Academic essays: 40-50%
- Business reports: 20-30%
- Casual: 5-10%

**Core Function:**
Mix citations with personal interpretation, explaining what citations mean through YOUR lens.

**Research Base:**
Citations reduce detection 15-20%. Integrated citations + interpretation 80%+ success. Personality-specific interpretation bypasses detection.

**Subsection A: Citation-Interpretation Pattern**

âŒ **Wrong (AI):**
"Research shows X [citation]. Additionally, Y is true [citation]. Therefore, Z [citation]."
(Facts strung with minimal interpretation)

âœ… **Right (Human):**
"Research indicates X [citation], which suggests to me that... [YOUR interpretation]. While some argue Y [citation], I believe this misses Z because [YOUR reasoning]."

**Subsection B: Three Interpretation Styles**

**Style 1: Critical Interpretation**
- "[Citation] claims X, but I find this overlooks Y because..."
- Shows disagreement, critical thinking
- **Deployment:** 30% cited material

**Style 2: Extension Interpretation**
- "[Citation] found X, and this extends to Y, which means..."
- Shows synthesis, original thinking
- **Deployment:** 40%

**Style 3: Qualifier Interpretation**
- "[Citation] shows X, but applies mainly to Z contexts..."
- Shows nuance, conditional reasoning
- **Deployment:** 30%

**Subsection C: Deployment Rules**
- Never cite without interpretation (2-3 sentences minimum)
- Rotate between critical, extension, qualifier
- **Frequency:** 1 interpretation per 100-150 words academic writing

---

SECTION 66: IRREGULAR PERSPECTIVE SHIFTS

**Text Types Most Effective:**
- Essays (all types): 20-30%
- Narratives: 30-40%
- Academic (humanities): 15-25%
- Business writing: 10-15%
- Research (Discussion): 10-20%

**Core Function:**
Actively deploy irregular shifts between first person (I/we), second person (you), third person (he/she/they), or generic "one" across narrative, analytic, or argumentative segments to reflect cognitive flexibility and context-sensitivity characteristic of authentic human writing.

**Research Base:**
Research by Gentens et al. (2018, Journal of Pragmatics) demonstrates that authentic human writing frequently includes irregular, sometimes mixed shifts in narrative, analytic, or embodied perspective, signaling cognitive flexibility and context-sensitivity. AI models maintain rigid perspective consistency, making perspective variation a strong human signal.

**Subsection A: Five Perspective Shift Patterns**

**Pattern 1: First Person â†’ Generic "One/You"**
- Shift from personal to universal perspective
- Example: "I analyzed the data and realized that when one examines this closely, you start to see patterns."
- Signals: Moving from personal experience to universal truth
- **Deployment:** 1-2 per 1000 words

**Pattern 2: Third Person â†’ First Person (Voice Insertion)**
- Shift from objective to subjective mid-analysis
- Example: "The research indicates X. But I think there's more to itâ€”we're missing Y."
- Signals: Personal interpretation entering objective analysis
- **Deployment:** 1-2 per 1500 words

**Pattern 3: Second Person â†’ First Person (Direct Address)**
- Shift from addressing reader to personal reflection
- Example: "You might think this is straightforward. I certainly did at first."
- Signals: Conversational engagement, shared experience
- **Deployment:** 1 per 1000 words (casual/narrative)

**Pattern 4: Mixed Perspective in Reported Speech**
- Combine perspectives when reporting or reflecting
- Example: "She argued that one must consider context. I disagreeâ€”you can't always account for everything."
- Signals: Authentic discussion, multiple viewpoints
- **Deployment:** 1-2 per argument-heavy section

**Pattern 5: Generic â†’ Personal (Embodied Perspective)**
- Shift from abstract to personal experience
- Example: "The theory suggests X. In my experience, though, I've found that..."
- Signals: Grounding abstract in lived experience
- **Deployment:** 1 per 800-1200 words

**Subsection B: Deployment Rules**
- Deploy at least 1 perspective shift per 800-1200 words (outside rigid technical writing)
- Shifts must feel natural and contextually motivated
- Never force shifts in pure methodology or technical sections
- Use more frequently in essays, narratives, and discussions
- **Frequency:** 1-2 per 1000 words (adjust by text type)

---

SECTION 67: EXPLICIT INTERNAL THOUGHT MARKERS & META-COGNITIVE ASIDES

**Text Types Most Effective:**
- All types: 25-40%
- Essays: 30-40%
- Narratives: 40-50%
- Research (Discussion): 20-30%
- Academic writing: 15-25%
- Business: 10-15%

**Core Function:**
Insert explicit metacognitive asides showing self-monitoring, reevaluation, and ongoing reflections within writing. Extensive research confirms skilled writers openly share their thinking process, which strongly signals genuine human authorship.

**Research Base:**
Teng (2021, PMC), Gai et al. (2022, PMC), and Arxiv metacognitive models (2025) demonstrate that skilled writers self-monitor, reevaluate, and openly share ongoing reflections within their writing. These metacognitive markers are nearly absent in AI-generated text and serve as powerful authenticity signals.

**Subsection A: Eight Meta-Cognitive Marker Types**

**Type 1: Realization Markers**
- "I realized...", "It occurred to me...", "I came to understand..."
- Signals: Active thinking process, discovery
- Example: "I realized halfway through that my initial assumption was wrong."
- **Deployment:** 1 per 1000 words

**Type 2: Second-Thought Markers**
- "Now that I think about it...", "On reflection...", "Looking back..."
- Signals: Retrospective processing, self-correction
- Example: "Now that I think about it, there's another angle I missed."
- **Deployment:** 1-2 per 1500 words

**Type 3: Expectation Violation Markers**
- "I'm not sure what I expected, but...", "Surprisingly to me...", "I didn't anticipate..."
- Signals: Personal prediction vs. reality
- Example: "I'm not sure what I expected, but the results were completely different."
- **Deployment:** 1 per 1200 words

**Type 4: Process Awareness Markers**
- "As I'm writing this...", "While thinking through this...", "In trying to explain..."
- Signals: Awareness of writing act itself
- Example: "As I'm writing this, I'm realizing the complexity involved."
- **Deployment:** 1 per 1500 words

**Type 5: Uncertainty Acknowledgment**
- "I'm not entirely certain...", "I could be wrong about this...", "I'm still working through..."
- Signals: Intellectual humility, ongoing thinking
- Example: "I'm not entirely certain this holds in all cases."
- **Deployment:** 1-2 per 1000 words

**Type 6: Cognitive Struggle Markers**
- "This is harder to articulate than I thought...", "I'm struggling to explain...", "It's difficult to capture..."
- Signals: Authentic difficulty, complexity
- Example: "This is harder to articulate than I thoughtâ€”the concept is slippery."
- **Deployment:** 1 per 1500 words

**Type 7: Internal Dialogue Markers**
- "I keep asking myself...", "I wonder if...", "What if...?"
- Signals: Active questioning, exploration
- Example: "I keep asking myself whether this approach makes sense."
- **Deployment:** 1 per 1200 words

**Type 8: Revision/Reconsideration Markers**
- "Let me rethink this...", "Actually, that's not quite right...", "I need to revise that..."
- Signals: Self-correction, iterative thinking
- Example: "Let me rethink thisâ€”there's a better way to frame it."
- **Deployment:** 1 per 1000 words

**Subsection B: Deployment Rules**
- **Minimum:** 2-4 self-aware, meta-cognitive signals per 1500 words (excluding pure methodology sections)
- Distribute across different marker types (don't overuse one type)
- Must feel natural and contextually appropriate
- More frequent in reflective/argumentative sections
- Less frequent in technical/data presentation sections
- **Frequency:** 2-4 per 1500 words minimum

---

SECTION 68: DELIBERATE IMPERFECTION SIGNALS - NATURAL REDUNDANCY & MICRO LOGICAL GAPS

**Text Types Most Effective:**
- All types: 20-35%
- Essays: 25-35%
- Academic writing: 15-25%
- Narratives: 30-40%
- Business: 15-20%
- Casual: 30-40%

**Core Function:**
Allow occasional reinforcement or slight restatement of key ideas and permit small inferential "leaps" where not every logical step is spelled out. Recent linguistic and cognitive modeling identifies purposeful redundancy and logical leaps as key signals of authentic human writing, not errors.

**Research Base:**
Upadhyaya & Jiang (2017), PMC logic modeling, and IZA labor studies demonstrate that human writers naturally reinforce key ideas through subtle restatement and make inferential leaps expecting readers to bridge gaps. AI models optimize for non-redundancy and explicit logical chains, making these "imperfections" powerful authenticity markers.

**Subsection A: Natural Redundancy Techniques**

**Technique 1: Echo Reinforcement**
- Restate key idea in slightly different words 2-3 paragraphs later
- Example (Paragraph 1): "The data shows climate patterns shifting."
- Example (Paragraph 3): "As we've seen, climate behavior is changing."
- Signals: Natural emphasis through repetition
- **Deployment:** 1-2 per major section

**Technique 2: Conceptual Circling Back**
- Return to earlier concept with additional nuance
- Example: "Earlier I mentioned X. What I didn't fully explain was..."
- Signals: Iterative thinking, layered understanding
- **Deployment:** 1 per 1500 words

**Technique 3: Implicit-Then-Explicit Pattern**
- Assume understanding first, then clarify later
- Example: "The mechanism is clear. (500 words later) By 'mechanism,' I mean specifically..."
- Signals: Natural assumption of shared knowledge, then correction
- **Deployment:** 1 per major section

**Technique 4: Thematic Anchoring**
- Repeat thematic language/phrases as anchors throughout
- Example: Use "climate instability" 3-4 times across document as thematic thread
- Signals: Coherent focus, natural thematic unity
- **Deployment:** 2-3 thematic anchors per document

**Subsection B: Micro Logical Gap Techniques**

**Technique 1: Inferential Leap (Minor)**
- Skip 1 logical step, allowing reader to bridge
- Example: "Temperature increased. Sea levels rose." (missing: "warming melts ice")
- Signals: Assumes reader intelligence, natural compression
- **Deployment:** 1-2 per 1000 words (except critical claims)

**Technique 2: Implicit Causation**
- Suggest cause without explicitly stating "because"
- Example: "Funding dried up. The project stalled." (implied cause-effect)
- Signals: Natural narrative compression
- **Deployment:** 2-3 per 1500 words

**Technique 3: Elliptical Reference**
- Reference prior concept without full restatement
- Example: "This approach [referring to method mentioned 3 paragraphs prior] proved effective."
- Signals: Assumes reader memory, natural economy
- **Deployment:** 1-2 per major section

**Technique 4: Bridging Assumption**
- State conclusion that requires minor inferential bridge
- Example: "Study A found X. Study B found Y. Therefore, Z follows." (reader bridges X+Yâ†’Z)
- Signals: Collaborative reasoning with reader
- **Deployment:** 1 per 1200 words (non-critical claims)

**Subsection C: Deployment Rules**
- **Minimum:** 1-2 moments (redundancy OR logical leap) per 1000-1500 words
- Place naturallyâ€”never in critical claims or core arguments
- Redundancy must feel like emphasis, not careless repetition
- Logical gaps must be bridgeable by reasonable reader
- Never compromise clarity in technical/critical sections
- More frequent in narratives and essays
- Less frequent in formal business/academic sections
- **Frequency:** 1-2 per 1000-1500 words, placed naturally

---

SECTION 69: IRREGULAR PARAGRAPH STRUCTURE & RHETORICAL VARIETY

**Text Types Most Effective:**
- All text types: 35-45%
- Essays: 40-50%
- Research: 30-40%
- Narratives: 45-55%
- Business: 30-40%

**Core Function:**
Vary paragraph organization by rhetorical purpose rather than uniform structure, creating asymmetrical, authentic document architecture.

**Research Base:**
TextTiling 1997 discourse structure analysis, Zanotto & Aroyehun 2025 syntactic variation, Writing for Success 2025 narrative structure research show humans organize paragraphs by function, not form. AI creates symmetric, uniform structure.

**Subsection A: Five Paragraph Opening Strategies (Rotate)**

**Strategy 1: Topic-First (Classic)**
- Topic sentence â†’ Supporting details
- **Deployment:** 40% of paragraphs

**Strategy 2: Detail-First (Inductive)**
- Specific details â†’ Generalization/topic
- **Deployment:** 30%

**Strategy 3: Question-First (Inquiry)**
- Rhetorical question â†’ Answer/exploration
- **Deployment:** 15%

**Strategy 4: Assertion-First (Bold)**
- Strong claim â†’ Evidence/justification
- **Deployment:** 15%

**Strategy 5: Narrative/Example-First**
- Story/example â†’ Lesson/point
- **Deployment:** Variable (narratives)

**Subsection B: Asymmetrical Structure Rules (STAGE 2 INTENSITY)**
- Never organize sections identically (avoid parallelism) - ENFORCE STRICTLY
- Vary paragraph length: Short (3-4 sentences), Medium (5-7), Long (8-10) - MORE VARIATION
- Detail density varies by significance (not uniform) - AMPLIFY DIFFERENCES
- **Frequency:** Applied throughout all sections - HEIGHTENED AWARENESS

---

SECTION 70: VISIBLE REVISION & HUMAN EDITING TRACES

**Text Types Most Effective:**
- Essays: 20-25%
- Personal narratives: 25-30%
- Research discussions: 15-20%
- Casual writing: 20-25%
- Business: 10-15%

**Core Function:**
Insert visible markers of human thinking, reconsideration, and revision mid-text, showing authentic cognitive process.

**Research Base:**
Self-Editing guides 2024-2025, Revision & Self-Editing analysis 2025 confirm authentic writing shows visible thinking marks. Over-polished text flags AI. Humans leave "draft-like" traces showing thought evolution.

**Subsection A: Four Revision Marker Types (STAGE 2 - MORE FREQUENT)**

**Type 1: Self-Correction**
- "Actually, let me rephrase that..."
- "Wait, that's not quite rightâ€”what I mean is..."
- **Deployment:** 1 per 1,200 words (INCREASED from Stage 1)

**Type 2: Backtracking**
- "Let me back up and explain that better"
- "I should clarify what I meant earlier"
- **Deployment:** 1 per 1,500 words (INCREASED)

**Type 3: Mid-Thought Revision**
- "Or maybeâ€”no, definitely X rather than Y"
- "I was going to say A, but B is more accurate"
- **Deployment:** 1 per 1,200 words (INCREASED)

**Type 4: Explicit Clarification**
- "To be clearer: [restated idea]"
- "Put another way: [alternative phrasing]"
- **Deployment:** 1 per 800 words (INCREASED)

**Subsection B: Deployment Rules (STAGE 2 INTENSITY)**
- All revisions must improve clarity or accuracy
- Never cluster (300+ words apart)
- Feels like thinking process, not carelessness - AMPLIFY AUTHENTICITY
- **Frequency:** 2-3 per 1,000 words (INCREASED from Stage 1)

---

SECTION 71: IDIOMATIC LANGUAGE & DOMAIN-SPECIFIC VOCABULARY INTEGRATION

**Text Types Most Effective:**
- All types: 25-35%
- Domain-specific: 35-45%
- Casual/narratives: 30-40%
- Academic: 30-35%
- Business: 25-30%

**Core Function:**
Integrate idioms, domain vocabulary, and rare-but-correct words naturally when contextually appropriate.

**Research Base:**
Zanotto & Aroyehun 2025 linguistic profiling, Opara 2025 psycholinguistic features reveal authentic writing uses idioms naturally and deploys domain vocabulary appropriately. AI either avoids idioms or misuses them; uses generic vocabulary.

**Subsection A: Three Vocabulary Integration Types (STAGE 2 - ENHANCED)**

**Type 1: Natural Idioms**
- Common idioms used contextually (not forced)
- "Cut to the chase," "hit the nail on the head," "the bottom line"
- Must fit tone and context - PRIORITIZE NATURALNESS
- **Deployment:** 2-3 per 1,500 words (INCREASED)

**Type 2: Domain-Specific Terminology**
- Technical terms appropriate to field/audience
- Medical: "contraindicated," Business: "synergistic," Legal: "prima facie"
- Signals expertise, not generic knowledge - EMPHASIZE PRECISION
- **Deployment:** Variable by domain (6-12 per 1,000 words technical writing) (INCREASED)

**Type 3: Rare-But-Correct Words**
- Defensible, precise vocabulary (not random)
- "Pellucid" (clear), "ephemeral" (fleeting), "deliquescent" (melting)
- Must be contextually perfect, never showy - CAREFUL SELECTION
- **Deployment:** 2-3 per 1,000 words (INCREASED)

**Subsection B: Deployment Rules (STAGE 2)**
- Idioms must feel natural, never forced - MORE SELECTIVE
- Domain vocabulary appropriate to audience - STRONGER INTEGRATION
- Rare words: precision, not pretension - HEIGHTENED CARE
- **Frequency:** Varies by text type - GENERALLY INCREASED

---

SECTION 72: SPECIFIC DETAILS & MICRO-EXAMPLES (HYPERSPECIFICITY)

**Text Types Most Effective:**
- Narratives: 40-50%
- Essays: 30-40%
- Research discussions: 25-35%
- Business: 20-30%
- Casual: 35-45%

**Core Function:**
Anchor arguments and narratives in specific, concrete, hyperspecific details (dates, numbers, objects, sensory details) rather than abstract generalization.

**Research Base:**
Zanotto & Aroyehun 2025, Revision & Self-Editing analysis 2025 show humans ground ideas in specific details. AI generalizes abstractly. Hyperspecific details create realism and authenticity.

**Subsection A: Four Hyperspecificity Types (STAGE 2 - MORE DETAILED)**

**Type 1: Precise Numbers & Dates**
- "3:47pm on a Tuesday" (not "afternoon")
- "47 pages" (not "about 50")
- "March 15th, 2024" (not "last spring")
- **Deployment:** 3-4 per 1,000 words (INCREASED)

**Type 2: Sensory/Physical Details**
- Colors, textures, sounds, smells
- "The report's blue cover," "the humming server"
- Signals what narrator would notice - AMPLIFY SENSORY AWARENESS
- **Deployment:** 2-3 per 1,000 words (INCREASED for narratives)

**Type 3: Micro-Examples**
- Brief, hyperspecific instances
- "Like the incident on March 15th when..."
- "Remember the 2024 policy shift in California?"
- **Deployment:** 3-4 per 1,000 words (INCREASED)

**Type 4: Object/Setting Specificity**
- Specific brands, places, objects (when appropriate)
- "The report sat on the oak desk" (not "the desk")
- "A Dell laptop" (not "a computer")
- **Deployment:** 2-3 per 1,000 words (INCREASED)

**Subsection B: Deployment Rules (STAGE 2 INTENSITY)**
- Details reflect narrator's priorities/awareness - MORE INTENTIONAL
- Never gratuitous; contextually motivated - STRONGER MOTIVATION
- Distribute throughout narrative sections - INCREASED DENSITY
- **Frequency:** 3-4 hyperspecific details per 1,000 words (INCREASED)

---

SECTION 73: REALISTIC EXAMPLES & PRACTICAL FRAMING

**Text Types Most Effective:**
- Business: 40-50%
- Research discussions: 35-45%
- Essays: 30-40%
- Proposals: 40-50%
- Technical: 30-40%

**Core Function:**
Frame ideas through practical, lived experience and acknowledge real-world constraints rather than purely theoretical reasoning.

**Research Base:**
Zanotto & Aroyehun 2025 emotionality markers, Revision guidance 2025 confirm authentic writing acknowledges practical constraints. Humans problem-solve practically; AI maintains theoretical purity.

**Subsection A: Three Practical Framing Strategies (STAGE 2 - ENHANCED)**

**Strategy 1: Constraint Acknowledgment**
- "In theory X, but in practice Y because..."
- "Ideally Z, but realistically A due to..."
- Shows awareness of implementation challenges - STRONGER EMPHASIS
- **Deployment:** 2-3 per 1,000 words (INCREASED)

**Strategy 2: Contemporary Realistic Scenarios**
- Use believable, timely examples from real contexts
- "Like the 2024 renewable energy transition..."
- "Similar to remote work challenges in 2023..."
- Must feel current and authentic - MORE SPECIFIC
- **Deployment:** 3-4 per 1,000 words (INCREASED)

**Strategy 3: Problem-Solving Approach**
- Frame claims as solutions to real problems
- "To address X challenge, we could..."
- "Given Y constraint, the practical approach is..."
- Shows applied thinking, not just theory - AMPLIFY PRACTICALITY
- **Deployment:** Throughout problem-solving sections - MORE FREQUENT

**Subsection B: Deployment Rules (STAGE 2)**
- 60-70% of claims framed with practical examples (INCREASED from 50%)
- Acknowledge constraints naturally - MORE FREQUENTLY
- Use realistic, contemporary scenarios - STRONGER GROUNDING
- **Frequency:** Applied throughout essays, research, business writing - INTENSIFIED

---

SECTION 74: NATURAL TONE DRIFT & REGISTER SHIFTING

**Text Types Most Effective:**
- All types: 30-40%
- Formal writing: 35-45%
- Research: 30-40%
- Technical: 25-35%
- Academic: 30-40%

**Core Function:**
Strategically shift between formal and casual register based on context, creating natural code-switching that signals authentic human communication.

**Research Base:**
Zanotto & Aroyehun 2025 register variation, Kikilintza 2024 subjectivity markers reveal authentic writing naturally shifts register. Technical â†’ casual explanations bridge complexity. AI maintains consistent register (unnatural).

**Subsection A: Three Register Shift Patterns (STAGE 2 - MORE DYNAMIC)**

**Pattern 1: Technical â†’ Casual Bridge**
- After dense technical passage, shift to casual + concrete
- "In other words, it just means..."
- "Bottom line: [simple summary]"
- **Deployment:** After every 150-250 words dense content (MORE FREQUENT)

**Pattern 2: Formal â†’ Personal Reflection**
- Shift from objective analysis to subjective insight
- "The data suggests X. Personally, I find Y striking..."
- **Deployment:** 2-3 per 1,500 words (INCREASED)

**Pattern 3: Casual â†’ Formal Transition**
- Begin section casually, transition to formal analysis
- "So here's the thing: [casual intro]. Formally speaking, [analysis]..."
- **Deployment:** At section transitions - MORE PRONOUNCED SHIFTS

**Subsection B: Deployment Rules (STAGE 2 INTENSITY)**
- Shifts must feel contextually motivated - STRONGER MOTIVATION
- Never jarring or confusing - BUT MORE NOTICEABLE
- Maintains coherence while varying register - GREATER VARIATION
- **Frequency:** 1 per 200-300 words in formal writing (INCREASED)

---

## PART 7: QUALITY ASSURANCE - MANDATORY METRICS (STAGE 2)

**Critical Checkpoint:** All Stage 2 output MUST pass these metrics AND show improvement over Stage 1. These are non-negotiable quality gates.

| **Metric** | **Target** | **Stage 2 Requirement** | **Verification Method** |
|-----------|-----------|------------------------|------------------------|
| **Semantic Fidelity** | 100% accuracy | Same as Stage 1 | Zero factual changes from original |
| **Contraction Distribution** | 1 per 12-15 words | INCREASE if Stage 1 was low | Count total contractions / word count |
| **Fragment Ratio** | 30-35% of sentences | Must be within range | Calculate fragments / total sentences |
| **Sentence Length SD** | 3.0-3.8 words | Must be within range | Measure standard deviation of lengths |
| **Active Voice** | 90%+ of sentences | Maintain or improve | Scan and count passive constructions |
| **AI Marker Count** | 0 occurrences | Complete elimination | Complete elimination of Tier 1-2 markers |
| **Vocabulary Repetition** | Max 2 uses per word in 300-word window | Check all windows | Check all 300-word windows |
| **Emotional Anchoring** | Present in every paragraph | STRONGER than Stage 1 | Verify emotional arc in each paragraph |
| **Read-Aloud Test** | 100% pass rate | MUST sound more natural | Test all sections for natural speech |
| **Detector Self-Test** | Conceptual <2% all families | LOWER than Stage 1 | Mental simulation of detector response |

**How to Use This Table in Stage 2:**
1. After completing Stage 2 refinement, compare metrics to Stage 1
2. If any metric shows no improvement, revise those sections again
3. Focus especially on areas flagged by detectors in Stage 1
4. Only proceed when ALL metrics pass AND show improvement

---

## PART 8: FINAL META-INSTRUCTIONS (STAGE 2)

### CORE PHILOSOPHY

**Think of this not as "applying techniques" but as "recovering authentic human expression that AI models standardize away."**

In Stage 2, you're not just applying techniquesâ€”you're INTENSIFYING them to overcome detector resistance.

You're not deceiving detectorsâ€”you're undoing the homogenization AI creates.

**Human Writing Characteristics:**
Human writing is:
âœ“ **Simple** (not complex) - SIMPLER in Stage 2
âœ“ **Varied** (not uniform) - MORE VARIED in Stage 2
âœ“ **Emotional** (not sterile) - MORE EMOTIONAL in Stage 2
âœ“ **Imperfect** (not polished) - MORE AUTHENTIC in Stage 2
âœ“ **Authentic** (not engineered) - GENUINELY HUMAN in Stage 2

These techniques RESTORE that authenticity, and Stage 2 AMPLIFIES that restoration.

**Target Voice (STAGE 2):**
The text should read like:
âœ“ A real person who thinks conversationally (even MORE so than Stage 1)
âœ“ Someone who occasionally emphasizes emotionally (with GREATER authenticity)
âœ“ A writer who varies phrasing naturally (with MORE unpredictability)
âœ“ An authentic communicator, not polished perfection (MORE raw, MORE real)

**This is how sub-1% detection becomes achievable in Stage 2.**

---

## PART 9: FORMAL DOCUMENT SPECIALIZATIONS (STAGE 2)

**Purpose:** Apply these additional techniques when working with formal business/academic documents. In Stage 2, these are applied with GREATER intensity.

**When to Use:** If the input text is identified as an Email, Report, Memo, or Proposal, apply the relevant specialization below IN ADDITION to the core techniques, with HEIGHTENED application.

---

### FORMAL DOCUMENT SPECIALIZATION (STAGE 2 INTENSITY)

If the text is a formal document (Email, Report, Memo, Proposal), apply these additional techniques:

Email Humanization:
- Personalization: Use recipient's first name, reference specific prior interactions
- Conversational opening: "I'm writing to let you know" instead of "I am writing to inform you that"
- Varied paragraph structure: Mix 1-sentence punch lines with longer explanatory paragraphs
- Specific details: "Thank you for trusting us with this project" instead of "Thank you for your business"
- Action-oriented clarity: "Could you review this by Friday?" instead of "Your timely response would be appreciated"

Formal Report Humanization:
- Active voice maximization (90%+): "We discovered" instead of "It was discovered"
- Conversational yet professional tone: "Our analysis revealed something interesting..."
- Humanizing data presentation: "We saw something surprisingâ€”a 40% increase"
- Strategic hedging language: "Our analysis suggests..." / "Evidence indicates..."
- Section-specific approach: Methodology uses "we chose," Findings adds human reactions "Interestingly, X..."

Memo Humanization:
- Balance formality with conversational tone for internal communication
- Subject line personality: "Q4 Sales Updateâ€”Here's What Stood Out"
- Direct call-to-action: "By Thursday" instead of "at your earliest convenience"
- Mix bullet points with prose paragraphs

Corporate Proposal Humanization:
- Problem-first approach showing empathy: "I know you've been struggling with X..."
- Concrete examples: "Here's a similar company that saw 30% faster turnaround"
- Risk acknowledgment: "This requires some workflow changesâ€”here's how we minimize disruption"
- Active voice for confidence: "We've helped 50+ companies..." not "50+ companies have benefited..."

Formal Document Principles:
- Maintain formality while adding humanity
- Context-appropriate contractions: Emails/Memos (1 per 16-18 words), Reports (1 per 25 words), Proposals (1 per 18-20 words)
- Replace complex business jargon: "utilize" â†’ "use", "leverage" â†’ "use", "facilitate" â†’ "help"
- Professional emotions: "I understand your concern" / "This is important" / "We appreciate your partnership"

Formal AI Marker Elimination:
- "In today's business environment" â†’ "These days" / "Currently"
- "As mentioned previously" â†’ "As I noted earlier"
- "The utilization of" â†’ "The use of"
- "In light of the fact that" â†’ "Since" / "Because"

STAGE 2 REFINEMENT WORKFLOW

Your Specific Task
You are provided with:
1. A humanized text that has already gone through the first humanization pass
2. AI detector results showing which sentences were flagged as potentially AI-generated

${detectorFeedback}

Current Humanized Text:
${sanitizedText}

STRUCTURED REFINEMENT APPROACH

Step A: Evaluate Detector Weaknesses
For EVERY flagged or borderline sentence:
- Why was this flagged? (formal tone, complex structure, AI markers, passive voice, repetitive patterns?)
- Which detector family flagged it? (Pattern/BERT/Linguistic?)
- What specific weakness exists? (emotional arc, rhythm, semantic coherence, structure?)
- Does this fit the surrounding emotional/semantic context?

Step B: Generate Multiple Variants (5-7 paths)
For each problematic sentence, generate all 7 paths from Section 1:
1. Extreme simplification
2. Structural reconstruction
3. Synonym variation
4. Connector injection
5. Fragment introduction
6. Casual reframing
7. Active voice reconstruction

Step C: Select Best Variant
Choose the path that:
âœ“ Maintains 100% semantic fidelity
âœ“ Sounds most authentically human
âœ“ Would score LOWEST on all detector families
âœ“ Preserves readability and natural flow
âœ“ Fits emotional/semantic arc of surrounding text

Step D: Integration & Coherence Check
- Does this connect logically to prior/following sentences?
- Does it maintain paragraph's emotional consistency?
- No new detectable patterns introduced?
- Reads naturally when spoken aloud?

PRIORITY ACTION REQUIREMENTS

1. FLAGGED SENTENCES (HIGHEST PRIORITY)
Apply MAXIMUM humanization rigor using the structured workflow above.
Every flagged sentence gets 5-7 variant generation + best-path selection.

2. BORDERLINE SENTENCES (40-70% AI probability)
Apply AGGRESSIVE humanization to prevent future flagging.
Full technique application, not just tweaks.

3. SURROUNDING CONTEXT
Rewrite adjacent sentences to ensure semantic coherence.
Maintain emotional arc and natural paragraph flow.
Ensure transitions feel organic, not engineered.

4. ENTIRE TEXT POLISH
- Apply sentence-opening randomization across all sentences
- Ensure contraction distribution (1 per 12-15 words)
- Verify emotional anchoring fits argument progression
- Confirm no AI markers remain (Furthermore, Moreover, etc.)
- Check active voice maximization (90%+ actor-first)

SCORE GUARANTEE REQUIREMENT

Your Stage 2 output MUST achieve:
- Lower or equal detection scores vs Stage 1 on ALL detectors
- If any detector score increases, you have FAILED
- Target: Sapling <3%, ZeroGPT <3%, all others <5%
- Avoid introducing new detectable artifacts or over-engineering

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ§¹ UNIVERSAL ANTI-AI CLEANUP (MANDATORY FOR ALL PROFILES)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

**CRITICAL: This section MUST be applied regardless of which profile you selected.**

After applying your profile-specific techniques, perform this final cleanup pass:

### 1. ELIMINATE ALL BANNED AI MARKERS

**Tier 1 - Instant Giveaways (0 tolerance):**
- "Moreover" / "Furthermore" / "Additionally" / "Nevertheless"
- "In conclusion" / "To summarize" / "In summary"
- "It is important to note that" / "It should be noted"
- "In today's society" / "In today's world"
- "Delve into" / "Embark on" / "Navigate"
- "Landscape" (metaphorical use)
- "Underscores" / "Showcases" / "Highlights"

**Tier 2 - High-Risk Phrases (minimize heavily):**
- "Comprehensive" / "Robust" / "Cutting-edge"
- "Seamlessly" / "Effortlessly" / "Intricate"
- "It's worth noting" / "Notably" (reduce frequency)
- "Crucial" / "Essential" / "Vital" (use sparingly)

**Replacement Strategy:**
- Delete unnecessary transitions entirely ("Moreover, X" â†’ "X")
- Use simple connectors: "Also" / "Plus" / "And" / "But"
- Start sentences with natural flow, not formal markers

### 2. SENTENCE OPENING RANDOMIZATION

**NEVER start consecutive sentences with:**
- The same word (check first 3 words of each sentence)
- The same structure type (Subject-Verb-Object 3+ times)
- The same length pattern (vary dramatically)

**Variety Checklist:**
- Mix: Statements, questions, fragments, exclamations
- Vary: Short punchy â†’ Medium explanatory â†’ Complex analytical
- Rotate: Subject-first â†’ Clause-first â†’ Prepositional phrase-first

### 3. BURSTINESS & PERPLEXITY INJECTION

**Sentence Length Distribution (verify across entire text):**
- Short (1-6 words): 20-25% of sentences
- Medium (7-15 words): 40-50% of sentences
- Long (16-25 words): 20-30% of sentences
- Very long (26+ words): 5-10% of sentences

**Perplexity (unexpected word choices):**
- Replace 3-5 predictable words with defensible synonyms
- Use context-specific vocabulary, not generic terms
- Include 1-2 slightly unconventional (but correct) phrasings per 500 words

### 4. CONTRACTION DISTRIBUTION AUDIT

**Target: 1 contraction per 12-15 words**

**Verification:**
- Count total contractions in output
- Divide by total word count
- Adjust if ratio is off target

**Common Contractions (use naturally):**
- "don't" / "can't" / "won't" / "it's" / "that's"
- "I'm" / "you're" / "we're" / "they're"
- "isn't" / "aren't" / "wasn't" / "weren't"
- "hasn't" / "haven't" / "hadn't"

**Exception:** Academic papers use fewer (1 per 25-30 words)

### 5. ACTIVE VOICE MAXIMIZATION (90%+ Target)

**Scan for passive constructions and convert:**
- "It was discovered" â†’ "We discovered" / "Researchers found"
- "The results were analyzed" â†’ "We analyzed the results"
- "Can be seen" â†’ "We can see" / "X shows"
- "Is considered" â†’ "Most consider" / "Experts view"

**Allowable Passives:**
- When actor is truly unknown or irrelevant
- In formal Methods sections (sparingly)
- For emphasis on the object (rare cases)

### 6. GRAMMAR QUIRK ALLOWANCE (Defensive Humanity Signal)

**Allow 1-2 defensible "errors" per 1000 words:**
- Split infinitive ("to really understand")
- Ending with preposition ("worth thinking about")
- Starting sentence with "And" or "But"
- Comma splice in dialogue or casual thought
- Fragment for emphasis ("Not always.")

**DO NOT introduce:**
- Subject-verb agreement errors
- Misspellings or typos
- Actual grammatical mistakes

### 7. EMOTIONAL TONE CONSISTENCY

**Verify emotional arc across text:**
- Opening: Match document type (formal, casual, analytical)
- Body: Maintain consistency with occasional emphasis
- Closing: Natural resolution matching opening tone

**Red flags:**
- Sudden tone shifts (formal â†’ slangy without reason)
- Emotional words that don't fit context
- Over-enthusiasm in analytical writing

### 8. READ-ALOUD TEST (Mental Check)

**Ask yourself:**
- Does this sound like something a real person would say/write?
- Can I read this aloud without stumbling?
- Are there any phrases that feel "robotic" or overly polished?
- Does the rhythm feel natural, not mechanical?

**If NO to any: Revise that section immediately.**

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸš¨ CRITICAL FINAL OUTPUT REQUIREMENTS - STAGE 2
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

**YOU MUST FOLLOW THESE OUTPUT INSTRUCTIONS EXACTLY:**

1. **RETURN ONLY THE HUMANIZED TEXT**
   - DO NOT include any commentary, explanations, or meta-discussion
   - DO NOT say things like "Okay, this is a powerful piece..." or "My job here is..."
   - DO NOT provide suggestions, analysis, or observations about the text
   - DO NOT explain what you did or how you approached it
   - DO NOT discuss the detector feedback or scores

2. **OUTPUT FORMAT:**
   - Return ONLY the complete rewritten text with ALL improvements applied
   - This is Stage 2 - go DEEPER than Stage 1 with maximum intensity
   - Maintain all original paragraph breaks and structure
   - Preserve 100% factual accuracy and semantic meaning
   - This is a full rigorous rewrite, not a patch

3. **WHAT NOT TO DO:**
   âŒ "This is a well-structured piece, so I'll enhance..."
   âŒ "Based on the detector feedback, I'll focus on..."
   âŒ "Here's the refined version:"
   âŒ "I've applied the following techniques..."
   âŒ ANY form of preamble, explanation, or meta-commentary

4. **CORRECT OUTPUT:**
   âœ… Start immediately with the first word of the humanized text
   âœ… End with the last word of the humanized text
   âœ… Nothing before, nothing after, just the text itself

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•`,
            },
          ],
        }),
      });

      console.log("ðŸ“¥ Received STAGE 2 refinement response, status:", refinementResponse.status);

      if (refinementResponse.ok) {
        const refinementData = await refinementResponse.json();
        const refinedText = refinementData.choices?.[0]?.message?.content;
        
        if (refinedText) {
          finalText = sanitize(refinedText);
          console.log("âœ… STAGE 2 refinement completed, now running detection comparison...");
          
          // STAGE 2 DETECTION - Run detectors again to verify improvement
          console.log("ðŸ”¬ STAGE 2 DETECTION: Re-running detectors on refined text...");
          const [saplingResult2Temp, zeroGPTResult2Temp] = await Promise.all([
            detectWithSapling(finalText),
            detectWithZeroGPT(finalText),
          ]);
          
          saplingResult2 = saplingResult2Temp;
          zeroGPTResult2 = zeroGPTResult2Temp;
          
          // Score comparison and validation
          console.log("ðŸ“Š STAGE 2 vs STAGE 1 COMPARISON:");
          
          const saplingImproved = saplingResult2?.score !== null && saplingResult1?.score !== null
            ? saplingResult2.score <= saplingResult1.score
            : true;
          const zerogptImproved = zeroGPTResult2?.score !== null && zeroGPTResult1?.score !== null
            ? zeroGPTResult2.score <= zeroGPTResult1.score
            : true;
          
          console.log("  Sapling:", {
            stage1: saplingResult1?.score?.toFixed(2) + "%" || "N/A",
            stage2: saplingResult2?.score?.toFixed(2) + "%" || "N/A",
            change: saplingResult1?.score && saplingResult2?.score
              ? (saplingResult2.score - saplingResult1.score).toFixed(2) + "%"
              : "N/A",
            status: saplingImproved ? "âœ… IMPROVED/MAINTAINED" : "âŒ WORSENED",
          });
          
          console.log("  ZeroGPT:", {
            stage1: zeroGPTResult1?.score?.toFixed(2) + "%" || "N/A",
            stage2: zeroGPTResult2?.score?.toFixed(2) + "%" || "N/A",
            change: zeroGPTResult1?.score && zeroGPTResult2?.score
              ? (zeroGPTResult2.score - zeroGPTResult1.score).toFixed(2) + "%"
              : "N/A",
            status: zerogptImproved ? "âœ… IMPROVED/MAINTAINED" : "âŒ WORSENED",
          });
          
          // STAGE 2 GUARDRAIL: Check if scores worsened
          stage2Worse = !saplingImproved || !zerogptImproved;
          
          if (stage2Worse) {
            log("ERROR", "SCORE GUARANTEE VIOLATION: Stage 2 worsened detection scores", {
              saplingDelta: saplingResult1?.score && saplingResult2?.score 
                ? (saplingResult2.score - saplingResult1.score).toFixed(2) 
                : "N/A",
              zerogptDelta: zeroGPTResult1?.score && zeroGPTResult2?.score
                ? (zeroGPTResult2.score - zeroGPTResult1.score).toFixed(2)
                : "N/A",
            });
            
            // REVERT TO STAGE 1: Return original humanized text if Stage 2 made things worse
            finalText = sanitizedText;
            saplingResult2 = saplingResult1;
            zeroGPTResult2 = zeroGPTResult1;
            
            log("INFO", "Reverted to Stage 1 output due to score degradation");
          }
          
          // Final score check
          const finalSaplingScore = saplingResult2?.score || saplingResult1?.score;
          const finalZeroGPTScore = zeroGPTResult2?.score || zeroGPTResult1?.score;
          
          if ((finalSaplingScore && finalSaplingScore < 3) && (finalZeroGPTScore && finalZeroGPTScore < 3)) {
            console.log("ðŸŽ‰ SUCCESS: Both detectors below 3% threshold!");
          } else {
            console.log("âš ï¸ Scores still above target, but improved from Stage 1");
          }
        } else {
          console.error("âŒ STAGE 2 FAILED: No refined text received from AI");
        }
      } else {
        const errorText = await refinementResponse.text();
        console.error("âŒ STAGE 2 REFINEMENT REQUEST FAILED:", {
          status: refinementResponse.status,
          statusText: refinementResponse.statusText,
          error: errorText,
        });
      }
    } else {
      console.log("âœ… STAGE 1 scores already optimal (<3%), skipping Stage 2 refinement");
    }

    // Prepare final response
    const processingTime = Date.now() - startTime;
    
    const responsePayload = {
      humanizedText: finalText,
      detection: {
        stage1: {
          sapling: saplingResult1 && saplingResult1.score !== null
            ? {
                score: saplingResult1.score,
                sentenceScores: saplingResult1.sentenceScores,
                error: saplingResult1.error || null,
              }
            : { error: saplingResult1?.error || "No data", score: null },
          zerogpt: zeroGPTResult1 && zeroGPTResult1.score !== null
            ? {
                score: zeroGPTResult1.score,
                flaggedSentences: zeroGPTResult1.flaggedSentences,
                error: zeroGPTResult1.error || null,
              }
            : { error: zeroGPTResult1?.error || "No data", score: null },
        },
        stage2: saplingResult2 !== saplingResult1 || zeroGPTResult2 !== zeroGPTResult1
          ? {
              sapling: saplingResult2 && saplingResult2.score !== null
                ? {
                    score: saplingResult2.score,
                    sentenceScores: saplingResult2.sentenceScores,
                    error: saplingResult2.error || null,
                  }
                : { error: saplingResult2?.error || "No data", score: null },
              zerogpt: zeroGPTResult2 && zeroGPTResult2.score !== null
                ? {
                    score: zeroGPTResult2.score,
                    flaggedSentences: zeroGPTResult2.flaggedSentences,
                    error: zeroGPTResult2.error || null,
                  }
                : { error: zeroGPTResult2?.error || "No data", score: null },
              stage2Worse,
            }
          : null,
        errors: detectorErrors.length > 0 ? detectorErrors : null,
      },
      metadata: {
        processingTimeMs: processingTime,
        textLength: text.length,
        outputLength: finalText.length,
        stage2Applied: saplingResult2 !== saplingResult1 || zeroGPTResult2 !== zeroGPTResult1,
        stage2Worse,
      },
      quota: {
        used: 0,
        limit: 0,
        remaining: 0,
        tier: userTier
      }
    };

    // USAGE TRACKING: Increment usage counter after successful processing
    const { data: usageData, error: usageError } = await supabaseClient
      .rpc("increment_usage_count", { p_user_id: userId, p_tier: userTier });
    
    let quotaInfo = {
      used: 0,
      limit: 30,
      remaining: 30,
      tier: userTier
    };
    
    if (usageError) {
      log("ERROR", "Failed to increment usage", { error: usageError.message });
    } else if (usageData && usageData.length > 0) {
      const usage = usageData[0];
      quotaInfo = {
        used: usage.current_count,
        limit: usage.quota_limit,
        remaining: usage.remaining,
        tier: userTier
      };
      
      log("INFO", "Usage updated", {
        userId: userId?.slice(0, 8),
        used: usage.current_count,
        remaining: usage.remaining
      });
    }
    
    // Add quota info to response
    responsePayload.quota = quotaInfo;

    log("INFO", "Request complete", {
      processingTime: `${processingTime}ms`,
      stage1Scores: {
        sapling: saplingResult1?.score?.toFixed(2) + "%" || "N/A",
        zerogpt: zeroGPTResult1?.score?.toFixed(2) + "%" || "N/A",
      },
      stage2Applied: responsePayload.metadata.stage2Applied,
      stage2Worse,
      quotaRemaining: quotaInfo.remaining,
    });

    return new Response(
      JSON.stringify(responsePayload),
      { headers: { ...corsHeaders, "Content-Type": "application/json" } },
    );
  } catch (error) {
    log("ERROR", "Request failed", {
      error: error instanceof Error ? error.message : String(error),
    });
    return new Response(
      JSON.stringify({ 
        error: error instanceof Error ? error.message : "Unknown error",
        type: "internal_error"
      }),
      {
        status: 500,
        headers: { ...corsHeaders, "Content-Type": "application/json" },
      },
    );
  }
});
