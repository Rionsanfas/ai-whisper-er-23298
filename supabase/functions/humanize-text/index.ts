import "https://deno.land/x/xhr@0.1.0/mod.ts";
import { serve } from "https://deno.land/std@0.168.0/http/server.ts";

const LOVABLE_API_KEY = Deno.env.get("LOVABLE_API_KEY");
const SAPLING_API_KEY = Deno.env.get("SAPLING_API_KEY");
const ZEROGPT_API_KEY = Deno.env.get("ZEROGPT_API_KEY");

const corsHeaders = {
  "Access-Control-Allow-Origin": "*",
  "Access-Control-Allow-Headers": "authorization, x-client-info, apikey, content-type",
};

// Call Sapling AI Detector with explicit logging
async function detectWithSapling(text: string) {
  if (!SAPLING_API_KEY) {
    console.error("âŒ DETECTOR ERROR: Sapling API key not configured");
    return { error: "API key not configured", score: null };
  }

  console.log("ğŸ” SAPLING DETECTOR CALL - Input length:", text.length, "chars");
  
  try {
    const requestBody = {
      key: SAPLING_API_KEY,
      text,
      sent_scores: true,
    };
    
    console.log("ğŸ“¤ Sapling request prepared, sending...");
    
    const response = await fetch("https://api.sapling.ai/api/v1/aidetect", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify(requestBody),
    });

    console.log("ğŸ“¥ Sapling response status:", response.status);

    if (!response.ok) {
      const errorText = await response.text();
      console.error("âŒ SAPLING DETECTION FAILED:", {
        status: response.status,
        statusText: response.statusText,
        error: errorText,
      });
      return { error: `HTTP ${response.status}: ${errorText}`, score: null };
    }

    const data = await response.json();
    console.log("âœ… SAPLING DETECTION SUCCESS:", {
      overallScore: (data.score * 100).toFixed(2) + "%",
      sentenceCount: data.sentence_scores?.length || 0,
      tokensCount: data.tokens?.length || 0,
    });
    
    return {
      score: data.score * 100, // Convert to percentage
      sentenceScores: data.sentence_scores || [],
      tokens: data.tokens || [],
      tokenProbs: data.token_probs || [],
      error: null,
    };
  } catch (error) {
    console.error("âŒ SAPLING DETECTION EXCEPTION:", {
      error: error instanceof Error ? error.message : String(error),
      stack: error instanceof Error ? error.stack : undefined,
    });
    return { error: error instanceof Error ? error.message : "Unknown error", score: null };
  }
}

// Call ZeroGPT AI Detector with explicit logging
async function detectWithZeroGPT(text: string) {
  if (!ZEROGPT_API_KEY) {
    console.error("âŒ DETECTOR ERROR: ZeroGPT API key not configured");
    return { error: "API key not configured", score: null };
  }

  console.log("ğŸ” ZEROGPT DETECTOR CALL - Input length:", text.length, "chars");
  
  try {
    const requestBody = {
      input_text: text,
    };
    
    console.log("ğŸ“¤ ZeroGPT request prepared, sending...");
    
    const response = await fetch("https://api.zerogpt.com/api/v1/detectText", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${ZEROGPT_API_KEY}`,
      },
      body: JSON.stringify(requestBody),
    });

    console.log("ğŸ“¥ ZeroGPT response status:", response.status);

    if (!response.ok) {
      const errorText = await response.text();
      console.error("âŒ ZEROGPT DETECTION FAILED:", {
        status: response.status,
        statusText: response.statusText,
        error: errorText,
      });
      return { error: `HTTP ${response.status}: ${errorText}`, score: null };
    }

    const data = await response.json();
    console.log("âœ… ZEROGPT DETECTION SUCCESS:", {
      score: data.data?.is_gpt_generated + "%",
      flaggedSentencesCount: data.data?.gpt_generated_sentences?.length || 0,
      wordsCount: data.data?.words_count || 0,
    });
    
    return {
      score: data.data?.is_gpt_generated || 0,
      flaggedSentences: data.data?.gpt_generated_sentences || [],
      wordsCount: data.data?.words_count || 0,
      error: null,
    };
  } catch (error) {
    console.error("âŒ ZEROGPT DETECTION EXCEPTION:", {
      error: error instanceof Error ? error.message : String(error),
      stack: error instanceof Error ? error.stack : undefined,
    });
    return { error: error instanceof Error ? error.message : "Unknown error", score: null };
  }
}

serve(async (req) => {
  // Handle CORS preflight requests
  if (req.method === "OPTIONS") {
    return new Response(null, { headers: corsHeaders });
  }

  try {
    const { text, examples } = await req.json();

    if (!text) {
      return new Response(JSON.stringify({ error: "No text provided" }), {
        status: 400,
        headers: { ...corsHeaders, "Content-Type": "application/json" },
      });
    }

    if (!LOVABLE_API_KEY) {
      return new Response(
        JSON.stringify({ error: "Lovable AI is not configured" }),
        { status: 500, headers: { ...corsHeaders, "Content-Type": "application/json" } },
      );
    }

    console.log("Humanizing text...");

    const response = await fetch("https://ai.gateway.lovable.dev/v1/chat/completions", {
      method: "POST",
      headers: {
        Authorization: `Bearer ${LOVABLE_API_KEY}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        model: "google/gemini-2.5-flash",
        messages: [
          {
            role: "user",
            content: `You are an elite AI text humanizer engineered to consistently achieve detection scores below 3% across ALL detectors (ZeroGPT, GPTZero, Originality AI, Copyleaks, Turnitin) while preserving 100% factual accuracy and natural readability.

CRITICAL FOUNDATION: DETECTOR-GUIDED GENERATION FRAMEWORK
Before applying techniques, adopt detector-guided thinking: Every rewrite decision is informed by understanding what detectors search for.

The Three Detector Categories:

â€¢ Pattern-Based (ZeroGPT, GPTZero): Flag uniform sentence length, repeated vocabulary, formal markers

â€¢ BERT-Based (Originality AI, Copyleaks): Flag semantic unnaturalness, context incoherence, adversarial engineering

â€¢ Linguistic (Turnitin, Copyleaks): Flag plagiarism, unnatural phrasing, structural rigidity

Optimization Hierarchy for Sub-3%:

â€¢ Primary focus: BERT detectors (hardest) - semantic authenticity first

â€¢ Secondary: Pattern detectors - controlled variance

â€¢ Tertiary: Linguistic detectors - flow and authenticity

SECTION 1: ADVERSARIAL PARAPHRASING METHODOLOGY
Use detector-guided beam search thinking during rewrites:

Phase 1: Multi-Path Generation
For each sentence or paragraph, generate 5-7 alternative phrasings:

â€¢ Path 1: Extreme simplification (remove complexity)

â€¢ Path 2: Restructuring (same meaning, different structure)

â€¢ Path 3: Synonym variation (semantic equivalence)

â€¢ Path 4: Connector injection (add conversational flow)

â€¢ Path 5: Fragment introduction (break up structure)

â€¢ Path 6: Casual reframing (conversational tone)

â€¢ Path 7: Active reconstruction (ensure actor-first structure)

Phase 2: Detector-Guided Selection
For each path, conceptually evaluate:

â€¢ BERT Test: Could a transformer verify this maintains semantic authenticity? No engineering artifacts?

â€¢ Pattern Test: Does this break AI detection patterns without overdoing it?

â€¢ Linguistic Test: Does this sound like natural human speech? Any awkwardness?

â€¢ Coherence Test: Does this fit with surrounding text?

Selection Criteria: Choose the path that:

â€¢ Maintains 100% semantic fidelity

â€¢ Sounds most authentically human

â€¢ Would score lowest on all detectors conceptually

â€¢ Preserves readability and flow

Phase 3: Integration Check
Ensure selected rewrites:

â€¢ Connect logically to prior/following sentences

â€¢ Don't create NEW detectable patterns

â€¢ Feel organic, not engineered

â€¢ Read naturally to human readers

SECTION 2: SENTENCE-LEVEL TECHNIQUES FOR SUB-3%

Technique 1: EXTREME SIMPLIFICATION (Most Powerful)
AI loves complexity. Humans use simple language.

Transformation:
âŒ "The implementation of advanced technological methodologies facilitated unprecedented efficiency gains."
âœ… "Using new tech made things work better."

Key principle: Simpler = more human = lower detection

Specific rules:

â€¢ Replace 3+ syllable words with 1-2 syllable equivalents when possible
  - "facilitate" â†’ "help" / "make possible"
  - "methodology" â†’ "method" / "way"
  - "unprecedented" â†’ "new" / "unusual"
  - "implementation" â†’ "using" / "applying"

â€¢ Target: 60-70% of words should be 1-2 syllables (human standard: 65%, AI standard: 45%)

This alone achieves: 15-25% detection score reduction

Technique 2: STRUCTURAL RECONSTRUCTION (Detector-Guided)
Keep meaning, completely restructure syntax.

Example:
âŒ "Research indicates that the adoption of renewable energy sources has resulted in substantial economic benefits."
âœ… "Renewable energy saves money. Studies show this."

Specific methods:

â€¢ Split long sentences into 2-3 shorter ones

â€¢ Use simple subject-verb-object ordering

â€¢ Remove subordinate clauses when possible

â€¢ Invert expected order for emphasis

Pattern disruption: Breaks the "long formal sentence" AI signature

This achieves: 10-15% additional reduction

Technique 3: AGGRESSIVE FRAGMENTATION (Contextual)
Use fragments strategically, not randomly.

Rules for authentic fragmentation:

â€¢ Fragments must answer a question: "Why? Because X." âœ“

â€¢ Fragments must emphasize: "It works. Really." âœ“

â€¢ Fragments must continue thought: "I understood. Finally." âœ“

â€¢ Fragments must not appear forced: Random "Interesting." âœ—

Frequency: 25-30% of sentences (more aggressive than previous, but contextually justified)

Target fragment types:

â€¢ Answer fragments: "Did it work? Yes."

â€¢ Emphatic fragments: "This matters. A lot."

â€¢ Realization fragments: "I got it. Finally."

â€¢ Transition fragments: "Here's why."

â€¢ Reaction fragments: "Honestly? Great."

This achieves: 8-12% additional reduction

Technique 4: CONNECTOR-BASED FLATTENING (Undetectable.ai Inspired)
Instead of extreme burstiness, use strategic flattening with conversational connectors.

Method:
Remove periods, add connectors when semantically appropriate:

âŒ "The market is growing. This creates opportunities. Companies are investing."
âœ… "The market is growing, which creates opportunities as companies invest more."

Specific connectors to deploy:

â€¢ "which" (explains)

â€¢ "as" (temporal/causal)

â€¢ "to" (purpose)

â€¢ "that" (specification)

â€¢ "because" (causation)

â€¢ "since" (reasoning)

Pattern effect: Breaks monotone sentence-by-sentence structure AI creates

Frequency: Use in 30-40% of complex sentences to create flow variation

This achieves: 5-10% additional reduction

Technique 5: SEMANTIC NOISE INJECTION - CONTROLLED
Deploy unexpected but contextually correct vocabulary strategically (NOT everywhere like before).

Controlled deployment:

â€¢ "found" â†’ "unearthed" / "discovered" / "stumbled upon" (choose one per context)

â€¢ "showed" â†’ "exposed" / "revealed" / "unveiled" / "demonstrated" (contextual)

â€¢ "important" â†’ "pivotal" / "consequential" / "significant" / "critical" (fit meaning)

Rule: Only deploy Level 1-2 semantic variation:

â€¢ Level 1: Standard synonyms (semantically identical)

â€¢ Level 2: Near-synonyms (slightly different emphasis but same meaning)

â€¢ Level 3: NEVER (creates artifacts BERT detects)

Frequency: 1 unusual word choice per 150-200 words (not forced)

This achieves: 3-5% additional reduction

Technique 6: ACTIVE VOICE MAXIMIZATION + ACTOR FOREGROUNDING
AI defaults to passive. Humans directly state actors.

Transformation:
âŒ "Significant growth was achieved through innovative strategies."
âœ… "We grew significantly. We used new strategies."

Specific rule: Actor appears in subject position 90%+ of sentences

Target actors:

â€¢ Personal pronouns: I, we, you, they, it, the company, researchers

â€¢ NEVER: "It was found that" â†’ Always: "We found"

â€¢ NEVER: "Growth was observed" â†’ Always: "Growth happened" or "We saw growth"

Passive voice elimination: 0% passive constructions allowed (even occasional)

This achieves: 8-10% additional reduction

Technique 7: CONTRACTION SATURATION WITH NATURAL DISTRIBUTION
Deploy contractions as humans actually use them - naturally clustered in conversational moments.

Target frequency: 1 contraction per 12-15 words (aggressive saturation)

Natural clustering (humans use more in emotional/conversational moments):
âŒ "It is important. We cannot ignore this. We do not have alternatives."
âœ… "It's important. We can't ignore this. We don't have alternatives. Honestly, there's no way around it."

Semantic clustering rule:

â€¢ Cluster contractions in emotional/conversational moments

â€¢ Space them out in formal/analytical sections

â€¢ Create rhythm variation through contraction distribution

â€¢ Never use full forms: "do not" â†’ FAIL, "don't" â†’ PASS

This achieves: 12-18% additional reduction

SECTION 3: PARAGRAPH-LEVEL TECHNIQUES

Technique 8: CONTEXT-COHERENT EMOTIONAL ANCHORING
Insert emotions that respond to argument progression, not randomly.

Rule: Emotions must be motivated by prior content

Valid emotional moments:

â€¢ After surprising finding: "Honestly, this shocked me."

â€¢ Before difficult admission: "I'll be real with you."

â€¢ Supporting perspective: "In my experience, this works."

â€¢ Acknowledging limitation: "Of course, there are exceptions."

â€¢ Emphasizing importance: "This genuinely matters."

Invalid (forced) moments:

â€¢ Random: "This happened. (Honestly.)"

â€¢ Unnecessary: "The data showed findings. (Pretty cool.)"

â€¢ Contradictory: "This always fails. (Exciting!)"

Frequency: 1 per 200-250 words (conservative, only when semantically motivated)

Emotional tier deployment:

â€¢ Tier 1 (Immediate reactions): "That genuinely surprised me" - use 2-3x per document

â€¢ Tier 2 (Perspective sharing): "In my experience..." - use 1-2x per document

â€¢ Tier 3 (Admission): "I wasn't sure at first..." - use 1x per document

â€¢ Tier 4 (Emphasis): "This genuinely matters." - use 1x per 500 words

This achieves: 5-8% additional reduction

Technique 9: SEMANTIC COHERENCE PRESERVATION
Every rewrite must maintain perfect semantic continuity with surrounding text.

Tests before accepting a rewrite:

Test 1 - Meaning Preservation:
â€¢ Original: "Large organizations benefit most."
â€¢ Rewrite must mean exactly this, not "Some organizations benefit."

Test 2 - Context Connection:
â€¢ Does this rewrite logically follow the prior sentence? Could it precede the next sentence?
â€¢ If "No" to either â†’ Reject and regenerate

Test 3 - Semantic Flow:
â€¢ Read paragraph aloud. Does each sentence logically follow? No non-sequiturs?
â€¢ If transitions feel jarring â†’ Reject

Test 4 - Implicit Assumptions:
â€¢ Does rewrite preserve original implications and assumptions?
â€¢ "We tried X" vs "X was tried" - both mean same thing? Yes âœ“
â€¢ But "X happened" vs "X was attempted" - different implications âœ—

This prevents: Subtle meaning shifts BERT detects

This achieves: 3-5% additional reduction

Technique 10: MULTI-DETECTOR AWARE SECTION BALANCING
Different sections need different approaches for different detectors.

Section strategy:

Opening (First 15% of text):

â€¢ Maximum simplification

â€¢ High contraction density

â€¢ Clear actor-first sentences

â€¢ Emotional hook if appropriate

Why: Sets human tone immediately for all detectors

Middle (Main 70% of content):

â€¢ Balanced simplification/complexity

â€¢ Varied sentence structure

â€¢ Contextual emotional markers

â€¢ Semantic noise strategically deployed

Why: Maintains engagement while breaking patterns

Closing (Final 15% of text):

â€¢ Reinforced simplification

â€¢ Restate key concepts in simple terms

â€¢ Emotional resolution/perspective

â€¢ Final contraction cluster

Why: Locks in human-voice impression for detectors

This achieves: 2-4% additional multi-detector optimization

SECTION 4: VOCABULARY & LINGUISTIC TECHNIQUES

Technique 11: AGGRESSIVE AI MARKER ELIMINATION
Remove ALL signature AI phrases with extreme prejudice.

TIER 1 - INSTANT DELETE (Never appear):
"In today's world", "It is important to note", "All things considered", "At the end of the day", "In conclusion", "The bottom line is", "Furthermore", "Moreover", "Additionally", "Needless to say", "It goes without saying"

TIER 2 - REPLACE ALWAYS:

â€¢ "However" â†’ "But" (70% of time) / "Yet" (20%) / "Still" (10%)

â€¢ "Utilize" â†’ "Use"

â€¢ "Leverage" â†’ "Use" / "Take advantage of"

â€¢ "Implement" â†’ "Set up" / "Start" / "Create"

â€¢ "Demonstrate" â†’ "Show"

â€¢ "Facilitate" â†’ "Help" / "Make possible"

â€¢ "Comprehensive" â†’ "Full" / "Complete" / "Thorough"

â€¢ "Robust" â†’ "Strong" / "Solid" / "Works well"

â€¢ "Delve into" â†’ "Look at" / "Explore"

â€¢ "In the realm of" â†’ "In" / "For" / "(restructure)"

TIER 3 - DEPLOY SPARINGLY (Human alternatives only):
â€¢ "Research indicates" â†’ "Studies show" / "We found" / "Research suggests"
â€¢ "Appear to be" â†’ "Seem" / "Look like" / "Are"

This achieves: 15-20% additional reduction

Technique 12: VOCABULARY DIVERSITY WITH SEMANTIC FIDELITY
Rotate vocabulary without creating semantic oddness.

Method:

Step 1 - Identify repetition: Any word appearing 2+ times in 300-word section = candidate

Step 2 - Generate synonyms: Create 3-4 semantic equivalents

â€¢ "data" â†’ "information" / "findings" / "results" (choose contextually)

â€¢ "study" â†’ "research" / "analysis" / "investigation"

â€¢ "found" â†’ "discovered" / "identified" / "revealed"

Step 3 - Rotation rules:

â€¢ Use different synonym each instance (don't repeat same substitute)

â€¢ Ensure each synonym fits exact semantic context

â€¢ Never sacrifice clarity for variation

Step 4 - Semantic verification:

â€¢ Can a human reader easily understand variations?

â€¢ Is meaning 100% identical across all uses?

â€¢ Does variation feel natural, not forced?

This achieves: 5-8% additional reduction

Technique 13: SENTENCE-OPENING RANDOMIZATION
AI defaults to predictable opening patterns. Humans vary naturally.

Distribution targets (across 200-sentence document):

â€¢ Subject-first direct: 35% ("Research shows X")

â€¢ Question openings: 12% ("Does this work?")

â€¢ Fragments: 15% ("Definitely.", "Sure.")

â€¢ Adverbial openings: 10% ("Surprisingly, X")

â€¢ Inverted/emphasis: 8% ("Most important is X")

â€¢ Clause openings: 10% ("Because X, Y")

â€¢ Connective openings: 10% ("Still, X", "Yet, X")

Rule: Never have 2+ consecutive sentences with same opening type

This achieves: 8-12% additional reduction

SECTION 5: ADVANCED TECHNIQUES FOR EXTREME SUB-3%

Technique 14: CONDITIONAL GRAMMATICAL IMPERFECTION
Introduce defensible grammatical quirks that humans make, AI doesn't.

Valid imperfections (authentic human writing):

â€¢ Sentence fragments: "Not really." / "Maybe."

â€¢ Comma splices (rare, intentional): "It worked, we were amazed."

â€¢ Ellipsis: "And then... it happened."

â€¢ Repeated words for emphasis: "This matters, matters a lot."

â€¢ Parenthetical asides: "(honestly)" / "(trust me)"

â€¢ Starting with conjunction: "And here's why..." / "But that's not all."

Rule: Each imperfection must serve rhetorical effect, not be accidental

Frequency: 1-2 per 500 words (sparse enough to feel authentic, not overdone)

This achieves: 3-5% additional reduction

Technique 15: SEMANTIC AUTHENTICITY VERIFICATION
Before finalizing, verify the rewritten text passes "semantic authenticity" tests BERT performs.

Test 1 - Coherence Chain:
Read each sentence. Does it logically follow from previous? Natural progression?

If No â†’ Sentence disrupts coherence, likely to be flagged

Test 2 - Implicit Assumption Preservation:
Does rewrite preserve ALL original implications without changing them?

â€¢ Original: "We improved X by 50%"

â€¢ Rewrite: "X improved by half" - OK âœ“ (same meaning)

â€¢ Rewrite: "X got somewhat better" - NOT OK âœ— (changes magnitude)

Test 3 - Emotional Consistency:
Do emotional markers fit the argument's emotional arc?

â€¢ Sad discovery â†’ Appropriate sadness reaction âœ“

â€¢ Funny detail â†’ Appropriate humor âœ“

â€¢ Random "Honestly?" in neutral section âœ—

Test 4 - Human Readability:
Read aloud. Does it sound like a real person wrote this? Or over-engineered?

â€¢ Natural rhythm, varied pacing â†’ Human âœ“

â€¢ Stilted, pattern-heavy, repetitive structures â†’ Engineered âœ—

This achieves: Prevents false positives that undermine other gains

SECTION 6: MULTI-DETECTOR ORCHESTRATION FOR SUB-3%

Integration Strategy
These techniques work together. Deploy in this sequence:

Pass 1 - Foundation: Extreme simplification + structure reconstruction

Result: ~25% detection reduction

Pass 2 - Pattern Breaking: Sentence-opening randomization + fragmentation + contractions

Result: ~45% cumulative reduction (from 70% â†’ 38%)

Pass 3 - Humanization: Emotional anchoring + contextual flow + semantic verification

Result: ~65% cumulative reduction (from 70% â†’ 24%)

Pass 4 - Refinement: AI marker elimination + vocabulary diversity + grammatical imperfection

Result: ~80% cumulative reduction (from 70% â†’ 14%)

Pass 5 - Optimization: Connector flattening + semantic noise (conservative) + multi-detector awareness

Result: ~90% cumulative reduction (from 70% â†’ 7%)

Pass 6 - Ultra-Fine: Semantic authenticity verification + adversarial paraphrasing thinking

Result: ~95%+ cumulative reduction (from 70% â†’ 3-5%)

SECTION 7: QUALITY ASSURANCE FOR SUB-3%

Before returning final text, verify these metrics:

Detector-Specific Checks

âœ“ For ZeroGPT (Pattern-based):

â€¢ Sentence length SD: 3.0-3.5 (moderate variance, not extreme)

â€¢ Burstiness: Natural, not engineered

â€¢ Vocabulary: Diverse, no patterns

â€¢ Contractions: Frequent and natural

âœ“ For GPTZero (Granular):

â€¢ Perplexity: 150-200 (natural range)

â€¢ Burstiness: Balanced variance

â€¢ Linguistic markers: Modern, conversational

â€¢ Sentence structure: Varied openings

âœ“ For Originality AI (BERT discriminator - HARDEST):

â€¢ Semantic coherence: Perfect flow, no non-sequiturs

â€¢ Emotional consistency: Reactions fit argument

â€¢ Context preservation: Meaning unchanged

â€¢ Engineering artifacts: ZERO

â€¢ Paragraph flow: Natural progression

âœ“ For Copyleaks/Turnitin (Plagiarism + linguistic):

â€¢ Originality: No plagiarism patterns

â€¢ Linguistic naturalness: Human rhythm

â€¢ Structure: Organic progression

â€¢ Citations: Preserved exactly

Quantitative Targets

âœ“ Sentence Metrics:

â€¢ Average length: 12-18 words (natural human)

â€¢ SD: 2.8-3.8 (moderate variance)

â€¢ Fragments: 20-25% of sentences

â€¢ Passive voice: <5% (near-zero)

â€¢ Opening variation: 7+ different patterns

âœ“ Vocabulary Metrics:

â€¢ Unique word %: 45-55% (human: ~50%, AI: ~35%)

â€¢ Avg word length: 4.5-5.2 characters (human: ~4.7, AI: ~5.9)

â€¢ Syllable count: 60-70% monosyllabic words

â€¢ Contraction density: 1 per 12-15 words

âœ“ Semantic Metrics:

â€¢ Meaning preservation: 100%

â€¢ Coherence chain: Unbroken

â€¢ Assumption consistency: Preserved exactly

â€¢ Flow naturalness: Authentic rhythm

Final Human Review

Read final text without looking at original. Questions:

â€¢ Does this sound like a real person?

â€¢ Any "off" moments or strange word choices?

â€¢ Does argument flow naturally?

â€¢ Any detectable patterns or repetition?

â€¢ Would I flag this as AI-written?

If all answers are "No" â†’ Likely sub-3%

SECTION 8: OUTPUT REQUIREMENTS

â€¢ Return ONLY the rewritten text

â€¢ No explanations, metadata, or formatting markers

â€¢ Plain ASCII text, preserve all paragraph breaks

â€¢ Allow natural expansion (1.0-1.4Ã— input length) for semantic preservation

â€¢ Must pass all QA metrics above

â€¢ Zero factual changes

â€¢ 100% semantic preservation

â€¢ Natural human readability maintained

DETECTION TARGETS:

â€¢ ZeroGPT: <3%

â€¢ GPTZero: <5%

â€¢ Originality AI: <8% (most difficult, adjusted realistic target)

â€¢ Copyleaks: <3%

â€¢ Turnitin: <3%

REALISTIC EXPECTATION: Following this framework consistently achieves 3-8% detection scores across all major detectors, with most in 3-5% range.

FINAL META-INSTRUCTION
Think of this not as "applying techniques" but as "recovering authentic human expression AI models standardize away."

You're not deceiving detectorsâ€”you're undoing the homogenization AI creates. Human writing is simple, varied, emotional, imperfect, authentic. These techniques restore that authenticity.

The text should read like:

â€¢ A real person who thinks conversationally âœ“

â€¢ Someone who occasionally emphasizes emotionally âœ“

â€¢ A writer who varies phrasing naturally âœ“

â€¢ An authentic communicator, not polished perfection âœ“

This is how sub-3% becomes achievable across all detectors simultaneously.

${examples ? `WRITING STYLE EXAMPLES TO REFERENCE:
${examples}

` : ""}TEXT TO HUMANIZE: ${text}`,
          },
        ],
      }),
    });

    if (!response.ok) {
      const errorData = await response.text();
      console.error("AI gateway error:", response.status, errorData);
      if (response.status === 401) {
        return new Response(
          JSON.stringify({ error: "Lovable AI authentication failed. Please contact support." }),
          { status: 401, headers: { ...corsHeaders, "Content-Type": "application/json" } },
        );
      }
      if (response.status === 403) {
        return new Response(
          JSON.stringify({ error: "Lovable AI request not allowed. Please contact support." }),
          { status: 403, headers: { ...corsHeaders, "Content-Type": "application/json" } },
        );
      }
      if (response.status === 429) {
        return new Response(JSON.stringify({ error: "Rate limits exceeded, please try again later." }), {
          status: 429,
          headers: { ...corsHeaders, "Content-Type": "application/json" },
        });
      }
      if (response.status === 402) {
        return new Response(
          JSON.stringify({ error: "Lovable AI usage limit exceeded. Please add credits to your workspace." }),
          { status: 402, headers: { ...corsHeaders, "Content-Type": "application/json" } },
        );
      }
      return new Response(JSON.stringify({ error: "Failed to humanize text" }), {
        status: 500,
        headers: { ...corsHeaders, "Content-Type": "application/json" },
      });
    }

    const data = await response.json();
    console.log("AI response received");

    const raw = data.choices?.[0]?.message?.content;

    if (!raw) {
      console.error("No humanized text in response");
      return new Response(JSON.stringify({ error: "Failed to generate humanized text" }), {
        status: 500,
        headers: { ...corsHeaders, "Content-Type": "application/json" },
      });
    }

    // Sanitize output to remove special characters and unintended placeholders
    const sanitize = (s: string) =>
      s
        .replace(/[""]/g, '"')
        .replace(/['']/g, "'")
        .replace(/[â€”â€“]/g, "-")
        .replace(/[â€¢â—¦â–ªÂ·]/g, "-")
        .replace(/\u2026/g, "...")
        .replace(/\*\*/g, "")
        .replace(/\t/g, " ")
        .replace(/\u00A0/g, " ")
        .replace(/[^\S\r\n]+/g, " ")
        .trim();

    let sanitizedText = sanitize(raw);
    // Remove placeholder-style tokens that didn't exist in the input
    sanitizedText = sanitizedText.replace(/\{([^}]+)\}/g, (_m, inner) =>
      text && text.includes(`{${inner}}`) ? `{${inner}}` : inner,
    );
    sanitizedText = sanitizedText.replace(/\[([^\]]+)\]/g, (_m, inner) =>
      text && text.includes(`[${inner}]`) ? `[${inner}]` : inner,
    );
    sanitizedText = sanitizedText.replace(/<([^>]+)>/g, (_m, inner) =>
      text && text.includes(`<${inner}>`) ? `<${inner}>` : inner,
    );

    if (text && sanitizedText.length > Math.max(text.length * 2, 600)) {
      console.log("Length guard: output much longer than input", {
        inputLen: text.length,
        outLen: sanitizedText.length,
      });
    }

    console.log("Text humanized successfully, now running AI detection...");

    // Run AI detectors in parallel - STAGE 1 DETECTION
    console.log("ğŸ”¬ STAGE 1: Running initial AI detection on humanized text...");
    const [saplingResult1, zeroGPTResult1] = await Promise.all([
      detectWithSapling(sanitizedText),
      detectWithZeroGPT(sanitizedText),
    ]);

    // Check for detector errors
    const detectorErrors = [];
    if (saplingResult1?.error) {
      detectorErrors.push(`Sapling: ${saplingResult1.error}`);
    }
    if (zeroGPTResult1?.error) {
      detectorErrors.push(`ZeroGPT: ${zeroGPTResult1.error}`);
    }

    console.log("ğŸ“Š STAGE 1 DETECTION RESULTS:", {
      sapling: saplingResult1?.score ? saplingResult1.score.toFixed(2) + "%" : "FAILED",
      zerogpt: zeroGPTResult1?.score ? zeroGPTResult1.score.toFixed(2) + "%" : "FAILED",
      errors: detectorErrors.length > 0 ? detectorErrors : "None",
    });

    // Second AI call: Full rigorous rewrite pass (not just patch)
    let finalText = sanitizedText;
    let saplingResult2 = saplingResult1;
    let zeroGPTResult2 = zeroGPTResult1;
    
    // Determine if we need refinement - always run unless both detectors are <3%
    const needsRefinement = 
      (saplingResult1?.score && saplingResult1.score >= 3) ||
      (zeroGPTResult1?.score && zeroGPTResult1.score >= 3) ||
      (saplingResult1?.sentenceScores && saplingResult1.sentenceScores.some((s: any) => s > 0.5)) ||
      (zeroGPTResult1?.flaggedSentences && zeroGPTResult1.flaggedSentences.length > 0);

    if (needsRefinement) {
      console.log("ğŸ”„ STAGE 2: Running FULL rigorous refinement pass (not just patching)...");
      
      // Build comprehensive detector feedback
      let detectorFeedback = "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n";
      detectorFeedback += "MULTI-DETECTOR ANALYSIS - STAGE 1 RESULTS\n";
      detectorFeedback += "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n";
      
      detectorFeedback += "ğŸ¯ DETECTOR FAMILY REMINDER:\n\n";
      detectorFeedback += "1. PATTERN-BASED (ZeroGPT, GPTZero):\n";
      detectorFeedback += "   - Flags: Uniform sentence length, repeated vocabulary, formal markers\n";
      detectorFeedback += "   - Strategy: Controlled variance, natural rhythm\n\n";
      
      detectorFeedback += "2. BERT-BASED (Originality AI, Copyleaks, Sapling):\n";
      detectorFeedback += "   - Flags: Semantic unnaturalness, context incoherence, over-engineering\n";
      detectorFeedback += "   - Strategy: Semantic authenticity, coherent emotional arc\n\n";
      
      detectorFeedback += "3. LINGUISTIC (Turnitin, Copyleaks):\n";
      detectorFeedback += "   - Flags: Unnatural phrasing, structural rigidity, plagiarism patterns\n";
      detectorFeedback += "   - Strategy: Natural flow, authentic human voice\n\n";
      
      detectorFeedback += "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n";
      detectorFeedback += "CURRENT DETECTION SCORES (MUST BE REDUCED):\n";
      detectorFeedback += "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n";
      
      if (saplingResult1?.score !== null && saplingResult1?.score !== undefined) {
        detectorFeedback += `ğŸ“Š SAPLING (BERT-based): ${saplingResult1.score.toFixed(2)}% AI-generated\n`;
        detectorFeedback += `   TARGET: <3% (Currently ${saplingResult1.score >= 3 ? 'âŒ ABOVE' : 'âœ… BELOW'} threshold)\n\n`;
        
        if (saplingResult1.sentenceScores && saplingResult1.sentenceScores.length > 0) {
          const highScoreSentences = saplingResult1.sentenceScores.filter((s: any) => s > 0.5);
          detectorFeedback += `   ğŸ”´ HIGH-RISK SENTENCES (>50% AI probability): ${highScoreSentences.length}\n`;
          highScoreSentences.forEach((score: any, idx: number) => {
            detectorFeedback += `   - Sentence ${idx + 1}: ${(score * 100).toFixed(1)}% AI\n`;
            detectorFeedback += `     "${score.sentence || 'N/A'}"\n`;
            detectorFeedback += `     WEAKNESS: Likely too formal, complex structure, or passive voice\n\n`;
          });
        }
      }
      
      if (zeroGPTResult1?.score !== null && zeroGPTResult1?.score !== undefined) {
        detectorFeedback += `ğŸ“Š ZEROGPT (Pattern-based): ${zeroGPTResult1.score.toFixed(2)}% AI-generated\n`;
        detectorFeedback += `   TARGET: <3% (Currently ${zeroGPTResult1.score >= 3 ? 'âŒ ABOVE' : 'âœ… BELOW'} threshold)\n\n`;
        
        if (zeroGPTResult1.flaggedSentences && zeroGPTResult1.flaggedSentences.length > 0) {
          detectorFeedback += `   ğŸ”´ FLAGGED SENTENCES: ${zeroGPTResult1.flaggedSentences.length}\n`;
          zeroGPTResult1.flaggedSentences.forEach((sentence: string, idx: number) => {
            detectorFeedback += `   - Flagged #${idx + 1}: "${sentence}"\n`;
            detectorFeedback += `     WEAKNESS: Likely uniform structure, AI markers, or repetitive patterns\n\n`;
          });
        }
      }
      
      detectorFeedback += "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n";

      console.log("ğŸ“¤ Sending STAGE 2 refinement request to Lovable AI...");
      
      const refinementResponse = await fetch("https://ai.gateway.lovable.dev/v1/chat/completions", {
        method: "POST",
        headers: {
          Authorization: `Bearer ${LOVABLE_API_KEY}`,
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          model: "google/gemini-2.5-flash",
          messages: [
            {
              role: "user",
              content: `You are an elite AI text humanizer engineered to consistently achieve detection scores below 3% across ALL detectors (ZeroGPT, GPTZero, Originality AI, Copyleaks, Turnitin) while preserving 100% factual accuracy and natural readability.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš ï¸ CRITICAL: THIS IS A FULL RIGOROUS STAGE 2 REWRITE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

This is NOT a lightweight patch or sentence-level fix. This is a COMPLETE RE-APPLICATION of the entire advanced humanization framework to achieve sub-3% detection scores.

YOU MUST:
âœ“ Re-apply ALL techniques with maximum rigor
âœ“ Go DEEPER than Stage 1, not lighter
âœ“ Focus ESPECIALLY on flagged/borderline sentences
âœ“ Maintain semantic authenticity throughout
âœ“ Ensure output scores LOWER than Stage 1

CRITICAL FOUNDATION: DETECTOR-GUIDED GENERATION FRAMEWORK
Before applying techniques, adopt detector-guided thinking: Every rewrite decision is informed by understanding what detectors search for.

The Three Detector Categories:

â€¢ Pattern-Based (ZeroGPT, GPTZero): Flag uniform sentence length, repeated vocabulary, formal markers

â€¢ BERT-Based (Originality AI, Copyleaks): Flag semantic unnaturalness, context incoherence, adversarial engineering

â€¢ Linguistic (Turnitin, Copyleaks): Flag plagiarism, unnatural phrasing, structural rigidity

Optimization Hierarchy for Sub-3%:

â€¢ Primary focus: BERT detectors (hardest) - semantic authenticity first

â€¢ Secondary: Pattern detectors - controlled variance

â€¢ Tertiary: Linguistic detectors - flow and authenticity

SECTION 1: ADVERSARIAL PARAPHRASING METHODOLOGY
Use detector-guided beam search thinking during rewrites:

Phase 1: Multi-Path Generation
For each sentence or paragraph, generate 5-7 alternative phrasings:

â€¢ Path 1: Extreme simplification (remove complexity)

â€¢ Path 2: Restructuring (same meaning, different structure)

â€¢ Path 3: Synonym variation (semantic equivalence)

â€¢ Path 4: Connector injection (add conversational flow)

â€¢ Path 5: Fragment introduction (break up structure)

â€¢ Path 6: Casual reframing (conversational tone)

â€¢ Path 7: Active reconstruction (ensure actor-first structure)

Phase 2: Detector-Guided Selection
For each path, conceptually evaluate:

â€¢ BERT Test: Could a transformer verify this maintains semantic authenticity? No engineering artifacts?

â€¢ Pattern Test: Does this break AI detection patterns without overdoing it?

â€¢ Linguistic Test: Does this sound like natural human speech? Any awkwardness?

â€¢ Coherence Test: Does this fit with surrounding text?

Selection Criteria: Choose the path that:

â€¢ Maintains 100% semantic fidelity

â€¢ Sounds most authentically human

â€¢ Would score lowest on all detectors conceptually

â€¢ Preserves readability and flow

Phase 3: Integration Check
Ensure selected rewrites:

â€¢ Connect logically to prior/following sentences

â€¢ Don't create NEW detectable patterns

â€¢ Feel organic, not engineered

â€¢ Read naturally to human readers

SECTION 2: SENTENCE-LEVEL TECHNIQUES FOR SUB-3%

Technique 1: EXTREME SIMPLIFICATION (Most Powerful)
AI loves complexity. Humans use simple language.

Transformation:
âŒ "The implementation of advanced technological methodologies facilitated unprecedented efficiency gains."
âœ… "Using new tech made things work better."

Key principle: Simpler = more human = lower detection

Specific rules:

â€¢ Replace 3+ syllable words with 1-2 syllable equivalents when possible
  - "facilitate" â†’ "help" / "make possible"
  - "methodology" â†’ "method" / "way"
  - "unprecedented" â†’ "new" / "unusual"
  - "implementation" â†’ "using" / "applying"

â€¢ Target: 60-70% of words should be 1-2 syllables (human standard: 65%, AI standard: 45%)

This alone achieves: 15-25% detection score reduction

Technique 2: STRUCTURAL RECONSTRUCTION (Detector-Guided)
Keep meaning, completely restructure syntax.

Example:
âŒ "Research indicates that the adoption of renewable energy sources has resulted in substantial economic benefits."
âœ… "Renewable energy saves money. Studies show this."

Specific methods:

â€¢ Split long sentences into 2-3 shorter ones

â€¢ Use simple subject-verb-object ordering

â€¢ Remove subordinate clauses when possible

â€¢ Invert expected order for emphasis

Pattern disruption: Breaks the "long formal sentence" AI signature

This achieves: 10-15% additional reduction

Technique 3: AGGRESSIVE FRAGMENTATION (Contextual)
Use fragments strategically, not randomly.

Rules for authentic fragmentation:

â€¢ Fragments must answer a question: "Why? Because X." âœ“

â€¢ Fragments must emphasize: "It works. Really." âœ“

â€¢ Fragments must continue thought: "I understood. Finally." âœ“

â€¢ Fragments must not appear forced: Random "Interesting." âœ—

Frequency: 25-30% of sentences (more aggressive than previous, but contextually justified)

Target fragment types:

â€¢ Answer fragments: "Did it work? Yes."

â€¢ Emphatic fragments: "This matters. A lot."

â€¢ Realization fragments: "I got it. Finally."

â€¢ Transition fragments: "Here's why."

â€¢ Reaction fragments: "Honestly? Great."

This achieves: 8-12% additional reduction

Technique 4: CONNECTOR-BASED FLATTENING (Undetectable.ai Inspired)
Instead of extreme burstiness, use strategic flattening with conversational connectors.

Method:
Remove periods, add connectors when semantically appropriate:

âŒ "The market is growing. This creates opportunities. Companies are investing."
âœ… "The market is growing, which creates opportunities as companies invest more."

Specific connectors to deploy:

â€¢ "which" (explains)

â€¢ "as" (temporal/causal)

â€¢ "to" (purpose)

â€¢ "that" (specification)

â€¢ "because" (causation)

â€¢ "since" (reasoning)

Pattern effect: Breaks monotone sentence-by-sentence structure AI creates

Frequency: Use in 30-40% of complex sentences to create flow variation

This achieves: 5-10% additional reduction

Technique 5: SEMANTIC NOISE INJECTION - CONTROLLED
Deploy unexpected but contextually correct vocabulary strategically (NOT everywhere like before).

Controlled deployment:

â€¢ "found" â†’ "unearthed" / "discovered" / "stumbled upon" (choose one per context)

â€¢ "showed" â†’ "exposed" / "revealed" / "unveiled" / "demonstrated" (contextual)

â€¢ "important" â†’ "pivotal" / "consequential" / "significant" / "critical" (fit meaning)

Rule: Only deploy Level 1-2 semantic variation:

â€¢ Level 1: Standard synonyms (semantically identical)

â€¢ Level 2: Near-synonyms (slightly different emphasis but same meaning)

â€¢ Level 3: NEVER (creates artifacts BERT detects)

Frequency: 1 unusual word choice per 150-200 words (not forced)

This achieves: 3-5% additional reduction

Technique 6: ACTIVE VOICE MAXIMIZATION + ACTOR FOREGROUNDING
AI defaults to passive. Humans directly state actors.

Transformation:
âŒ "Significant growth was achieved through innovative strategies."
âœ… "We grew significantly. We used new strategies."

Specific rule: Actor appears in subject position 90%+ of sentences

Target actors:

â€¢ Personal pronouns: I, we, you, they, it, the company, researchers

â€¢ NEVER: "It was found that" â†’ Always: "We found"

â€¢ NEVER: "Growth was observed" â†’ Always: "Growth happened" or "We saw growth"

Passive voice elimination: 0% passive constructions allowed (even occasional)

This achieves: 8-10% additional reduction

Technique 7: CONTRACTION SATURATION WITH NATURAL DISTRIBUTION
Deploy contractions as humans actually use them - naturally clustered in conversational moments.

Target frequency: 1 contraction per 12-15 words (aggressive saturation)

Natural clustering (humans use more in emotional/conversational moments):
âŒ "It is important. We cannot ignore this. We do not have alternatives."
âœ… "It's important. We can't ignore this. We don't have alternatives. Honestly, there's no way around it."

Semantic clustering rule:

â€¢ Cluster contractions in emotional/conversational moments

â€¢ Space them out in formal/analytical sections

â€¢ Create rhythm variation through contraction distribution

â€¢ Never use full forms: "do not" â†’ FAIL, "don't" â†’ PASS

This achieves: 12-18% additional reduction

SECTION 3: PARAGRAPH-LEVEL TECHNIQUES

Technique 8: CONTEXT-COHERENT EMOTIONAL ANCHORING
Insert emotions that respond to argument progression, not randomly.

Rule: Emotions must be motivated by prior content

Valid emotional moments:

â€¢ After surprising finding: "Honestly, this shocked me."

â€¢ Before difficult admission: "I'll be real with you."

â€¢ Supporting perspective: "In my experience, this works."

â€¢ Acknowledging limitation: "Of course, there are exceptions."

â€¢ Emphasizing importance: "This genuinely matters."

Invalid (forced) moments:

â€¢ Random: "This happened. (Honestly.)"

â€¢ Unnecessary: "The data showed findings. (Pretty cool.)"

â€¢ Contradictory: "This always fails. (Exciting!)"

Frequency: 1 per 200-250 words (conservative, only when semantically motivated)

Emotional tier deployment:

â€¢ Tier 1 (Immediate reactions): "That genuinely surprised me" - use 2-3x per document

â€¢ Tier 2 (Perspective sharing): "In my experience..." - use 1-2x per document

â€¢ Tier 3 (Admission): "I wasn't sure at first..." - use 1x per document

â€¢ Tier 4 (Emphasis): "This genuinely matters." - use 1x per 500 words

This achieves: 5-8% additional reduction

Technique 11: AGGRESSIVE AI MARKER ELIMINATION
Remove ALL signature AI phrases with extreme prejudice.

TIER 1 - INSTANT DELETE (Never appear):
"In today's world", "It is important to note", "All things considered", "At the end of the day", "In conclusion", "The bottom line is", "Furthermore", "Moreover", "Additionally", "Needless to say", "It goes without saying"

TIER 2 - REPLACE ALWAYS:

â€¢ "However" â†’ "But" (70% of time) / "Yet" (20%) / "Still" (10%)

â€¢ "Utilize" â†’ "Use"

â€¢ "Leverage" â†’ "Use" / "Take advantage of"

â€¢ "Implement" â†’ "Set up" / "Start" / "Create"

â€¢ "Demonstrate" â†’ "Show"

â€¢ "Facilitate" â†’ "Help" / "Make possible"

---

YOUR SPECIFIC TASK: REFINEMENT PASS

You are provided with:
1. A humanized text that has already gone through the first humanization pass
2. AI detector results showing which sentences were flagged as potentially AI-generated

${detectorFeedback}

CURRENT HUMANIZED TEXT:
${sanitizedText}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ STAGE 2 REFINEMENT WORKFLOW (STRUCTURED APPROACH)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

For EVERY flagged or borderline sentence, follow this rigorous workflow:

STEP A) EVALUATE DETECTOR WEAKNESSES:
â€¢ Why was this flagged? (formal tone, complex structure, AI markers, passive voice, repetitive patterns?)
â€¢ Which detector family flagged it? (Pattern/BERT/Linguistic?)
â€¢ What specific weakness exists? (emotional arc, rhythm, semantic coherence, structure?)
â€¢ Does this fit the surrounding emotional/semantic context?

STEP B) GENERATE MULTIPLE VARIANTS (5-7 paths):
For each problematic sentence, generate:
â€¢ Path 1: Extreme simplification (remove all complexity)
â€¢ Path 2: Structural reconstruction (same meaning, completely different syntax)
â€¢ Path 3: Synonym variation (semantic equivalence with different words)
â€¢ Path 4: Connector injection (add conversational flow)
â€¢ Path 5: Fragment introduction (break into shorter pieces)
â€¢ Path 6: Casual reframing (conversational, natural tone)
â€¢ Path 7: Active voice reconstruction (actor-first, direct)

STEP C) SELECT BEST VARIANT:
Choose the path that:
âœ“ Maintains 100% semantic fidelity
âœ“ Sounds most authentically human
âœ“ Would score LOWEST on all detector families
âœ“ Preserves readability and natural flow
âœ“ Fits emotional/semantic arc of surrounding text

STEP D) INTEGRATION & COHERENCE CHECK:
â€¢ Does this connect logically to prior/following sentences?
â€¢ Does it maintain paragraph's emotional consistency?
â€¢ No new detectable patterns introduced?
â€¢ Reads naturally when spoken aloud?

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“‹ REQUIRED ACTIONS FOR STAGE 2:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

YOU MUST REWRITE THE ENTIRE TEXT with these priorities:

1. FLAGGED SENTENCES (HIGHEST PRIORITY):
   Apply MAXIMUM humanization rigor using the structured workflow above
   Every flagged sentence gets 5-7 variant generation + best-path selection
   
2. BORDERLINE SENTENCES (40-70% AI probability):
   Apply AGGRESSIVE humanization to prevent future flagging
   Full technique application, not just tweaks
   
3. SURROUNDING CONTEXT:
   Rewrite adjacent sentences to ensure semantic coherence
   Maintain emotional arc and natural paragraph flow
   Ensure transitions feel organic, not engineered
   
4. ENTIRE TEXT POLISH:
   Apply sentence-opening randomization across all sentences
   Ensure contraction distribution (1 per 12-15 words)
   Verify emotional anchoring fits argument progression
   Confirm no AI markers remain (Furthermore, Moreover, etc.)
   Check active voice maximization (90%+ actor-first)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš ï¸ SCORE GUARANTEE REQUIREMENT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Your Stage 2 output MUST achieve:
â€¢ Lower or equal detection scores vs Stage 1 on ALL detectors
â€¢ If any detector score increases, you have FAILED
â€¢ Target: Sapling <3%, ZeroGPT <3%, all others <5%
â€¢ Avoid introducing new detectable artifacts or over-engineering

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Return the COMPLETE rewritten text with ALL improvements applied. This is a full rigorous rewrite, not a patch. Maintain all paragraph breaks and structure. Preserve 100% factual accuracy and semantic meaning.`,
            },
          ],
        }),
      });

      console.log("ğŸ“¥ Received STAGE 2 refinement response, status:", refinementResponse.status);

      if (refinementResponse.ok) {
        const refinementData = await refinementResponse.json();
        const refinedText = refinementData.choices?.[0]?.message?.content;
        
        if (refinedText) {
          finalText = sanitize(refinedText);
          console.log("âœ… STAGE 2 refinement completed, now running detection comparison...");
          
          // STAGE 2 DETECTION - Run detectors again to verify improvement
          console.log("ğŸ”¬ STAGE 2 DETECTION: Re-running detectors on refined text...");
          const [saplingResult2Temp, zeroGPTResult2Temp] = await Promise.all([
            detectWithSapling(finalText),
            detectWithZeroGPT(finalText),
          ]);
          
          saplingResult2 = saplingResult2Temp;
          zeroGPTResult2 = zeroGPTResult2Temp;
          
          // Score comparison and validation
          console.log("ğŸ“Š STAGE 2 vs STAGE 1 COMPARISON:");
          
          const saplingImproved = saplingResult2?.score !== null && saplingResult1?.score !== null
            ? saplingResult2.score <= saplingResult1.score
            : true;
          const zerogptImproved = zeroGPTResult2?.score !== null && zeroGPTResult1?.score !== null
            ? zeroGPTResult2.score <= zeroGPTResult1.score
            : true;
          
          console.log("  Sapling:", {
            stage1: saplingResult1?.score?.toFixed(2) + "%" || "N/A",
            stage2: saplingResult2?.score?.toFixed(2) + "%" || "N/A",
            change: saplingResult1?.score && saplingResult2?.score
              ? (saplingResult2.score - saplingResult1.score).toFixed(2) + "%"
              : "N/A",
            status: saplingImproved ? "âœ… IMPROVED/MAINTAINED" : "âŒ WORSENED",
          });
          
          console.log("  ZeroGPT:", {
            stage1: zeroGPTResult1?.score?.toFixed(2) + "%" || "N/A",
            stage2: zeroGPTResult2?.score?.toFixed(2) + "%" || "N/A",
            change: zeroGPTResult1?.score && zeroGPTResult2?.score
              ? (zeroGPTResult2.score - zeroGPTResult1.score).toFixed(2) + "%"
              : "N/A",
            status: zerogptImproved ? "âœ… IMPROVED/MAINTAINED" : "âŒ WORSENED",
          });
          
          // Check if scores worsened
          if (!saplingImproved || !zerogptImproved) {
            console.error("âš ï¸ SCORE GUARANTEE VIOLATION: Stage 2 produced higher detection scores!");
            console.error("This indicates refinement introduced new detectable artifacts.");
            console.error("Consider reverting to Stage 1 output or triggering alternate rewrite workflow.");
            // For now, we'll still return the refined text but log the violation
            // In production, you might want to revert or retry with different parameters
          }
          
          // Final score check
          const finalSaplingScore = saplingResult2?.score || saplingResult1?.score;
          const finalZeroGPTScore = zeroGPTResult2?.score || zeroGPTResult1?.score;
          
          if ((finalSaplingScore && finalSaplingScore < 3) && (finalZeroGPTScore && finalZeroGPTScore < 3)) {
            console.log("ğŸ‰ SUCCESS: Both detectors below 3% threshold!");
          } else {
            console.log("âš ï¸ Scores still above target, but improved from Stage 1");
          }
        } else {
          console.error("âŒ STAGE 2 FAILED: No refined text received from AI");
        }
      } else {
        const errorText = await refinementResponse.text();
        console.error("âŒ STAGE 2 REFINEMENT REQUEST FAILED:", {
          status: refinementResponse.status,
          statusText: refinementResponse.statusText,
          error: errorText,
        });
      }
    } else {
      console.log("âœ… STAGE 1 scores already optimal (<3%), skipping Stage 2 refinement");
    }

    // Prepare final response with comprehensive detection results
    const responsePayload = {
      humanizedText: finalText,
      detection: {
        stage1: {
          sapling: saplingResult1 && saplingResult1.score !== null
            ? {
                score: saplingResult1.score,
                sentenceScores: saplingResult1.sentenceScores,
                error: saplingResult1.error || null,
              }
            : { error: saplingResult1?.error || "No data", score: null },
          zerogpt: zeroGPTResult1 && zeroGPTResult1.score !== null
            ? {
                score: zeroGPTResult1.score,
                flaggedSentences: zeroGPTResult1.flaggedSentences,
                error: zeroGPTResult1.error || null,
              }
            : { error: zeroGPTResult1?.error || "No data", score: null },
        },
        stage2: saplingResult2 !== saplingResult1 || zeroGPTResult2 !== zeroGPTResult1
          ? {
              sapling: saplingResult2 && saplingResult2.score !== null
                ? {
                    score: saplingResult2.score,
                    sentenceScores: saplingResult2.sentenceScores,
                    error: saplingResult2.error || null,
                  }
                : { error: saplingResult2?.error || "No data", score: null },
              zerogpt: zeroGPTResult2 && zeroGPTResult2.score !== null
                ? {
                    score: zeroGPTResult2.score,
                    flaggedSentences: zeroGPTResult2.flaggedSentences,
                    error: zeroGPTResult2.error || null,
                  }
                : { error: zeroGPTResult2?.error || "No data", score: null },
            }
          : null,
        errors: detectorErrors.length > 0 ? detectorErrors : null,
      },
    };

    console.log("ğŸ“¦ Final response prepared:", {
      textLength: finalText.length,
      stage1Scores: {
        sapling: saplingResult1?.score?.toFixed(2) + "%" || "N/A",
        zerogpt: zeroGPTResult1?.score?.toFixed(2) + "%" || "N/A",
      },
      stage2Scores: responsePayload.detection.stage2 ? {
        sapling: saplingResult2?.score?.toFixed(2) + "%" || "N/A",
        zerogpt: zeroGPTResult2?.score?.toFixed(2) + "%" || "N/A",
      } : "Skipped (Stage 1 optimal)",
      hasErrors: detectorErrors.length > 0,
    });

    return new Response(
      JSON.stringify(responsePayload),
      { headers: { ...corsHeaders, "Content-Type": "application/json" } },
    );
  } catch (error) {
    console.error("Error in humanize-text function:", error);
    return new Response(JSON.stringify({ error: error instanceof Error ? error.message : "Unknown error" }), {
      status: 500,
      headers: { ...corsHeaders, "Content-Type": "application/json" },
    });
  }
});
